[
  {
    "objectID": "Lab9.html",
    "href": "Lab9.html",
    "title": "Lab 9",
    "section": "",
    "text": "Recall that \\(AR(1)\\) model describes the times series which satisfies\n\\[\nX_n = a X_{n-1} + Z_n,\n\\]\nwhere \\(\\{Z_n\\}\\) is a white noise. We have, hence,\n\\[\n\\begin{aligned}\nX_1 & = a X_0 + Z_1,\\\\\nX_2 & = a X_1 + Z_2,\\\\\nX_3 & = a X_2 + Z_3,\\\\\n\\vdots\\\\\nX_n & = a X_{n-1} + Z_n,\\\\\n\\vdots\n\\end{aligned}\n\\]\nThus, to generate the first \\(n\\) values of the time series, i.e. \\(X_1,\\ldots,X_n\\), one needs to have:\n\n\\(X_0\\)\n\\(Z_1,\\ldots,Z_n\\)\n\n\\(X_0\\) can be chosen as an arbitrary number. Next, we will model \\(Z_1,\\ldots,Z_n\\) as i.i.d. standard normal r.v.: \\(Z_i\\sim \\mathcal{N}(0,1)\\).\nLet’s generate first \\(1000\\) values of \\(\\{X_n\\}\\) (i.e. \\(X_1,\\ldots,X_{1000}\\)) for \\(a=0.999\\); recall that then \\(\\{X_n\\}\\) is stationary. We will use a loop from \\(1\\) to \\(n=1000\\). We choose \\(X_0=1\\). Note that if \\(X\\) is stored in a Numpy array, it’s better to create first an “empty” array with all entries equal to \\(0\\), and then just assign (otherwise, the array would become larger on each step, and this is very much time and memory consuming). So, let’s run the following code, and plot the corresponding time series:\n\nimport numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\na = 0.999\nn = 1000\nz = norm.rvs(size = n+1) # If we generate all Z_0, Z_1, ..., Z_n at once\nx = np.zeros(n+1) # Numpy array of n+1 zeroes\nx[0] = 1\nfor i in range(1,n+1):  # Recall that the end is not included\n    x[i] = a * x[i-1] + z[i]\nplt.plot(x)\nplt.show()\n\n\n\n\n\n\n\n\nalternatively, we could generate \\(Z_i\\) on each step:\n\nimport numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\na = 0.999\nn = 1000\nx = np.zeros(n+1) # Numpy array of n+1 zeroes\nx[0] = 1\nfor i in range(1,n+1):  # Recall that the end is not included\n    x[i] = a * x[i-1] + norm.rvs(size = 1)[0] # Generate Z_i on each step\nplt.plot(x, color = \"green\")\nplt.show()\n\n\n\n\n\n\n\n\nSurely, we got quite different results: not because we generated \\(\\{Z_n\\}\\) in different ways, but because of the randomness of \\(\\{Z_n\\}\\) (we can fix them by using random_state parameter inside function norm.rvs).\nThe result may be a bit confusing, as here \\(|a|&lt;1\\), so \\(\\{X_n\\}\\) is a stationary time series, i.e. we would expect more “regular” behaviour. However, this can be seen on large time intervals only.\n\n\n\n\nGenerate the first \\(10^6\\) values of the time series \\(\\{X_n\\}\\) and plot the graph. Include some comments into your code explaining the steps. (Recall that comments in Python are any text after # symbol.)\nYour output should be similar to this:\n\n\n\n\n\n\n\n\n\nIf you still think that the graph too much fluctuates, think about the scale: the length is (approximately) in \\(5000\\) times larger than the height.\n\n\n\n\n\nVery important task as a part of the Lab Test preparation.\nSave your Python notebook with solved Task 1.1 in the Anaconda.com/app. In menu File, choose Download and save the Python Notebook to your device. Rename the file on your device, so that the file name would be your student number. Upload the file to Canvas Assignment “Task 1.2 of Lab 9” and submit the Assignment.\n\nNow you may continue working with your file in Anaconda.com/app; further changes there do not need to be uploaded to Canvas.\n\n\n\n\nNow generate the first \\(10^6\\) of the time series \\(Y_n= Y_{n-1}+W_n\\), where \\(\\{W_n\\}\\) is another white noise, \\(W_n\\sim\\mathcal{N}(0,1)\\), generated separately from \\(\\{Z_n\\}\\). Choose \\(Y_0=5\\). Store it in y and make two graphs for \\(X_n\\) and \\(Y_n\\) on the same plot (by typing two commands plt.plot with one plt.show() at the end). Make them in different colours: blue for x and red for y.\n\n\n\n\n\n\n\n\n\nYou can see now how different is the behaviour of a stationary (\\(X_n\\)) and a non-stationary (\\(Y_n\\)) time series.\n\n\n\nIn \\(AR(2)\\)-model, we have\n\\[\nX_n = a X_{n-1} + b X_{n-2} + Z_n.\n\\]\nThen, we need to set values to \\(X_0\\) and \\(X_1\\), and then we can define iteratively \\[\n\\begin{aligned}\nX_2 & = a X_1 + b X_0 + Z_2,\\\\\nX_3 & = a X_2 + b X_1 + Z_3,\\\\\nX_4 & = a X_3 + b X_2 + Z_4,\\\\\n\\vdots\n\\end{aligned}\n\\]\n\n\n\n\n\nConsider the example from a lecture: \\(X_n=\\frac1{12} X_{n-1}+\\frac12 X_{n-2}+Z_n\\). Set \\(X_0=X_1=1\\). Generate \\(10^4\\) values (note that you should start from \\(n=2\\)) and assign them to x. Plot the graph of x.\n\n\n\n\n\n\n\n\n\nHence, this is quite agreed with the theoretical prediction we made on lecture: the time series demonstrates stationary behaviour.\n\n\n\nRecall that \\(ARMA(2,q)\\) means that\n\\[\nX_n = a X_{n-1} + b X_{n-2} + Z_n + \\beta_1 Z_{n-1}+\\ldots+\\beta_q Z_{n-q}.\n\\]\nWe discussed on a lecture that \\(MA\\)-part with past white noises does not affect the stationarity (or non-stationarity) of $AR-part.\nTo model e.g. \\(ARMA(2,3)\\), i.e. \\[\nX_n = a X_{n-1} + b X_{n-2} + Z_n + \\beta_1 Z_{n-1}+\\beta_2 Z_{n-2} +\\beta_3 Z_{n-3}.\n\\]\nwe notice that\n\\[\n\\begin{aligned}\nX_2 & = a X_1 + b X_0 + Z_2 + \\beta_1 Z_1 + \\beta_2 Z_0 + \\beta_3 Z_{-1},\\\\\nX_3 & = a X_2 + b X_1 + Z_3 + \\beta_1 Z_2 + \\beta_2 Z_1 +\\beta_3 Z_0,\\\\\nX_4 & = a X_3 + b X_2 + Z_4 + \\beta_1 Z_3 +\\beta_2 Z_2 + \\beta_3 Z_1,\\\\\n\\vdots\n\\end{aligned}\n\\] Hence, we need to set \\(X_0\\) and \\(X_1\\) and also \\(Z_{-1}\\). Usually, \\(Z_{-1}=0\\).\n\nConsider a modification of the previous time series \\(X_n\\), keeping \\(AR\\)-part: \\(Y_n=\\frac1{12} Y_{n-1}+\\frac12 Y_{n-2}+Z_n + 2Z_{n-1}+3Z_{n-2} + 4 Z_{n-3}\\). Choose \\(Z_{-1}=0\\) (note that in Python, if a is an array or list, then a[-1] means the last element of this array), and \\(Y_0=Y_1=1\\). Generate \\(10^4\\) values and assign them to y.\nLet’s plot both \\(\\{X_n\\}\\) and \\(\\{Y_n\\}\\) on the same diagram:\n\nplt.plot(y, color = \"blue\", label = \"Y\")\nplt.plot(x, color = \"red\", label = \"X\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nHence, as you can see, the large coefficients before past noises \\(Z_{n-1}, Z_{n-2}, Z_{n-3}\\) made large fluctuations around the zero mean, but \\(Y_n\\) is still stationary as expected.\n\n\n\n\nRecall that, for \\(ARMA(3,3)\\)-model:\n\\[\nX_n = a X_{n-1} + b X_{n-2} + c X_{n-3} + Z_n + \\beta_1 Z_{n-1}+\\beta_2 Z_{n-2} +\\beta_3 Z_{n-3}.\n\\]\nwe would need to consider the characteristic equation\n\\[\n1 -  a\\lambda - b \\lambda^2 -c \\lambda^3 =0\n\\]\nWe know from algebra, that\n\\[\n1 -  a\\lambda - b \\lambda^2 -c \\lambda^3 = -c(\\lambda - \\lambda_1)(\\lambda - \\lambda_2)\n(\\lambda - \\lambda_3),\n\\]\nwhere \\(\\lambda_1,\\lambda_2,\\lambda_3\\) are the roots of the characteristic equation. We know also that if \\(|\\lambda_1|&gt;1\\), \\(|\\lambda_2|&gt;1\\), \\(|\\lambda_3|&gt;1\\), then the \\(ARMA(3,q)\\) time series is stationary (for any \\(q\\), actually).",
    "crumbs": [
      "Labs - Problems",
      "Lab 9 - Problems"
    ]
  },
  {
    "objectID": "Lab9.html#section",
    "href": "Lab9.html#section",
    "title": "Lab 9",
    "section": "",
    "text": "Generate the first \\(10^6\\) values of the time series \\(\\{X_n\\}\\) and plot the graph. Include some comments into your code explaining the steps. (Recall that comments in Python are any text after # symbol.)\nYour output should be similar to this:\n\n\n\n\n\n\n\n\n\nIf you still think that the graph too much fluctuates, think about the scale: the length is (approximately) in \\(5000\\) times larger than the height.",
    "crumbs": [
      "Labs - Problems",
      "Lab 9 - Problems"
    ]
  },
  {
    "objectID": "Lab9.html#section-1",
    "href": "Lab9.html#section-1",
    "title": "Lab 9",
    "section": "",
    "text": "Very important task as a part of the Lab Test preparation.\nSave your Python notebook with solved Task 1.1 in the Anaconda.com/app. In menu File, choose Download and save the Python Notebook to your device. Rename the file on your device, so that the file name would be your student number. Upload the file to Canvas Assignment “Task 1.2 of Lab 9” and submit the Assignment.\n\nNow you may continue working with your file in Anaconda.com/app; further changes there do not need to be uploaded to Canvas.",
    "crumbs": [
      "Labs - Problems",
      "Lab 9 - Problems"
    ]
  },
  {
    "objectID": "Lab9.html#section-2",
    "href": "Lab9.html#section-2",
    "title": "Lab 9",
    "section": "",
    "text": "Now generate the first \\(10^6\\) of the time series \\(Y_n= Y_{n-1}+W_n\\), where \\(\\{W_n\\}\\) is another white noise, \\(W_n\\sim\\mathcal{N}(0,1)\\), generated separately from \\(\\{Z_n\\}\\). Choose \\(Y_0=5\\). Store it in y and make two graphs for \\(X_n\\) and \\(Y_n\\) on the same plot (by typing two commands plt.plot with one plt.show() at the end). Make them in different colours: blue for x and red for y.\n\n\n\n\n\n\n\n\n\nYou can see now how different is the behaviour of a stationary (\\(X_n\\)) and a non-stationary (\\(Y_n\\)) time series.\n\n\n\nIn \\(AR(2)\\)-model, we have\n\\[\nX_n = a X_{n-1} + b X_{n-2} + Z_n.\n\\]\nThen, we need to set values to \\(X_0\\) and \\(X_1\\), and then we can define iteratively \\[\n\\begin{aligned}\nX_2 & = a X_1 + b X_0 + Z_2,\\\\\nX_3 & = a X_2 + b X_1 + Z_3,\\\\\nX_4 & = a X_3 + b X_2 + Z_4,\\\\\n\\vdots\n\\end{aligned}\n\\]",
    "crumbs": [
      "Labs - Problems",
      "Lab 9 - Problems"
    ]
  },
  {
    "objectID": "Lab9.html#section-3",
    "href": "Lab9.html#section-3",
    "title": "Lab 9",
    "section": "",
    "text": "Consider the example from a lecture: \\(X_n=\\frac1{12} X_{n-1}+\\frac12 X_{n-2}+Z_n\\). Set \\(X_0=X_1=1\\). Generate \\(10^4\\) values (note that you should start from \\(n=2\\)) and assign them to x. Plot the graph of x.\n\n\n\n\n\n\n\n\n\nHence, this is quite agreed with the theoretical prediction we made on lecture: the time series demonstrates stationary behaviour.\n\n\n\nRecall that \\(ARMA(2,q)\\) means that\n\\[\nX_n = a X_{n-1} + b X_{n-2} + Z_n + \\beta_1 Z_{n-1}+\\ldots+\\beta_q Z_{n-q}.\n\\]\nWe discussed on a lecture that \\(MA\\)-part with past white noises does not affect the stationarity (or non-stationarity) of $AR-part.\nTo model e.g. \\(ARMA(2,3)\\), i.e. \\[\nX_n = a X_{n-1} + b X_{n-2} + Z_n + \\beta_1 Z_{n-1}+\\beta_2 Z_{n-2} +\\beta_3 Z_{n-3}.\n\\]\nwe notice that\n\\[\n\\begin{aligned}\nX_2 & = a X_1 + b X_0 + Z_2 + \\beta_1 Z_1 + \\beta_2 Z_0 + \\beta_3 Z_{-1},\\\\\nX_3 & = a X_2 + b X_1 + Z_3 + \\beta_1 Z_2 + \\beta_2 Z_1 +\\beta_3 Z_0,\\\\\nX_4 & = a X_3 + b X_2 + Z_4 + \\beta_1 Z_3 +\\beta_2 Z_2 + \\beta_3 Z_1,\\\\\n\\vdots\n\\end{aligned}\n\\] Hence, we need to set \\(X_0\\) and \\(X_1\\) and also \\(Z_{-1}\\). Usually, \\(Z_{-1}=0\\).\n\nConsider a modification of the previous time series \\(X_n\\), keeping \\(AR\\)-part: \\(Y_n=\\frac1{12} Y_{n-1}+\\frac12 Y_{n-2}+Z_n + 2Z_{n-1}+3Z_{n-2} + 4 Z_{n-3}\\). Choose \\(Z_{-1}=0\\) (note that in Python, if a is an array or list, then a[-1] means the last element of this array), and \\(Y_0=Y_1=1\\). Generate \\(10^4\\) values and assign them to y.\nLet’s plot both \\(\\{X_n\\}\\) and \\(\\{Y_n\\}\\) on the same diagram:\n\nplt.plot(y, color = \"blue\", label = \"Y\")\nplt.plot(x, color = \"red\", label = \"X\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nHence, as you can see, the large coefficients before past noises \\(Z_{n-1}, Z_{n-2}, Z_{n-3}\\) made large fluctuations around the zero mean, but \\(Y_n\\) is still stationary as expected.\n\n\n\n\nRecall that, for \\(ARMA(3,3)\\)-model:\n\\[\nX_n = a X_{n-1} + b X_{n-2} + c X_{n-3} + Z_n + \\beta_1 Z_{n-1}+\\beta_2 Z_{n-2} +\\beta_3 Z_{n-3}.\n\\]\nwe would need to consider the characteristic equation\n\\[\n1 -  a\\lambda - b \\lambda^2 -c \\lambda^3 =0\n\\]\nWe know from algebra, that\n\\[\n1 -  a\\lambda - b \\lambda^2 -c \\lambda^3 = -c(\\lambda - \\lambda_1)(\\lambda - \\lambda_2)\n(\\lambda - \\lambda_3),\n\\]\nwhere \\(\\lambda_1,\\lambda_2,\\lambda_3\\) are the roots of the characteristic equation. We know also that if \\(|\\lambda_1|&gt;1\\), \\(|\\lambda_2|&gt;1\\), \\(|\\lambda_3|&gt;1\\), then the \\(ARMA(3,q)\\) time series is stationary (for any \\(q\\), actually).",
    "crumbs": [
      "Labs - Problems",
      "Lab 9 - Problems"
    ]
  },
  {
    "objectID": "Lab8-solutions.html",
    "href": "Lab8-solutions.html",
    "title": "Lab 8 - Solutions",
    "section": "",
    "text": "Let \\(X\\) be a random variable whose distribution depends on a parameter \\(\\theta\\in\\mathbb{R}\\), or on a group of parameters, \\(\\theta=(\\theta_1,\\ldots,\\theta_k)\\). Let \\(x_1,\\ldots,x_n\\) be a sample of values of the random variable \\(X\\). The likelihood function \\(\\mathcal{L}(\\theta\\mid x_1,\\ldots,x_n)\\) is defined as follows:\n\nfor a discrete random variable \\(X\\) with the probability mass function \\(p_X(x)=\\mathbb{P}(X=x)\\),\n\n\\[\n\\mathcal{L}(\\theta\\mid x_1,\\ldots,x_n) = p_X(x_1)\\cdot\\ldots\\cdot p_X(x_n);\n\\]\n\nfor a continuous random variable \\(X\\) with the probability density function \\(f_X(x)\\),\n\n\\[\n\\mathcal{L}(\\theta\\mid x_1,\\ldots,x_n) = f_X(x_1)\\cdot\\ldots\\cdot f_X(x_n).\n\\]\nThe log-likelihood function is the (natural) logarithm of the likelihood:\n\\[\n\\begin{aligned}\nL(\\theta\\mid x_1,\\ldots,x_n): &= \\ln \\mathcal{L}(\\theta\\mid x_1,\\ldots,x_n)\\\\\n&= \\ln p_X(x_1)+\\ldots + \\ln p_X(x_n)\n\\end{aligned}\n\\]\nfor the discrete case; and the same formula with \\(p_X\\) replaced by \\(f_X\\) holds for the continuous case.\n\n\n\nSuppose we are given a sample\n\nimport numpy as np\ndata = np.array([8,  4,  7, 11,  9,  7,  5,  9,  8,  7])\n\nAnd suppose that we are told that this is a sample generated by a binomial random variable \\(X\\sim Bin(20,p)\\), i.e \\(X:\\Omega\\to\\{0,1,\\ldots,20\\}\\) with\n\\[\n\\mathbb{P}(X=k)= \\binom{20}{k} p^k (1-p)^{n-k}.\n\\]\nhowever, the probability \\(p\\) of a “success” is unknown (i.e. here \\(\\theta=p\\)). One needs to find the value of \\(p\\) which would make the probability to observe data the highest possible.\nThe following code would do the needed:\n\nfrom scipy.stats import binom, fit\nbounds = [(20,20), (0, 1)]\nfit(binom, data, bounds)\n\n  params: FitParams(n=20.0, p=0.37499999528423467, loc=0.0)\n success: True\n message: 'Optimization terminated successfully.'\n\n\nLet’s discuss this code from the end. The function fit has three arguments:\n\nbinom is the class of the distributions (from scipy.stats) in which we are looking for a best fit;\ndata is the given sample;\nbounds sets bounds for the parameters of the considered distribution: in this case, binom has two parameters: \\(X\\sim Bin(n,p)\\), i.e. there are \\(n\\) and \\(p\\). \\(n\\) is given to be \\(20\\) (we know the number of attempts), whereas \\(p\\in[0,1]\\) is the unknown probability. Note that bounds is a Python list, whose entries are tuples. Tuples are very similar to lists, the main difference is that the tuple can’t be changed after it is defined (whereas a list may be e.g. extended or some its elements may be changed or removed). Each tuple sets the lower and the upper bounds for the corresponding parameter. So, for \\(n\\) we request \\(20\\leq n\\leq 20\\) (that just means \\(n=20\\), as needed), and for \\(p\\) we have \\(0\\leq p\\leq 1\\).\n\nThe output tells us that the best possible \\(n=20.0\\) (that is not surprising as we requested this) and the best possible \\(p\\approx 0.375\\) (we ignore loc for now). The values are accessible:\n\nres = fit(binom, data, bounds)\nres.params\n\nFitParams(n=20.0, p=0.3749999940793132, loc=0.0)\n\n\nso that there are two parameters, and the maximum likelihood estimator is the second of them:\n\nres.params[1]\n\n0.3749999940793132\n\n\n(and res.params[0] would return 20.0).\n\nRemark. You may notice that the values of p appeared to be slightly different. It’s because the internal algorithm of how fit maximises the likelihood function is the so-called stochastic algorithm, it has a random output (though pretty close to the “real” point of maximum).\n\nIn the first task, we will test the work of fit function for the case when we know the answer. Recall that, to generate e.g. \\(5\\) values (outputs) of a Bernoulli random variable \\(X\\in\\{0,1\\}\\) with \\(\\mathbb{P}(X=1)=p\\) for some \\(p\\), e.g. for \\(p=0.4\\), we need to use the code\n\nfrom scipy.stats import bernoulli\np = 0.4\nbernoulli.rvs(p, size = 5, random_state = 10)\n\narray([1, 0, 1, 1, 0], dtype=int64)\n\n\nHere, recall, random_state can be any integer number, but if you use 10 your output will be exactly like above.\n\nRemark. Note also that you could write here more “full” code\n\nbernoulli.rvs(p = p, size = 5, random_state = 10)\n\narray([1, 0, 1, 1, 0], dtype=int64)\n\n\nDon’t be confused with p = p: the first p here is the name of an argument of function bernoulli.rvs (indeed, just such a short name), where the second p in p = p is the name of the variable we defined previously. Python allows such clashes (as it is clear what is what). Moreover, this is a kind of standard coding practice.\n\n\n\n\n\nGenerate \\(n = 100\\) values of a Bernoulli random variable \\(X\\) with \\(\\mathbb{P}(X=1)=0.3\\), using random_state = 100, and assign the resulting Numpy-array to the variable data_ber.\n\n\nCode\nn = 100\np = 0.3\ndata_ber = bernoulli.rvs(p, size = n, random_state = 100)\n\n\nCheck the output by calculating its sum:\n\ndata_ber.sum()\n\n25\n\n\nStress that the calculated sum is the number of \\(1\\)-s in the generated data.\n\n\n\n\n\nFind the maximum likelihood estimate for \\(p=\\mathbb{P}(X=1)\\), using fit function for the data data_ber (stress that bernoulli has only one parameter, denoted below by p, hence, bounds should contain only one tuple for the range of \\(p\\in[0,1]\\)). Assign the result to res_ber.\n\n\nCode\nres_ber = fit(bernoulli, data_ber, bounds = [(0,1)])\n\n\nCheck the output:\n\n\nCode\nres_ber.params\n\n\nFitParams(p=0.24999999259172612, loc=0.0)\n\n\n\nAs we can see, the answer is pretty close to the real maximum likelihood estimate (known from lectures), which \\(\\frac{k}{n}\\), where \\(k\\) is the number of \\(1\\)-s and \\(n\\) is the total number of trials: the absolution value of the difference:\n\nnp.abs(data_ber.sum()/n - res_ber.params[0])\n\n7.40827388323595e-09\n\n\nOn the other hand, the maximum likelihood estimator does not recover the real probability p = 0.3 with which the data data_ber was generated. This because the sample size is relatively small.\n\n\n\n\nRepeat the previous steps but generate now a sample of the size \\(n = 100000\\). Assign the output of fit function to res_big_ber.\n\n\nCode\nn = 100000\np = 0.3\ndata_ber = bernoulli.rvs(p, size = n, random_state = 100)\nres_big_ber = fit(bernoulli, data_ber, bounds = [(0,1)])\n\n\nCheck the found value:\n\nres_big_ber.params[0]\n\n0.2998799973367203\n\n\nso the result is much closer to \\(0.3\\).\n\n\n\nLet’s now return back to the dataset data. Suppose now that we know only that it follows the binomial distribution: \\(X\\sim Bin(n,p)\\), where \\(n\\) is not know exactly, but we expect that \\(1\\leq n\\leq 25\\). We can then modify the bounds for \\(n\\):\n\nbounds = [(1, 25), (0, 1)]\nres = fit(binom, data, bounds)\nres.params\n\nFitParams(n=14.0, p=0.535714273902257, loc=0.0)\n\n\nThus, the highest probability to see the sample data, for the given restriction on \\(n\\), would be if \\(X\\sim Bin(14, \\approx 0.5357)\\).\nMoreover, the output of fit function (which we denoted res) can be plotted:\n\nimport matplotlib.pyplot as plt \nres.plot()\nplt.show()\n\n\n\n\n\n\n\n\n\nRemark. Similarly, to the tasks above, the data here was initially generated by very different parameters: \\(X\\sim Bin(18, 0.435)\\). However, the fitting for so small sample can’t reconstruct this. The sample is random, and it appeared that among all \\(Bin(n,p)\\) the distribution \\(X\\sim Bin(14, \\approx 0.5357)\\) is the closest (the most typical) for such sample.\n\n\n\n\n\n\nGenerate \\(n=10\\) random values of the normal distribution of \\(X\\sim \\mathcal{N}(10, 1.5^2)\\) (see Lab 5 (solutions): Task 3.4 for rvs function and e.g. Task 3.3 for using loc argument for the mean and scale for the standard deviation; don’t forget to import norm). Use random_state = 123. Assign the output to data_norm.\n\n\nCode\nfrom scipy.stats import norm\ndata_norm = norm.rvs(loc = 10, scale = 1.5, size = 10, random_state = 123)\n\n\nCheck yourself by calculating mean and standard deviation of the output:\n\n[data_norm.mean(), data_norm.std()]\n\n[9.595725834510507, 1.8544572022485344]\n\n\nAs you can see, the sample is not large enough to “catch” the original mean, though, it’s relatively close.\n\n\n\n\n\nFit the data to a normal distribution: find the normal distribution \\(\\mathcal{N}(\\mu,\\sigma^2)\\) with \\(\\mu\\in [7,12]\\) and \\(\\sigma\\in[1,2]\\) which is most likely to be the distribution for the sample data_norm. Assign the result to res_norm.\n\n\nCode\nbounds = [(7, 12), (1,2)]\nres_norm = fit(norm, data_norm, bounds)\n\n\nCheck the output:\n\nres_norm.params\n\nFitParams(loc=9.595726502801217, scale=1.8544564399196046)\n\n\nAs you can see, the fitting reflects the characteristics of the sample, not the initial distribution (as the sample was not large enough).\n\n\nRemark. Some of distributions in scipy.stats have own fit methods which do not require using bounds. For example, one can write\n\nnorm.fit(data_norm)\n\n(9.595725834510507, 1.8544572022485344)\n\n\nIn other words, Python looked here among all \\(\\mathcal{N}(\\mu,\\sigma^2)\\), without any restrictions on \\(\\mu\\) and \\(\\sigma\\).",
    "crumbs": [
      "Labs - Solutions",
      "Lab 8 - Solutions"
    ]
  },
  {
    "objectID": "Lab8-solutions.html#section",
    "href": "Lab8-solutions.html#section",
    "title": "Lab 8 - Solutions",
    "section": "",
    "text": "Generate \\(n = 100\\) values of a Bernoulli random variable \\(X\\) with \\(\\mathbb{P}(X=1)=0.3\\), using random_state = 100, and assign the resulting Numpy-array to the variable data_ber.\n\n\nCode\nn = 100\np = 0.3\ndata_ber = bernoulli.rvs(p, size = n, random_state = 100)\n\n\nCheck the output by calculating its sum:\n\ndata_ber.sum()\n\n25\n\n\nStress that the calculated sum is the number of \\(1\\)-s in the generated data.",
    "crumbs": [
      "Labs - Solutions",
      "Lab 8 - Solutions"
    ]
  },
  {
    "objectID": "Lab8-solutions.html#section-1",
    "href": "Lab8-solutions.html#section-1",
    "title": "Lab 8 - Solutions",
    "section": "",
    "text": "Find the maximum likelihood estimate for \\(p=\\mathbb{P}(X=1)\\), using fit function for the data data_ber (stress that bernoulli has only one parameter, denoted below by p, hence, bounds should contain only one tuple for the range of \\(p\\in[0,1]\\)). Assign the result to res_ber.\n\n\nCode\nres_ber = fit(bernoulli, data_ber, bounds = [(0,1)])\n\n\nCheck the output:\n\n\nCode\nres_ber.params\n\n\nFitParams(p=0.24999999259172612, loc=0.0)\n\n\n\nAs we can see, the answer is pretty close to the real maximum likelihood estimate (known from lectures), which \\(\\frac{k}{n}\\), where \\(k\\) is the number of \\(1\\)-s and \\(n\\) is the total number of trials: the absolution value of the difference:\n\nnp.abs(data_ber.sum()/n - res_ber.params[0])\n\n7.40827388323595e-09\n\n\nOn the other hand, the maximum likelihood estimator does not recover the real probability p = 0.3 with which the data data_ber was generated. This because the sample size is relatively small.",
    "crumbs": [
      "Labs - Solutions",
      "Lab 8 - Solutions"
    ]
  },
  {
    "objectID": "Lab8-solutions.html#section-2",
    "href": "Lab8-solutions.html#section-2",
    "title": "Lab 8 - Solutions",
    "section": "",
    "text": "Repeat the previous steps but generate now a sample of the size \\(n = 100000\\). Assign the output of fit function to res_big_ber.\n\n\nCode\nn = 100000\np = 0.3\ndata_ber = bernoulli.rvs(p, size = n, random_state = 100)\nres_big_ber = fit(bernoulli, data_ber, bounds = [(0,1)])\n\n\nCheck the found value:\n\nres_big_ber.params[0]\n\n0.2998799973367203\n\n\nso the result is much closer to \\(0.3\\).\n\n\n\nLet’s now return back to the dataset data. Suppose now that we know only that it follows the binomial distribution: \\(X\\sim Bin(n,p)\\), where \\(n\\) is not know exactly, but we expect that \\(1\\leq n\\leq 25\\). We can then modify the bounds for \\(n\\):\n\nbounds = [(1, 25), (0, 1)]\nres = fit(binom, data, bounds)\nres.params\n\nFitParams(n=14.0, p=0.535714273902257, loc=0.0)\n\n\nThus, the highest probability to see the sample data, for the given restriction on \\(n\\), would be if \\(X\\sim Bin(14, \\approx 0.5357)\\).\nMoreover, the output of fit function (which we denoted res) can be plotted:\n\nimport matplotlib.pyplot as plt \nres.plot()\nplt.show()\n\n\n\n\n\n\n\n\n\nRemark. Similarly, to the tasks above, the data here was initially generated by very different parameters: \\(X\\sim Bin(18, 0.435)\\). However, the fitting for so small sample can’t reconstruct this. The sample is random, and it appeared that among all \\(Bin(n,p)\\) the distribution \\(X\\sim Bin(14, \\approx 0.5357)\\) is the closest (the most typical) for such sample.",
    "crumbs": [
      "Labs - Solutions",
      "Lab 8 - Solutions"
    ]
  },
  {
    "objectID": "Lab8-solutions.html#section-3",
    "href": "Lab8-solutions.html#section-3",
    "title": "Lab 8 - Solutions",
    "section": "",
    "text": "Generate \\(n=10\\) random values of the normal distribution of \\(X\\sim \\mathcal{N}(10, 1.5^2)\\) (see Lab 5 (solutions): Task 3.4 for rvs function and e.g. Task 3.3 for using loc argument for the mean and scale for the standard deviation; don’t forget to import norm). Use random_state = 123. Assign the output to data_norm.\n\n\nCode\nfrom scipy.stats import norm\ndata_norm = norm.rvs(loc = 10, scale = 1.5, size = 10, random_state = 123)\n\n\nCheck yourself by calculating mean and standard deviation of the output:\n\n[data_norm.mean(), data_norm.std()]\n\n[9.595725834510507, 1.8544572022485344]\n\n\nAs you can see, the sample is not large enough to “catch” the original mean, though, it’s relatively close.",
    "crumbs": [
      "Labs - Solutions",
      "Lab 8 - Solutions"
    ]
  },
  {
    "objectID": "Lab8-solutions.html#section-4",
    "href": "Lab8-solutions.html#section-4",
    "title": "Lab 8 - Solutions",
    "section": "",
    "text": "Fit the data to a normal distribution: find the normal distribution \\(\\mathcal{N}(\\mu,\\sigma^2)\\) with \\(\\mu\\in [7,12]\\) and \\(\\sigma\\in[1,2]\\) which is most likely to be the distribution for the sample data_norm. Assign the result to res_norm.\n\n\nCode\nbounds = [(7, 12), (1,2)]\nres_norm = fit(norm, data_norm, bounds)\n\n\nCheck the output:\n\nres_norm.params\n\nFitParams(loc=9.595726502801217, scale=1.8544564399196046)\n\n\nAs you can see, the fitting reflects the characteristics of the sample, not the initial distribution (as the sample was not large enough).\n\n\nRemark. Some of distributions in scipy.stats have own fit methods which do not require using bounds. For example, one can write\n\nnorm.fit(data_norm)\n\n(9.595725834510507, 1.8544572022485344)\n\n\nIn other words, Python looked here among all \\(\\mathcal{N}(\\mu,\\sigma^2)\\), without any restrictions on \\(\\mu\\) and \\(\\sigma\\).",
    "crumbs": [
      "Labs - Solutions",
      "Lab 8 - Solutions"
    ]
  },
  {
    "objectID": "Lab8-solutions.html#section-5",
    "href": "Lab8-solutions.html#section-5",
    "title": "Lab 8 - Solutions",
    "section": "2.1 ",
    "text": "2.1 \n\nAssign to logprobabilities the array of natural logarithms of all entries in probabilities. Assign to loglikelihood the sum of all these logarithms.\n\n\nCode\nlogprobabilities = np.log(probabilities)\nloglikelihood = logprobabilities.sum() # or np.sum(log-probabilities)\n\n\nCheck the output:\n\nloglikelihood\n\n-21.08353673407401\n\n\n\n\nRemark. Note that the distributions in scipy.stats contains functions logpmf and logpdf for calculation logarithms of PMF (for discrete random variables) and PDF (for continuous random variables), respectively. We could write also:\n\nlogprobabilities = binom.logpmf(data, n = n, p = p)\nloglikelihood = logprobabilities.sum()\nloglikelihood\n\n-21.08353673407403\n\n\n\nNow, we are going to calculate the log-likelihood for a range of \\(p\\in[0,1]\\). We, hence, define a function which combines all previous steps. Let’s call e.g. loglbinom, it will have only one argument: the value of \\(p\\); we keep the value of \\(n\\) and data fixed. We can do this in one line\n\ndef loglbinom(p):\n    return binom.logpmf(data, n = n, p = p).sum()\n\nIt can be also written “longer”:\n\ndef loglbinom1(p):\n    probabilities = binom.pmf(data, n = n, p = p)\n    logprobabilities = np.log(probabilities)\n    loglikelihood = logprobabilities.sum()\n    return loglikelihood\n\nLet’s check that it would be the same as in the previous computations for \\(p=0.4\\):\n\n[loglbinom(0.4), loglbinom1(0.4)]\n\n[-21.08353673407403, -21.08353673407401]\n\n\nso both of course coincides with the previously found loglikelihood.\nWe are going to apply loglbinom to an array fo values of p. For this, we create its vectorised version:\n\nvloglbinom = np.vectorize(loglbinom)",
    "crumbs": [
      "Labs - Solutions",
      "Lab 8 - Solutions"
    ]
  },
  {
    "objectID": "Lab8-solutions.html#section-6",
    "href": "Lab8-solutions.html#section-6",
    "title": "Lab 8 - Solutions",
    "section": "2.2 ",
    "text": "2.2 \n\nDefine array x of 1000 points from \\([0,1]\\), using np.linspace function (see e.g. Lab 4 or Lab 5). Apply the vectorised function vloglbinom to x (it keeps n=20 and data fixed), and assign the result to y. Plot the graph of y against x.\n\n\nCode\nimport matplotlib.pyplot as plt\nx = np.linspace(0, 1, 1000)\ny = vloglbinom(x)\nplt.plot(x,y)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nAnd finally, one can find now the point of maximum of the log-likelihood: namely, np.argmax(y) returns the index in array y of the maximal element:\n\nind_max = np.argmax(y)\n\ntherefore, the value y[ind_max] is the maximal value of the log-likelihood function. However, we are interested in the argument, that is the corresponding x[ind_max]:\n\nx[ind_max]\n\n0.37537537537537535\n\n\nthat is pretty close to the initially found value. (Surely, if we divide \\([0,1]\\) on a larger number of pieces, the prediction will be better.)\nThe following graph has the vertical line at the found value of x[ind_max], as you can see, it comes (pretty close) to the maximum of the log-likelihood function.",
    "crumbs": [
      "Labs - Solutions",
      "Lab 8 - Solutions"
    ]
  },
  {
    "objectID": "Lab8-solutions.html#section-7",
    "href": "Lab8-solutions.html#section-7",
    "title": "Lab 8 - Solutions",
    "section": "3.1 ",
    "text": "3.1 \n\nGenerate \\(m=1000\\) samples, each sample of size \\(5\\), of Bernoulli random variable with \\(p=\\mathbb{P}(X=1)=0.3\\). For each sample calculate the maximum likelihood estimate \\(\\hat{p}\\) of \\(p\\). Find the average of the obtained \\(MLE\\).\n\n\nCode\ns = 0\nm = 1000\nfor i in range(m):\n  data_ber = bernoulli.rvs(p = 0.3, size = 5)\n  res_ber = fit(bernoulli, data_ber, bounds = [(0,1)])\n  s += res_ber.params[0]\ns/m\n\n\n0.3040000054846187\n\n\nSince the samples are random, your answer will be different. Notice that it’s quite close to \\(p=0.3\\).",
    "crumbs": [
      "Labs - Solutions",
      "Lab 8 - Solutions"
    ]
  },
  {
    "objectID": "Lab7-solutions.html",
    "href": "Lab7-solutions.html",
    "title": "Lab 7 - Solutions",
    "section": "",
    "text": "As you know from the Lecture Notes, we use \\(z\\)-tests if we either know that the data is distributed normally or if the sample is large enough so we may assume that its averaged value is distributed normally according to the central limit theorem. Note that, to use \\(z\\)-tests, we have to know the variance of the distribution.\nRecall the main quantities we may need:\n\n\nE.g. in the case of one sample, it’s\n\\[\nz=\\frac{\\bar{x}-\\mu}{\\dfrac{\\sigma}{\\sqrt{n}}}\n\\tag{1}\\]\nThis works e.g. for the estimation of \\(\\mu\\) of \\(n\\) observations from \\(\\mathcal{N}(\\mu, \\sigma^2)\\) (when \\(\\sigma^2\\) is known); and this also works for large samples.\n\n\n\nInformally speaking, \\(p\\)-value describes the area of the “tail” of distribution bounded by the test statistic. On the diagram below, the tails are shaded in pink. Recall that \\(\\varphi(z)=f_Z(z)\\) is the PDF of the standard normal distribution, where \\(Z\\sim \\mathcal{N}(0,1)\\). The pink areas are then related to the CDF \\(\\Phi_Z\\) of \\(Z\\sim \\mathcal{N}(0,1)\\), namely:\n\n\n\n\n\n\n\n\n\n\n\n\nIf the test statistic \\(z&lt;0\\), then the area of the tail left to the \\(z\\) is \\(\\Phi(z)=F_Z(z)\\).\nIf the test statistic \\(z&gt;0\\), then the area of the tail right to the \\(z\\) is \\(1-\\Phi(z)=1-F_Z(z)\\).\nIf we consider the two-tailed case (e.g. the alternative hypothesis \\(H_1\\) is \\(\\mu\\neq\\mu_0\\)), then \\(p\\)-value is the sum of the tail pink areas (i.e. \\(p\\) is twice bigger than each of them).\nIf we consider the one-tailed case (e.g. \\(H_1\\) is \\(\\mu&lt;\\mu_0\\) or \\(H_1\\) is \\(\\mu&gt;\\mu_0\\)), the \\(p\\)-value is either of the (equal) tail pink areas.\n\n\n\n\n\nWe reject the null-hypothesis \\(H_0\\) if \\(p\\leq \\alpha\\).\nWe do not reject the null-hypothesis \\(H_0\\) if \\(p&gt;\\alpha\\).\n\n\n\n\n\\(z\\)-scores provide another approach to test the hypothesis (i.e. practically we calculate either \\(p\\)-value or \\(z\\)-score).\nIn simple words, \\(z_\\mathrm{score}\\) is the number such that its \\(p\\)-value would be \\(\\alpha\\). Therefore, to find \\(z_\\mathrm{score}\\), we would use the inverse function to \\(\\Phi\\), i.e. the percentile (see Lab 5). We will choose \\(z_\\mathrm{score}\\) so that its sign is the same as it is for the test statistic.\n\n\n\n\n\n\n\n\n\n\n\n\nWe reject \\(H_0\\), if the test statistic \\(z\\) is inside the critical region constructed by \\(z_\\mathrm{score}\\) (the pink area), i.e. if \\(z&gt;z_\\mathrm{score}&gt;0\\) or if \\(z&lt;z_\\mathrm{score}&lt;0\\).\nWe do note reject \\(H_0\\) otherwise.\n\n\n\n\n\nConsider an example from the Lecture notes.\nA machine produces items having a nominal mass of \\(1\\) kg. The mass of a randomly selected item \\(x\\) follows the distribution \\(X\\sim \\mathcal{N}(\\mu, 0.02^2)\\). If \\(\\mu\\neq 1\\) then the machinery should be corrected. The mean mass of a randomly sample of \\(25\\) items was found to be \\(0.989\\) kg. Test the null hypothesis that \\(H_0:\\mu=1\\) at the \\(1\\%\\) significance level.\nNote that the alternative hypothesis here is \\(\\mu\\neq1\\), i.e. we deal with the two-tailed test.\nWe have\n\nmu = 1\nsigma = 0.02\nn = 25\nxbar = 0.989\nalpha = 0.01\n\nand we can calculate the test statistic. Recall that, for the square root function we need either use an additional module, e.g. import numpy as np and then np.sqrt(n) (this is reasonable in the case we use numpy also for some other purposes) or we just use that \\(\\sqrt{n}=n^{\\frac12}\\), namely, we write by Equation 1,\n\nz = (xbar - mu) / (sigma / (n ** 0.5))\nz\n\n-2.750000000000002\n\n\nSince the test statistic is negative, to calculate the \\(p\\)-value we consider the \\(\\Phi(z)\\), and since this is the two-tailed case, we have that \\(p=2\\Phi(z)\\). Namely, we calculate\n\nfrom scipy.stats import norm\np = 2 * norm.cdf(z)\np\n\n0.0059595264701090694\n\n\nSince \\(p &lt; \\alpha = 0.01\\), i.e. the significance of the \\(p\\)-value is below the significance level, we reject \\(H_0\\).\nAlternatively, we can use the \\(z\\)-score approach (that is exactly what you use doing calculations by hand and using the statistical tables), we need to use norm.ppf function (discussed on Lab 5); since this is the two-tailed case, we calculate it at \\(\\frac{\\alpha}2\\), that is then the area of the left pink tail on Figure 2, as we need to choose the negative \\(z_\\mathrm{score}\\):\n\nzscore = norm.ppf(alpha/2)\nzscore\n\n-2.575829303548901\n\n\nSince \\(z&lt;z_\\mathrm{score}\\), i.e. the test statistic is inside the critical (left tail, left pink) region, we reject \\(H_0\\).\n\n\n\nThe number of strokes a golfer takes to complete a round of golf has mean \\(84.1\\) and standard deviation \\(2.6\\). After a month of holidays without playing golf her mean is now \\(85.1\\) in \\(36\\) subsequent rounds. At the \\(5\\%\\) significance level test the null hypothesis that her standard of play is unaltered against the alternative hypothesis that it became worse, i.e.\n\n\\(H_0: \\mu=84.1\\)\n\\(H_1: \\mu&gt;84.1\\) (one-tailed)\n\n\n\n\n\nCalculate the test statistic and assign it to z.\n\n\nCode\nmu = 84.1\nsigma = 2.6\nn = 36\nxbar = 85.1\nalpha = 0.05\nz = (xbar - mu) / (sigma / (n ** 0.5))\n\n\nCheck the answer:\n\nz\n\n2.3076923076923075\n\n\n\n\n\n\n\nCalculate the \\(p\\)-value, taking into account that \\(z&gt;0\\) and we test the one-tailed case.\n\n\nCode\np = 1 - norm.cdf(z)\n\n\nCheck the answer:\n\np\n\n0.01050812811375934\n\n\nSince \\(p&lt;0.05=\\alpha\\), we reject \\(H_0\\).\n\n\n\n\n\nAlternatively, calculate the \\(z\\)-score correspondent to the considered significance level (and assign it to zscore). Note that we are going to compare the \\(z\\)-score with the test statistic, hence, we will be looking for the positive \\(z\\)-score (as the test statistic is positive).\n\n\nCode\nzscore = norm.ppf(1-0.05)\n\n\nCheck the answer:\n\nzscore\n\n1.6448536269514722\n\n\nSince \\(0&lt;z_\\mathrm{score}&lt;z\\), we have that the test statistic is inside the critical (tail) region, and we will reject \\(H_0\\).",
    "crumbs": [
      "Labs - Solutions",
      "Lab 7 - Solutions"
    ]
  },
  {
    "objectID": "Lab7-solutions.html#section",
    "href": "Lab7-solutions.html#section",
    "title": "Lab 7 - Solutions",
    "section": "",
    "text": "Calculate the test statistic and assign it to z.\n\n\nCode\nmu = 84.1\nsigma = 2.6\nn = 36\nxbar = 85.1\nalpha = 0.05\nz = (xbar - mu) / (sigma / (n ** 0.5))\n\n\nCheck the answer:\n\nz\n\n2.3076923076923075",
    "crumbs": [
      "Labs - Solutions",
      "Lab 7 - Solutions"
    ]
  },
  {
    "objectID": "Lab7-solutions.html#section-1",
    "href": "Lab7-solutions.html#section-1",
    "title": "Lab 7 - Solutions",
    "section": "",
    "text": "Calculate the \\(p\\)-value, taking into account that \\(z&gt;0\\) and we test the one-tailed case.\n\n\nCode\np = 1 - norm.cdf(z)\n\n\nCheck the answer:\n\np\n\n0.01050812811375934\n\n\nSince \\(p&lt;0.05=\\alpha\\), we reject \\(H_0\\).",
    "crumbs": [
      "Labs - Solutions",
      "Lab 7 - Solutions"
    ]
  },
  {
    "objectID": "Lab7-solutions.html#section-2",
    "href": "Lab7-solutions.html#section-2",
    "title": "Lab 7 - Solutions",
    "section": "",
    "text": "Alternatively, calculate the \\(z\\)-score correspondent to the considered significance level (and assign it to zscore). Note that we are going to compare the \\(z\\)-score with the test statistic, hence, we will be looking for the positive \\(z\\)-score (as the test statistic is positive).\n\n\nCode\nzscore = norm.ppf(1-0.05)\n\n\nCheck the answer:\n\nzscore\n\n1.6448536269514722\n\n\nSince \\(0&lt;z_\\mathrm{score}&lt;z\\), we have that the test statistic is inside the critical (tail) region, and we will reject \\(H_0\\).",
    "crumbs": [
      "Labs - Solutions",
      "Lab 7 - Solutions"
    ]
  },
  {
    "objectID": "Lab7-solutions.html#section-3",
    "href": "Lab7-solutions.html#section-3",
    "title": "Lab 7 - Solutions",
    "section": "2.1 ",
    "text": "2.1 \n\nDownload file Heights.csv, upload it to Anaconda cloud, and load its content to Pandas data frame df_heights (see Lab 1 if you don’t remember how to do this).\n\n\nCode\nimport pandas as pd\ndf_heights = pd.read_csv('Heights.csv')\n\n\nCheck the first rows to see the structure:\n\ndf_heights.head()\n\n\n\n\n\n\n\n\nHEIGHTS\n\n\n\n\n0\n6.50\n\n\n1\n6.25\n\n\n2\n6.33\n\n\n3\n6.50\n\n\n4\n6.42\n\n\n\n\n\n\n\n\nWe assume that it is known that this sample of heights has normal distribution.",
    "crumbs": [
      "Labs - Solutions",
      "Lab 7 - Solutions"
    ]
  },
  {
    "objectID": "Lab7-solutions.html#section-4",
    "href": "Lab7-solutions.html#section-4",
    "title": "Lab 7 - Solutions",
    "section": "2.2 ",
    "text": "2.2 \n\nAssign the values of the only column of this data frame to Numpy array x. Assign to n the size of the sample, i.e. the length of x using function len.\n\n\nCode\nx = df_heights['HEIGHTS'].to_numpy()\nn  = len(x)\n\n\nCheck the answer:\n\nn\n\n65\n\n\n\nRecall that, to calculate the sample standard deviation \\(s\\) (that has \\(n-1\\) in the denominator), we need to use the key ddof = 1, e.g.\n\nimport numpy as np\na = np.array([1,2,3])\nnp.std(a, ddof = 1)\n\n1.0\n\n\nNote that we can also write\n\na.std(ddof = 1)\n\n1.0\n\n\ni.e. instead of applying function np.std to a we can use the property (method) std of the Numpy-array a. The same trick works for other statistical characteristics, e.g. a.mean() and np.mean(a) give the same.",
    "crumbs": [
      "Labs - Solutions",
      "Lab 7 - Solutions"
    ]
  },
  {
    "objectID": "Lab7-solutions.html#section-5",
    "href": "Lab7-solutions.html#section-5",
    "title": "Lab 7 - Solutions",
    "section": "2.3 ",
    "text": "2.3 \n\nAssign to xbar the mean of x. Assign to s the sample standard deviation of x.\n\n\nCode\nxbar = x.mean() # Or xbar = np.mean(x)\ns = x.std(ddof = 1)\n\n\nCheck the answers:\n\n[xbar, s]\n\n[6.467692323784616, 0.3302970630589082]",
    "crumbs": [
      "Labs - Solutions",
      "Lab 7 - Solutions"
    ]
  },
  {
    "objectID": "Lab7-solutions.html#section-6",
    "href": "Lab7-solutions.html#section-6",
    "title": "Lab 7 - Solutions",
    "section": "2.4 ",
    "text": "2.4 \n\nWe are going to use a one-sample \\(t\\)-test to determine whether the heights in the data frame has a mean of \\(\\mu = 6.5\\). Assign to tstat the corresponding test statistic, using Equation 2.\n\n\nCode\nmu = 6.5\ntstat = (xbar - mu) / (s / np.sqrt(n)) # As numpy is anyway in use, or keep n ** 0.5 instead\n\n\nCheck the answer\n\ntstat\n\n-0.7886016620455049\n\n\nHere \\(H_0: \\mu=6.5\\) and \\(H_1: \\mu\\neq 6.5\\), i.e. we deal with the two-tailed test.\nWe can calculate \\(p\\)-value, recall that for the one-sample test we should take \\(t\\)-distribution with \\(n-1\\) degrees of freedom. Since the test statistic is negative, we have\n\np = 2 * t.cdf(tstat, df = n-1)\np\n\n0.433256350170654\n\n\nTherefore, if we consider any significance level below this \\(p\\): e.g. \\(5\\%=0.05\\), then we do not have any evidence to reject \\(H_0\\).\n\nAlternatively, we calculate the \\(t\\)-score corresponding to the chosen significance level, e.g. \\(\\alpha = 0.05\\). Since this is two-tailed test, we calculate the percentile at \\(\\frac{\\alpha}{2}\\):\n\ntscore = t.ppf(0.05/2, df = n-1)\ntscore\n\n-1.997729654317693\n\n\nSince (the negative) \\(t\\)-score is smaller than (the negative) \\(t\\)-statistic, the latter lies outside of the critical interval, and again, we can’t reject \\(H_0\\).\nApparently, scipy.stats module proposes a faster way to calculate both the test statistic and the \\(p\\)-value for the \\(1\\)-sample \\(t\\)-test:\n\nfrom scipy.stats import ttest_1samp\nmu = 6.5 # If you haven't assign it before\nttest_1samp(x, mu)\n\nTtestResult(statistic=-0.7886016620455049, pvalue=0.433256350170654, df=64)\n\n\nSurely the answers are the same as we found previously. They are also accessible directly:\n\nresults = ttest_1samp(x, mu)\nresults.pvalue\n\n0.433256350170654\n\n\n(and similarly for results.statistic).",
    "crumbs": [
      "Labs - Solutions",
      "Lab 7 - Solutions"
    ]
  },
  {
    "objectID": "Lab7-solutions.html#section-7",
    "href": "Lab7-solutions.html#section-7",
    "title": "Lab 7 - Solutions",
    "section": "2.5 ",
    "text": "2.5 \n\nDownload the file HealthData.csv, upload it to Anaconda.com/app, and consider the column DENSITY. Use a one-sample \\(t\\)-test to determine whether the density variable in the data set Health Data has a mean of \\(1.051\\) using the \\(5\\)% significance level. Assign the corresponding \\(p\\)-value to p (you may use either of the considered approaches).\n\n\nCode\ndf_health = pd.read_csv('HealthData.csv')\nx = df_health['DENSITY']\nmu = 1.051\nresults = ttest_1samp(x, mu)\np = results.pvalue\n\n\nCheck the answer:\n\np\n\n0.00017145125375254356\n\n\nAs you can see \\(p\\)-value is less than the significance level: surely, you check this in Python :-)\n\np &lt; 0.05\n\nTrue\n\n\nHence, we reject the null-hypothesis that \\(\\mu=1.051\\).\n\n\nTwo-sample test\nWe consider now examples of two-sample tests.\nFirst, we will use a paired \\(t\\)-test to determine whether there is any difference in the two processes (Process A and Process B) of preserving meat joints. This data can be found in the file MeatJoints.csv, and we will test at the 5% significance level whether the means of the two processes are equal.\nSo, we have \\(H_0: \\mu_A=\\mu_B\\) and \\(H_1: \\mu_A\\neq \\mu_B\\).",
    "crumbs": [
      "Labs - Solutions",
      "Lab 7 - Solutions"
    ]
  },
  {
    "objectID": "Lab7-solutions.html#section-8",
    "href": "Lab7-solutions.html#section-8",
    "title": "Lab 7 - Solutions",
    "section": "2.6 ",
    "text": "2.6 \n\nLoad the the data to Anaconda.com/app, and assign to \\(a\\) the column Process A and to \\(b\\) the column Process B.\n\n\nCode\ndf_meat = pd.read_csv('MeatJoints.csv')\na = df_meat['Process A'].to_numpy()\nb = df_meat['Process B'].to_numpy()\n\n\nThen use the following command to apply two-sample test\n\nfrom scipy.stats import ttest_rel\nresults = ttest_rel(a,b)\nresults\n\nTtestResult(statistic=-2.29517764444372, pvalue=0.047371692861499864, df=9)\n\n\nSince\n\nresults.pvalue &lt; 0.05\n\nTrue\n\n\nwe reject \\(H_0\\).",
    "crumbs": [
      "Labs - Solutions",
      "Lab 7 - Solutions"
    ]
  },
  {
    "objectID": "Lab7-solutions.html#section-9",
    "href": "Lab7-solutions.html#section-9",
    "title": "Lab 7 - Solutions",
    "section": "3.1 ",
    "text": "3.1 \n\nLoad the file to Anaconda.com/app and assign to f the heights of football players and to b the heights of basketball players.\n\n\nCode\ndf_sport = pd.read_csv('SportHeights.csv')\nf = df_sport['football'].to_numpy()\nb = df_sport['basketball'].to_numpy()\n\n\nThen perform the following test:\n\nfrom scipy.stats import ttest_ind\nresults = ttest_ind(f, b, nan_policy='omit')\nresults\n\nTtestResult(statistic=-3.684107948156318, pvalue=0.00040777606606915155, df=83.0)\n\n\nNote the key nan_policy='omit' as b and f have different sizes here.\nSince\n\nresults.pvalue &lt; 0.05\n\nTrue\n\n\nwe reject the null hypothesis that the mean heights of basketball and football players are equal.",
    "crumbs": [
      "Labs - Solutions",
      "Lab 7 - Solutions"
    ]
  },
  {
    "objectID": "Lab6-solutions.html",
    "href": "Lab6-solutions.html",
    "title": "Lab 6 - Solutions",
    "section": "",
    "text": "Consider an example similar to the one discussed on a lecture: let \\(X:\\Omega\\to\\{0,1\\}\\) and \\(Y:\\Omega\\to\\{0, 1, 2, 3\\}\\) be two discrete random random variables whose joint probability mass function\n\\[\np_{X,Y}(x_i,y_j)=\\mathbb{P}(X=x_i, Y=y_j), \\qquad x_i\\in \\{0,1\\}, y_j\\in \\{0.1,2,3\\}\n\\]\nis given through the following table:\n\n\n\n\\(X\\) \\(\\backslash\\) \\(Y\\)\n\\(0\\)\n\\(1\\)\n\\(2\\)\n\\(3\\)\n\n\n\n\n\\(0\\)\n\\(0.1\\)\n\\(0.15\\)\n\\(0.05\\)\n\\(0.25\\)\n\n\n\\(1\\)\n\\(0.15\\)\n\\(0.05\\)\n\\(0.15\\)\n\\(0.1\\)\n\n\n\n(so that here, e.g. \\(p_{X,Y}(0,0)=0.1\\), \\(p_{X,Y}(0,3)=0.25\\) and so on).\nThe values of \\(p_{X,Y}\\) naturally form a matrix, of size \\(2\\times 4\\). To work with this matrix in Python, we will use two-dimensional Numpy-arrays:\n\nimport numpy as np\npxy = np.array([[0.1, 0.15, 0.05, 0.25],\n                [0.15, 0.05, 0.15, 0.1]])\npxy\n\narray([[0.1 , 0.15, 0.05, 0.25],\n       [0.15, 0.05, 0.15, 0.1 ]])\n\n\nNote that indents inside np.array are NOT important, you may write e.g. everything in one line.\nRecall that marginal distributions (i.e. distributions of \\(X\\) and of \\(Y\\) separately) can be obtained from this table by summing over rows (for \\(X\\)) or columns (for \\(Y\\)):\n\\[\n\\begin{aligned}\np_X(x_i)&=\\mathbb{P}(X=x_i) = \\sum_{j}p_{X,Y}(x_i,y_j),\\\\\np_Y(y_j)&=\\mathbb{P}(Y=y_j) = \\sum_{i}p_{X,Y}(x_i,y_j).\n\\end{aligned}\n\\]\nIn Numpy, we can sum-up along any dimension, called axis. The first dimension (rows) corresponds to axis = 0, the second dimension (columns) corresponds toaxis = 1. However, to get sums of elements in each row of array pxy we can write np.sum(pxy, axis = 1) or pxy.sum(axis = 1):\n\npx = np.sum(pxy, axis = 1)\npx\n\narray([0.55, 0.45])\n\n\ni.e. \\(\\mathbb{P}(X=0)=0.55\\) (the sum of the first row) and \\(\\mathbb{P}(X=1)=0.45\\) (the sum of the second row).\nThe reason of axis = 1 here is that these sums naturally form a column:\n\n\n\n\\(X\\) \\(\\backslash\\) \\(Y\\)\n\\(0\\)\n\\(1\\)\n\\(2\\)\n\\(3\\)\nsum of row\n\n\n\n\n\\(0\\)\n\\(0.1\\)\n\\(0.15\\)\n\\(0.05\\)\n\\(0.25\\)\n\\(0.55\\)\n\n\n\\(1\\)\n\\(0.15\\)\n\\(0.05\\)\n\\(0.15\\)\n\\(0.1\\)\n\\(0.45\\)\n\n\n\nSimilarly, to get the sums of all columns we write np.sum(pxy, axis = 0) or pxy.sum(axis = 0) as the results form a row:\n\npy = pxy.sum(axis = 0)\npy\n\narray([0.25, 0.2 , 0.2 , 0.35])\n\n\ni.e. \\(\\mathbb{P}(Y=0)=0.25\\) (the sum of the first column) and so on:\n\n\n\n\\(X\\) \\(\\backslash\\) \\(Y\\)\n\\(0\\)\n\\(1\\)\n\\(2\\)\n\\(3\\)\n\n\n\n\n\\(0\\)\n\\(0.1\\)\n\\(0.15\\)\n\\(0.05\\)\n\\(0.25\\)\n\n\n\\(1\\)\n\\(0.15\\)\n\\(0.05\\)\n\\(0.15\\)\n\\(0.1\\)\n\n\nsum of column\n\\(0.25\\)\n\\(0.2\\)\n\\(0.2\\)\n\\(0.35\\)\n\n\n\nAnother way of thinking is that axis = 0 tells “to get rid of” rows, so only columns would remain (that’s why py has 4 components, the number of columns in the original pxy), whereas axis = 1 tells to get rid of columns so only 2 rows remain in px.\nNote that, as it should be, the sum of all probabilities for \\(X\\) is \\(1\\), and the same sum for \\(Y\\) is \\(1\\). Again, we can use either np.sum(px) or px.sum() command:\n\n[np.sum(px), py.sum()]\n\n[1.0, 1.0]\n\n\nThis corresponds to\n\\[\n\\sum_{i}\\sum_{j}p_{X,Y}(x_i,y_j)=1.\n\\]\nWe will also need the array of values of \\(X\\) and of \\(Y\\). In this case, they can be defined manually:\n\nx = np.array([0,1])\nx\n\narray([0, 1])\n\n\nor using e.g. arange function:\n\ny = np.arange(4)\ny\n\narray([0, 1, 2, 3])\n\n\nWe can e.g. calculate\n\\[\n\\mathbb{E}(X) = x_1\\cdot \\mathbb{P}(X=x_1)+\\ldots + x_n\\cdot \\mathbb{P}(X=x_n)\n\\]\nWe discussed how to do this using basic entry-wise product of arrays in Numpy, however, it can be also done faster, using the dot-product of the arrays x and px (you will study the dot-product or inner-product of vectors in MA-M26). Namely, for vectors \\(a=(a_1,\\ldots,a_n)\\) and \\(b=(b_1,\\ldots,b_n)\\),\n\\[\na \\cdot b = a_1b_1+\\ldots+b_n b_n.\n\\] In Numpy, we can write just a.dot(b), or b.dot(a) (the results will be the same).\n\n\n\nCalculate \\(\\mathbb{E}(X)\\) and \\(\\mathbb{E}(Y)\\) and assign the results to mx and my, respectively.\n\n\nCode\nmx = x.dot(px)\nmy = y.dot(py)\n\n\nCheck the answer:\n\n[mx, my]\n\n[0.44999999999999996, 1.65]\n\n\n(Surely, \\(\\mathbb{E}(X)=0\\cdot 0.55+1\\cdot 0.45=0.45\\), so here it’s just a rounding error.)\n\nRecall that\n\\[\n\\mathbb{E}(XY) = \\sum_{i}\\sum_{j} x_i\\cdot  y_j \\cdot p_{X,Y}(x_i,y_j)=\\sum_{i}x_i\\cdot \\sum_{j}  y_j \\cdot p_{X,Y}(x_i,y_j).\n\\]\nFor each \\(x_i\\), the second sum \\(\\sum\\limits_{j}  y_j\\cdot p_{X,Y}(x_i,y_j)\\) is nothing but the \\(i\\)-th component of the vector \\(Qy\\), where \\(Q\\) is the matrix whose entries are \\(p_{X,Y}(x_i,y_j)\\):\n\\[\nQ = \\begin{pmatrix}\np_{X,Y}(x_1,y_1) & \\ldots & p_{X,Y}(x_1,y_n)\\\\\n\\vdots & \\vdots &\\vdots\\\\\np_{X,Y}(x_m,y_1) & \\ldots & p_{X,Y}(x_m,y_n)\n\\end{pmatrix}\n\\]\n(in our example, \\(m=2\\), \\(x_1=0\\), \\(x_2=1\\), \\(n=4\\), \\(y_1=0\\), \\(y_2=1\\), \\(y_3=2\\), \\(y_4=3\\)).\nand for \\(y=(y_1,\\ldots, y_n)\\), we indeed have that \\(Qy = ((Qy)_1, \\ldots, (Qy)_m)\\) and\n\\[\n(Qy)_i = \\sum_{j}  y_j \\cdot p_{X,Y}(x_i,y_j).\n\\]\nThe Numpy command is the same Q.dot(y) (in our case Q was previously denoted pxy), though here the order is important (we multiply a matrix by a vector).\n\nQy = pxy.dot(y)\nQy\n\narray([1.  , 0.65])\n\n\nand now\n\\[\n\\mathbb{E}(XY) = \\sum_{i} x_i \\cdot (Qy)_i = x\\cdot Qy,\n\\]\ni.e.\n\nmxy = x.dot(Qy)\nmxy\n\n0.65\n\n\nSurely, it was possible to do not introduce the intermediate Qy and claculate all in one command:\n\nx.dot(pxy.dot(y))\n\n0.65\n\n\n\n\n\n\nFind \\(\\mathrm{cov}(X,Y)=\\mathbb{E}(XY) - \\mathbb{E}(X)\\mathbb{E}(Y)\\) and assign it to covxy.\n\n\nCode\ncovxy = mxy - mx*my\n\n\nCheck the answer:\n\ncovxy\n\n-0.09249999999999992\n\n\n\n\n\n\n\nFind \\(\\sigma(X)\\) and \\(\\sigma(Y)\\) using the formulas\n\\[\n\\sigma^2(X)=\\mathrm{Var}(X)= \\mathbb{E}(X^2)- (\\mathbb{E}(X))^2\n\\]\nand assign them to sx,sy, respectively. Recall that the square root can be obtained by using np.sqrt function.\n\n\nCode\nsx = np.sqrt((x**2).dot(px) - mx ** 2)\nsy = np.sqrt((y**2).dot(py) - my ** 2)\n\n\nCheck the answer:\n\n[sx, sy]\n\n[0.49749371855331, 1.1947803145348523]\n\n\n\n\n\n\n\nFind \\(\\mathrm{corr}(X,Y)=\\frac{\\mathrm{cov}(X,Y)}{\\sigma(X)\\sigma(Y)}\\) and assign it to corrxy.\n\n\nCode\ncorrxy = covxy/(sx*sy)\n\n\nCheck the answer:\n\ncorrxy\n\n-0.15562023709382963\n\n\n\nTherefore, \\(X\\) and \\(Y\\) are weakly negatively correlated.\n\n\n\n\n\n\n\nDownload file jointdistr.csv, and upload it to anaconda.com/app. Import Pandas library and load the data from this file to Pandas data frame df.\n\n\nCode\nimport pandas as pd\ndf = pd.read_csv('jointdistr.csv')\n\n\nCheck the result:\n\ndf.head()\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n\n\n0\n0.003\n0.005\n0.001\n0.002\n0.004\n0.001\n0.005\n0.004\n0.004\n0.004\n0.003\n0.006\n0.001\n0.004\n0.006\n0.001\n0.006\n0.003\n0.005\n\n\n1\n0.001\n0.006\n0.002\n0.006\n0.001\n0.006\n0.006\n0.002\n0.002\n0.002\n0.003\n0.003\n0.006\n0.006\n0.001\n0.006\n0.002\n0.001\n0.001\n\n\n2\n0.001\n0.001\n0.006\n0.004\n0.004\n0.001\n0.004\n0.006\n0.005\n0.006\n0.006\n0.004\n0.001\n0.004\n0.003\n0.006\n0.001\n0.001\n0.005\n\n\n3\n0.005\n0.005\n0.002\n0.004\n0.006\n0.006\n0.006\n0.004\n0.001\n0.005\n0.006\n0.006\n0.001\n0.006\n0.003\n0.006\n0.006\n0.001\n0.001\n\n\n4\n0.001\n0.006\n0.006\n0.002\n0.001\n0.001\n0.004\n0.001\n0.002\n0.002\n0.001\n0.001\n0.002\n0.005\n0.005\n0.001\n0.003\n0.006\n0.001\n\n\n\n\n\n\n\n\nWe can also inspect the size of the data frame:\n\ndf.shape\n\n(15, 19)\n\n\nIt is possible to work with the data frame directly, but for simplicity, we convert its content into Numpy array:\n\npxy = df.to_numpy()\n\nLook at the output of pxy (it’s quite long).\n\n\n\n\nLet \\(X\\in\\{0,\\ldots,14\\}\\) and \\(Y\\in\\{0,\\ldots,18\\}\\) be discrete random variables with the joint distribution given by the table above. Find \\(\\mathrm{corr}(X,Y)\\).\n\n\nCode\npx = np.sum(pxy, axis = 1)\npy = np.sum(pxy, axis = 0)\nx = np.arange(df.shape[0]) # i.e. np.arange(14)\ny = np.arange(df.shape[1]) # i.e. np.arange(18)\nmx = x.dot(px)\nmy = y.dot(py)\nQy = pxy.dot(y)\nmxy = x.dot(Qy)\ncovxy = mxy - mx*my\nsx = np.sqrt((x**2).dot(px) - mx**2)\nsy = np.sqrt((y**2).dot(py) - my**2)\ncorrxy = covxy/(sx*sy)\n\n\nCheck the answer:\n\ncorrxy\n\n-0.014393645358846184",
    "crumbs": [
      "Labs - Solutions",
      "Lab 6 - Solutions"
    ]
  },
  {
    "objectID": "Lab6-solutions.html#section",
    "href": "Lab6-solutions.html#section",
    "title": "Lab 6 - Solutions",
    "section": "",
    "text": "Calculate \\(\\mathbb{E}(X)\\) and \\(\\mathbb{E}(Y)\\) and assign the results to mx and my, respectively.\n\n\nCode\nmx = x.dot(px)\nmy = y.dot(py)\n\n\nCheck the answer:\n\n[mx, my]\n\n[0.44999999999999996, 1.65]\n\n\n(Surely, \\(\\mathbb{E}(X)=0\\cdot 0.55+1\\cdot 0.45=0.45\\), so here it’s just a rounding error.)\n\nRecall that\n\\[\n\\mathbb{E}(XY) = \\sum_{i}\\sum_{j} x_i\\cdot  y_j \\cdot p_{X,Y}(x_i,y_j)=\\sum_{i}x_i\\cdot \\sum_{j}  y_j \\cdot p_{X,Y}(x_i,y_j).\n\\]\nFor each \\(x_i\\), the second sum \\(\\sum\\limits_{j}  y_j\\cdot p_{X,Y}(x_i,y_j)\\) is nothing but the \\(i\\)-th component of the vector \\(Qy\\), where \\(Q\\) is the matrix whose entries are \\(p_{X,Y}(x_i,y_j)\\):\n\\[\nQ = \\begin{pmatrix}\np_{X,Y}(x_1,y_1) & \\ldots & p_{X,Y}(x_1,y_n)\\\\\n\\vdots & \\vdots &\\vdots\\\\\np_{X,Y}(x_m,y_1) & \\ldots & p_{X,Y}(x_m,y_n)\n\\end{pmatrix}\n\\]\n(in our example, \\(m=2\\), \\(x_1=0\\), \\(x_2=1\\), \\(n=4\\), \\(y_1=0\\), \\(y_2=1\\), \\(y_3=2\\), \\(y_4=3\\)).\nand for \\(y=(y_1,\\ldots, y_n)\\), we indeed have that \\(Qy = ((Qy)_1, \\ldots, (Qy)_m)\\) and\n\\[\n(Qy)_i = \\sum_{j}  y_j \\cdot p_{X,Y}(x_i,y_j).\n\\]\nThe Numpy command is the same Q.dot(y) (in our case Q was previously denoted pxy), though here the order is important (we multiply a matrix by a vector).\n\nQy = pxy.dot(y)\nQy\n\narray([1.  , 0.65])\n\n\nand now\n\\[\n\\mathbb{E}(XY) = \\sum_{i} x_i \\cdot (Qy)_i = x\\cdot Qy,\n\\]\ni.e.\n\nmxy = x.dot(Qy)\nmxy\n\n0.65\n\n\nSurely, it was possible to do not introduce the intermediate Qy and claculate all in one command:\n\nx.dot(pxy.dot(y))\n\n0.65",
    "crumbs": [
      "Labs - Solutions",
      "Lab 6 - Solutions"
    ]
  },
  {
    "objectID": "Lab6-solutions.html#section-1",
    "href": "Lab6-solutions.html#section-1",
    "title": "Lab 6 - Solutions",
    "section": "",
    "text": "Find \\(\\mathrm{cov}(X,Y)=\\mathbb{E}(XY) - \\mathbb{E}(X)\\mathbb{E}(Y)\\) and assign it to covxy.\n\n\nCode\ncovxy = mxy - mx*my\n\n\nCheck the answer:\n\ncovxy\n\n-0.09249999999999992",
    "crumbs": [
      "Labs - Solutions",
      "Lab 6 - Solutions"
    ]
  },
  {
    "objectID": "Lab6-solutions.html#section-2",
    "href": "Lab6-solutions.html#section-2",
    "title": "Lab 6 - Solutions",
    "section": "",
    "text": "Find \\(\\sigma(X)\\) and \\(\\sigma(Y)\\) using the formulas\n\\[\n\\sigma^2(X)=\\mathrm{Var}(X)= \\mathbb{E}(X^2)- (\\mathbb{E}(X))^2\n\\]\nand assign them to sx,sy, respectively. Recall that the square root can be obtained by using np.sqrt function.\n\n\nCode\nsx = np.sqrt((x**2).dot(px) - mx ** 2)\nsy = np.sqrt((y**2).dot(py) - my ** 2)\n\n\nCheck the answer:\n\n[sx, sy]\n\n[0.49749371855331, 1.1947803145348523]",
    "crumbs": [
      "Labs - Solutions",
      "Lab 6 - Solutions"
    ]
  },
  {
    "objectID": "Lab6-solutions.html#section-3",
    "href": "Lab6-solutions.html#section-3",
    "title": "Lab 6 - Solutions",
    "section": "",
    "text": "Find \\(\\mathrm{corr}(X,Y)=\\frac{\\mathrm{cov}(X,Y)}{\\sigma(X)\\sigma(Y)}\\) and assign it to corrxy.\n\n\nCode\ncorrxy = covxy/(sx*sy)\n\n\nCheck the answer:\n\ncorrxy\n\n-0.15562023709382963\n\n\n\nTherefore, \\(X\\) and \\(Y\\) are weakly negatively correlated.",
    "crumbs": [
      "Labs - Solutions",
      "Lab 6 - Solutions"
    ]
  },
  {
    "objectID": "Lab6-solutions.html#section-4",
    "href": "Lab6-solutions.html#section-4",
    "title": "Lab 6 - Solutions",
    "section": "",
    "text": "Download file jointdistr.csv, and upload it to anaconda.com/app. Import Pandas library and load the data from this file to Pandas data frame df.\n\n\nCode\nimport pandas as pd\ndf = pd.read_csv('jointdistr.csv')\n\n\nCheck the result:\n\ndf.head()\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n\n\n0\n0.003\n0.005\n0.001\n0.002\n0.004\n0.001\n0.005\n0.004\n0.004\n0.004\n0.003\n0.006\n0.001\n0.004\n0.006\n0.001\n0.006\n0.003\n0.005\n\n\n1\n0.001\n0.006\n0.002\n0.006\n0.001\n0.006\n0.006\n0.002\n0.002\n0.002\n0.003\n0.003\n0.006\n0.006\n0.001\n0.006\n0.002\n0.001\n0.001\n\n\n2\n0.001\n0.001\n0.006\n0.004\n0.004\n0.001\n0.004\n0.006\n0.005\n0.006\n0.006\n0.004\n0.001\n0.004\n0.003\n0.006\n0.001\n0.001\n0.005\n\n\n3\n0.005\n0.005\n0.002\n0.004\n0.006\n0.006\n0.006\n0.004\n0.001\n0.005\n0.006\n0.006\n0.001\n0.006\n0.003\n0.006\n0.006\n0.001\n0.001\n\n\n4\n0.001\n0.006\n0.006\n0.002\n0.001\n0.001\n0.004\n0.001\n0.002\n0.002\n0.001\n0.001\n0.002\n0.005\n0.005\n0.001\n0.003\n0.006\n0.001\n\n\n\n\n\n\n\n\nWe can also inspect the size of the data frame:\n\ndf.shape\n\n(15, 19)\n\n\nIt is possible to work with the data frame directly, but for simplicity, we convert its content into Numpy array:\n\npxy = df.to_numpy()\n\nLook at the output of pxy (it’s quite long).",
    "crumbs": [
      "Labs - Solutions",
      "Lab 6 - Solutions"
    ]
  },
  {
    "objectID": "Lab6-solutions.html#section-5",
    "href": "Lab6-solutions.html#section-5",
    "title": "Lab 6 - Solutions",
    "section": "",
    "text": "Let \\(X\\in\\{0,\\ldots,14\\}\\) and \\(Y\\in\\{0,\\ldots,18\\}\\) be discrete random variables with the joint distribution given by the table above. Find \\(\\mathrm{corr}(X,Y)\\).\n\n\nCode\npx = np.sum(pxy, axis = 1)\npy = np.sum(pxy, axis = 0)\nx = np.arange(df.shape[0]) # i.e. np.arange(14)\ny = np.arange(df.shape[1]) # i.e. np.arange(18)\nmx = x.dot(px)\nmy = y.dot(py)\nQy = pxy.dot(y)\nmxy = x.dot(Qy)\ncovxy = mxy - mx*my\nsx = np.sqrt((x**2).dot(px) - mx**2)\nsy = np.sqrt((y**2).dot(py) - my**2)\ncorrxy = covxy/(sx*sy)\n\n\nCheck the answer:\n\ncorrxy\n\n-0.014393645358846184",
    "crumbs": [
      "Labs - Solutions",
      "Lab 6 - Solutions"
    ]
  },
  {
    "objectID": "Lab6-solutions.html#section-6",
    "href": "Lab6-solutions.html#section-6",
    "title": "Lab 6 - Solutions",
    "section": "2.1 ",
    "text": "2.1 \n\nRepeat the previous calculations fr \\(n=50\\), use the same random_state = 12. Assign \\(\\overline{X}_n\\) to xbar.\n\n\nCode\nn = 50\nx = binom.rvs(10, 0.3, size = n, random_state = 12)\nxbar = x.mean()\n\n\nCheck the answer:\n\nxbar\n\n3.06\n\n\nThat is much closer to \\(\\mathbb{E}(X)=3\\).\n\nSurely, if you change random_state, you will get another sample and another value of \\(\\overline{X}_n\\), that may be more or less closer to \\(\\mathbb{E}(X)\\). Let now generate many such samples (of the same size \\(n=50\\)).\nWe will use a loop for this. The following code generates \\(K=50\\) samples of length \\(n=50\\) of values of \\(X\\sim Bin(10,0.3)\\) and store the sample means in the list Xbar (we use the loop index as the random state, it’s not compulsory, of course):\n\nn = 50\nK = 50\nXbar = []\nfor i in range(K):\n        x = binom.rvs(10, 0.3, size = n, random_state = i)\n        Xbar.append(x.mean())\n\nLook at the output of Xbar.\nNote that more “Pythonic”-way would be to use the list comprehension:\n\nXbar = [np.mean(binom.rvs(10, 0.3, size = n, random_state = i)) for i in range(K)]\n\nWe will work with a Numpy array, not just a list:\n\nXbar = np.array(Xbar)\n\nNow, suppose we want to calculate \\(\\mathbb{P}(\\overline{X}_{n=50}&lt;2.9)\\). We may look at the proportion of the entries of Xbar which are less than 2.9: namely, Xbar&lt;2.9 is the list of True and False, where True = 1 and False = 0. Hence, the sum of this list gives the number of True among all \\(K\\) trials. Dividing by \\(K\\) we get the desired proportion:\n\np = sum(Xbar &lt; 2.9)/K\np\n\n0.28\n\n\nNext, we know that \\(\\mu=\\mathbb{E}(X)=3\\) and \\(\\sigma=\\sigma(X)=\\sqrt{10\\cdot0.3\\cdot(1-0.3)}=1.449\\), or:\n\nbinom.std(10, 0.3)\n\n1.4491376746189437\n\n\nThen\n\\[\n\\begin{aligned}\n\\mathbb{P}(\\overline{X}_{n=50}&lt;2.9)&=\n\\mathbb{P}\\biggl(\\frac{\\overline{X}_{n}-\\mu}{\\frac{\\sigma}{\\sqrt{n}}}&lt;\\frac{2.9-\\mu}{\\frac{\\sigma}{\\sqrt{n}}}\\biggr)\\\\\n&=\\mathbb{P}(Z_n &lt;\\underbrace{\\frac{2.9-3}{\\frac{1.449}{\\sqrt{50}}}}_{\\color{red}=z})\n\\end{aligned}\n\\]\n\nz = (2.9 - binom.mean(10, 0.3))/(binom.std(10, 0.3)/np.sqrt(n))\nz\n\n-0.48795003647426705\n\n\nBy CLT, since \\(z&lt;0\\),\n\\[\n\\mathbb{P}(\\overline{X}_{n=50}&lt;2.9)\\approx \\Phi(z)=\\Phi(-0.488).\n\\]\nWe have\n\nfrom scipy.stats import norm\nnorm.cdf(z)\n\n0.31279261576216244\n\n\nand we see that the value is quite close to the previously found p. Recall that p was the frequency of the event \\(\\overline{X}_{n=50}&lt;2.9\\) in \\(K=50\\) trials, rather than real probability.",
    "crumbs": [
      "Labs - Solutions",
      "Lab 6 - Solutions"
    ]
  },
  {
    "objectID": "Lab6-solutions.html#section-7",
    "href": "Lab6-solutions.html#section-7",
    "title": "Lab 6 - Solutions",
    "section": "2.2 ",
    "text": "2.2 \n\nRepeat the previous considerations find p for \\(K=1000\\).\n\n\nCode\nn = 50\nK = 1000\nXbar = np.array([np.mean(binom.rvs(10, 0.3, size = n, random_state = i)) for i in range(K)])\np = sum(Xbar &lt; 2.9)/K\n\n\nCheck the answer:\n\np\n\n0.306\n\n\nThe answer now is closer to the desired \\(\\Phi(z)\\).\n\n\nIf you have time\nWe can also build the histogram for e.g. \\(Z_n=\\frac{\\overline{X}_n-\\mu}{\\frac{\\sigma}{\\sqrt{n}}}\\) and compare it visually with the PDF of the standard normal distribution:\n\nimport matplotlib.pyplot as plt\nZ = (Xbar - binom.mean(10, 0.3))/(binom.std(10, 0.3)/np.sqrt(n))\nplt.hist(Z, bins = 10, density = True) # To have density values, i.e. the frequences\nx = np.linspace(min(Z), max(Z), 100)\nplt.plot(x, norm.pdf(x), color = 'r')\nplt.show()",
    "crumbs": [
      "Labs - Solutions",
      "Lab 6 - Solutions"
    ]
  },
  {
    "objectID": "Lab6-solutions.html#section-8",
    "href": "Lab6-solutions.html#section-8",
    "title": "Lab 6 - Solutions",
    "section": "2.3 ",
    "text": "2.3 \n\nGenerate \\(K=10000\\) samples of \\(n=50\\) exponential random variables with the parameter \\(\\lambda=0.1\\). Use random_state equal to loop index as before. Calculate \\(p\\approx\\mathbb{P}(\\overline{X}_{50}&lt;9.5)\\) from the frequency of the corresponding events. Calculate the approximate value \\(q=\\Phi(z)\\) of \\(\\mathbb{P}(\\overline{X}_{50}&lt;10)\\) using the central limit theorem. (Don’t forget that expon from scipy.stats has the parameter scale inverse to \\(\\lambda\\).)\n\n\nCode\nfrom scipy.stats import expon, norm\nn = 50\nK = 10000\nla = 0.1\nXbar = np.array([np.mean(expon.rvs(scale = 1/la, size = n, random_state = i)) for i in range(K)])\np = sum(Xbar &lt; 9.5)/K\nz = (9.5 - expon.mean(scale = 1/la))/(expon.std(scale = 1/la)/np.sqrt(n))\nq = norm.cdf(z)\n\n\nCheck the difference:\n\nnp.abs(p-q)\n\n0.01326319508411844",
    "crumbs": [
      "Labs - Solutions",
      "Lab 6 - Solutions"
    ]
  },
  {
    "objectID": "Lab5-solutions.html",
    "href": "Lab5-solutions.html",
    "title": "Lab 5 - Solutions",
    "section": "",
    "text": "Continuous probability distributions in Python can be treated similarly to the discrete distributions. For example, for the uniform distribution we start with\n\nfrom scipy.stats import uniform\n\nWe know that a uniform random variables \\(X\\) depends on two parameters \\(a\\) and \\(b\\), namely, \\(X\\sim U(a,b)\\) is distributed uniformly on a segment \\([a,b]\\). The length of this segment is \\(b-a\\). In Python, the uniform distribution also depends on two parameters: loc = a and scale = b - a. The default values (which are used if these parameters are omitted) are loc = 0 and scale = 1 that corresponds to \\(U(0,1)\\).\n\n\nThe PDF \\(f_X(x)\\) is then 1/scale on \\([a,b]\\) and \\(0\\) otherwise. For example, for \\(X\\sim(1,6)\\), we can calculate\n\nuniform.pdf(3, loc = 1, scale = 5)\n\n0.2\n\n\nthat is \\(\\frac15\\), whereas,\n\nuniform.pdf(7, loc = 1, scale = 5)\n\n0.0\n\n\nas \\(7\\notin[1,6]\\).\nWe can also plot the graph of the PDF for \\(X\\sim U(1,6)\\), e.g. we plot it for \\(x\\in[-2,9]\\):\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nx = np.linspace(-2, 9, 1000)\nplt.plot(x, uniform.pdf(x, loc = 1, scale = 5))\nplt.show()\n\n\n\n\n\n\n\n\nNot that the jumps at \\(x=1\\) and \\(x=6\\) are shown by vertical segments.\n\n\n\nSimilarly, CDF \\(F_X(x)\\) of \\(X\\sim U(a,b)\\) can be calculated by using the command uniform.cdf(x, loc = a, scale = b - a).\n\n\n\n\nPlot the graph of \\(F_X(x)\\) for \\(X\\sim U(1,6)\\) on the interval \\([-2,9]\\). Check the output:\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\nx = np.linspace(-2, 9, 1000)\nplt.plot(x, uniform.cdf(x, loc = 1, scale = 5))\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\\(n\\) values of the random variable \\(X\\sim U(a,b)\\) can be generated using the command uniform.rvs(size = n, loc = a, scale = b - a). As we discussed on Lab 3, if we want to fix the output, we use the key random_state, for example, the following output\n\nuniform.rvs(size = 3, loc = 1, scale = 5, random_state = 1)\n\narray([3.08511002, 4.60162247, 1.00057187])\n\n\nwill be the same each time you run the code, whereas if you omit random_state = 1 the result will be different every time you run the code (try!)\n\n\n\n\n\nAssign to variable x the array of \\(10^6\\) random values uniformly distributed on \\([0,1]\\), fix the random state equal to 123. Calculate the mean of \\(x\\). Check the output.\n\n\nCode\nx = uniform.rvs(size = 10 ** 6, random_state = 123) # Since loc = a = 0, scale = b-a = 1, we may omit their values\nnp.mean(x)\n\n\n0.49993343872814583\n\n\nAs you can see the output is pretty close to \\(0.5 =  \\frac{0+1}{2}=\\mathbb{E}(X)\\).",
    "crumbs": [
      "Labs - Solutions",
      "Lab 5 - Solutions"
    ]
  },
  {
    "objectID": "Lab5-solutions.html#section",
    "href": "Lab5-solutions.html#section",
    "title": "Lab 5 - Solutions",
    "section": "",
    "text": "Plot the graph of \\(F_X(x)\\) for \\(X\\sim U(1,6)\\) on the interval \\([-2,9]\\). Check the output:\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\nx = np.linspace(-2, 9, 1000)\nplt.plot(x, uniform.cdf(x, loc = 1, scale = 5))\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\\(n\\) values of the random variable \\(X\\sim U(a,b)\\) can be generated using the command uniform.rvs(size = n, loc = a, scale = b - a). As we discussed on Lab 3, if we want to fix the output, we use the key random_state, for example, the following output\n\nuniform.rvs(size = 3, loc = 1, scale = 5, random_state = 1)\n\narray([3.08511002, 4.60162247, 1.00057187])\n\n\nwill be the same each time you run the code, whereas if you omit random_state = 1 the result will be different every time you run the code (try!)",
    "crumbs": [
      "Labs - Solutions",
      "Lab 5 - Solutions"
    ]
  },
  {
    "objectID": "Lab5-solutions.html#section-1",
    "href": "Lab5-solutions.html#section-1",
    "title": "Lab 5 - Solutions",
    "section": "",
    "text": "Assign to variable x the array of \\(10^6\\) random values uniformly distributed on \\([0,1]\\), fix the random state equal to 123. Calculate the mean of \\(x\\). Check the output.\n\n\nCode\nx = uniform.rvs(size = 10 ** 6, random_state = 123) # Since loc = a = 0, scale = b-a = 1, we may omit their values\nnp.mean(x)\n\n\n0.49993343872814583\n\n\nAs you can see the output is pretty close to \\(0.5 =  \\frac{0+1}{2}=\\mathbb{E}(X)\\).",
    "crumbs": [
      "Labs - Solutions",
      "Lab 5 - Solutions"
    ]
  },
  {
    "objectID": "Lab5-solutions.html#section-2",
    "href": "Lab5-solutions.html#section-2",
    "title": "Lab 5 - Solutions",
    "section": "2.1 ",
    "text": "2.1 \n\nGenerate \\(10^6\\) values of the random variable \\(X\\sim \\mathrm{Exp}(0.2)\\), using the random_state key equal to 12. Calculate the variance of the generated values, using the formula\n\\[\n\\mathrm{Var}(X) = \\mathbb{E}(X^2)- \\bigl(\\mathbb{E}(X)\\bigr)^2\n\\]\nCheck the answer.\n\n\nCode\nx = expon.rvs(size = 10**6, scale = 1/0.2, random_state = 12)\nnp.mean(x ** 2) - (np.mean(x)) ** 2\n\n\n25.04882818201701\n\n\nYou can also np.var command. Check the output in this case:\n\n\nCode\nnp.var(x)\n\n\n25.048828182017044\n\n\nThe results are almost identical (there is always some numerical error). Moreover, the result is close to the theoretical value \\(\\frac1{0.2^2}=25\\).\n\n\nPercentiles\nAnother important function available for all random variables in scipy.stats module is ppf, which provides percentiles. By the definition, for a random variable \\(X\\) and for any \\(q\\in[0,1]\\), the \\(q\\)-percentile of \\(X\\) is the number \\(a\\) such that\n\\[\nF_X(a) = \\mathbb{P}(X\\leq a) = q.\n\\]\nIn other words, the percentile is the inverse function to CDF.\nFor example, for \\(X\\sim \\mathrm{Exp}(0.2)\\),\n\na = expon.ppf(0.3, scale = 1/0.2)\na\n\n1.7833747196936622\n\n\nis \\(0.3\\)-percentile of \\(X\\), and then we can see that\n\nexpon.cdf(a, scale = 1/0.2)\n\n0.30000000000000004\n\n\nis effectively the initial \\(0.3\\).",
    "crumbs": [
      "Labs - Solutions",
      "Lab 5 - Solutions"
    ]
  },
  {
    "objectID": "Lab5-solutions.html#section-3",
    "href": "Lab5-solutions.html#section-3",
    "title": "Lab 5 - Solutions",
    "section": "2.2 ",
    "text": "2.2 \n\nLet \\(X\\sim \\mathrm{Exp}(0.7)\\). Find \\(b\\) such that\n\\[\n\\mathbb{P}(1 \\leq X \\leq b) = 0.4.\n\\]\nHint: use first the formula\n\\[\n\\mathbb{P}(a \\leq X \\leq b) = F_X(b) - F_X(a).\n\\]\nCheck the answer:\n\n\nCode\n# F(c) - F(1) = 0.4,\n# then F(c) = F(1) + 0.4\nq = expon.cdf(1, scale = 1/0.7) + 0.4\nexpon.ppf(q, scale = 1/0.7)\n\n\n3.3390409771454475",
    "crumbs": [
      "Labs - Solutions",
      "Lab 5 - Solutions"
    ]
  },
  {
    "objectID": "Lab5-solutions.html#section-4",
    "href": "Lab5-solutions.html#section-4",
    "title": "Lab 5 - Solutions",
    "section": "3.1 ",
    "text": "3.1 \n\nPlot the graph of CDF for \\(X\\sim \\mathcal{N}(2,3^2)\\) (also on the interval \\(x\\in(-10,14)\\)). Use green colour and label the axis with \\(x\\) and \\(y=F(x)\\).\n\n\nCode\nx = np.linspace(-10, 14, 1000)\ny = norm.cdf(x, loc = 2, scale = 3)\nplt.plot(x, y, color = 'green')\nplt.xlabel('x')\nplt.ylabel('y=F(x)')\nplt.show()",
    "crumbs": [
      "Labs - Solutions",
      "Lab 5 - Solutions"
    ]
  },
  {
    "objectID": "Lab5-solutions.html#section-5",
    "href": "Lab5-solutions.html#section-5",
    "title": "Lab 5 - Solutions",
    "section": "3.2 ",
    "text": "3.2 \n\nCalculate the probability that a randomly selected individual has a height between \\(160\\) cm and \\(170\\) cm, given that the population mean height is \\(165.5\\) cm and the standard deviation is \\(10.2\\) cm, and that the heights follow the normal distribution.\nSub-task 1: calculate the answer using functions for general normal random variable \\(X\\sim \\mathcal{N}(\\mu,\\sigma^2)\\). Check the answer:\n\n\nCode\nnorm.cdf(170, loc = 165.5, scale = 10.2) - norm.cdf(160, loc = 165.5, scale = 10.2)\n\n\n0.37558835807069463\n\n\nSub-task 2: recalculate the answer using functions for standard normal random variable \\(Z\\sim \\mathcal{N}(0,1)\\). Check the answer:\n\n\nCode\nnorm.cdf((170-165.5)/10.2) - norm.cdf((160-165.5)/10.2)\n\n\n0.37558835807069463",
    "crumbs": [
      "Labs - Solutions",
      "Lab 5 - Solutions"
    ]
  },
  {
    "objectID": "Lab5-solutions.html#section-6",
    "href": "Lab5-solutions.html#section-6",
    "title": "Lab 5 - Solutions",
    "section": "3.3 ",
    "text": "3.3 \n\nLet \\(X\\sim\\mathcal{N}(12,5^2)\\). Find \\(c\\) such that\n\\[\n\\mathbb{P}(c \\leq X \\leq 15) = 0.5.\n\\]\nCheck the answer.\n\n\nCode\n# F(15) - F(c) = 0.5\n# F(c) = F(15) - 0.5\nq = norm.cdf(15, loc = 12, scale = 5) - 0.5\nnorm.ppf(q, loc = 12, scale = 5)\n\n\n8.235364852055193",
    "crumbs": [
      "Labs - Solutions",
      "Lab 5 - Solutions"
    ]
  },
  {
    "objectID": "Lab5-solutions.html#section-7",
    "href": "Lab5-solutions.html#section-7",
    "title": "Lab 5 - Solutions",
    "section": "3.4 ",
    "text": "3.4 \n\nGenerate \\(10^6\\) values of standard normal random variable. Check their standard deviation (use np.std function) and ensure that the result is close to \\(1\\).\n\n\nCode\nx = norm.rvs(size = 10 ** 6)\nnp.std(x)\n\n\n1.0016138810315194\n\n\n(you may get a different answer, as we didn’t fix random_state here).",
    "crumbs": [
      "Labs - Solutions",
      "Lab 5 - Solutions"
    ]
  },
  {
    "objectID": "Lab4-solutions.html",
    "href": "Lab4-solutions.html",
    "title": "Lab 4 - Solutions",
    "section": "",
    "text": "Recall that the linear regression provides the “best line” that reflects the relation between two sets of data. Namely, let \\(X=(x_1,\\ldots,x_n)\\) and \\(Y=(y_1,\\ldots,y_n)\\) be vectors (arrays) of data. We define\n\\[\n\\begin{aligned}\n\\bar{x}& = \\frac1n \\sum_{i=1}^n x_i,\\\\\nS_{xx} &= \\sum_{i=1}^n(x_i-\\bar{x})^2=\\sum_{i=1}^nx_i^2-n\\bar{x}^2,\\\\\nS_{xy}&=\\sum_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y})=\\sum_{i=1}^nx_iy_i-n\\bar{x}\\bar{y},\\\\\nS_{yy}&= \\sum_{i=1}^n(y_i-\\bar{y})^2=\\sum_{i=1}^ny_i^2-n\\bar{y}^2.\n\\end{aligned}\n\\]\nThen the best fit line is\n\\[\n\\hat{y}=\\hat{\\beta}_0+\\hat{\\beta}_1 x,\n\\]\nwhere\n\\[\n\\begin{aligned}\n\\hat{\\beta}_1 &= \\dfrac{S_{xy}}{S_{xx}},\\\\\n\\hat{\\beta}_0 &= \\bar{y}-\\hat{\\beta}_1\\bar{x}.\n\\end{aligned}\n\\]\nThe strength of a linear relationship between the variables can be measured by the Pearson correlation coefficient (or just the correlation coefficient) which is given by\n\\[\nr=\\dfrac{S_{xy}}{\\sqrt{S_{xx}S_{yy}}}.\n\\]\nWe know that \\(-1\\leq r\\leq 1\\) and we say that\n\\[\n\\begin{aligned}\n|r|&gt;0.7 & \\quad \\text{means strong correlation}\\\\\n0.7\\geq |r|&gt;0.4& \\quad \\text{means moderate correlation}\\\\\n|r|\\leq 0.4 & \\quad \\text{means weak correlation}\n\\end{aligned}\n\\]\n\n\n\nDownload file Birthweight.csv from Canvas (see Week 4 section in Modules) and upload it to Anaconda.com/app. Assign it to the dataframe named df. Show the first rows of df.\nHint: to import CSV file, use commands discussed in Lab 3. Don’t forget about pandas library.\nYou should get the following output\n\n\nCode\nimport pandas as pd\ndf = pd.read_csv(\"Birthweight.csv\")\ndf.head()\n\n\n\n\n\n\n\n\n\nid\nheadcircumference\nlength\nBirthweight\nGestation\nsmoker\nmotherage\nmnocig\nmheight\nmppwt\nfage\nfedyrs\nfnocig\nfheight\nlowbwt\nmage35\nLowBirthWeight\nQCL_1\n\n\n\n\n0\n1313\n12\n17\n5.8\n33\n0\n24\n0\n58\n99\n26\n16\n0\n66\n1\n0\nLow\n1\n\n\n1\n431\n12\n19\n4.2\n33\n1\n20\n7\n63\n109\n20\n10\n35\n71\n1\n0\nLow\n1\n\n\n2\n808\n13\n19\n6.4\n34\n0\n26\n0\n65\n140\n25\n12\n25\n69\n0\n0\nNormal\n2\n\n\n3\n300\n12\n18\n4.5\n35\n1\n41\n7\n65\n125\n37\n14\n25\n68\n1\n1\nLow\n1\n\n\n4\n516\n13\n18\n5.8\n35\n1\n20\n35\n67\n125\n23\n12\n50\n73\n1\n0\nLow\n2\n\n\n\n\n\n\n\n\nAs you can see, this dataframe contains the data about newborns and their parents. (Here values of headcircumference and length are in inches and Birthweight is in pounds.)\nWe will study dependence of newborn’s weights on their lengths.\n\n\n\n\nPlot a scatter plot making length data on the horizontal axes and Birthweight data on the vertical axes. Label axes appropriately, and show in labels the units (in and lb).\nHint: use commands discussed in Lab 3. Don’t forget about matplotlib.pyplot module. Note also that matplotlib allows to use Pandas series (e.g. dataframe columns), it’s not necessary to convert them into Numpy arrays using .to_numpy() command.\n\n\nCode\nimport matplotlib.pyplot as plt\nplt.scatter(df['length'], df['Birthweight'])\nplt.xlabel('Length (in)')\nplt.ylabel('Weight (lb)')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nAt first, we calculate the regression line manually, using the formulas above.\n\n\n\n\n\nConvert columns length and Birthweight to Numpy arrays x and y, respectively. Assign \\(\\bar{x}\\) and \\(\\bar{y}\\) to mx and my, respectively. Note that you may use either mean or np.mean functions.\n\n\nCode\nimport numpy as np\nx = df['length'].to_numpy()\ny = df['Birthweight'].to_numpy()\nmx = np.mean(x)\nmy = np.mean(y)\n\n\nCheck your answer:\n\n[mx, my]\n\n[19.928571428571427, 7.264285714285713]\n\n\n\n\n\n\n\nAssign values of \\(S_{xx}, S_{xy}, S_{yy}\\) to variables sxx, sxy, and syy, respectively (use the formulas at the beginning of this Lab). Note that you can use functions sum or np.sum, and remember about vector operations in Python.\n\n\nCode\nsxx = np.sum((x-mx)**2)\nsxy = np.sum((x-mx)*(y-my))\nsyy= np.sum((y-my)**2)\n\n\nCheck the answer:\n\n[sxx, sxy, syy]\n\n[50.785714285714285, 42.292857142857144, 72.49642857142857]\n\n\n\n\n\n\n\nAssign values of \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_0\\) to variables b1 and b0, respectively. Assign the Pearson regression coefficient to variable r. (To find the square root, you may use np.sqrt function.)\n\n\nCode\nb1 = sxy/sxx\nb0 = my - b1 * mx\nr = sxy / np.sqrt(sxx * syy)\n\n\nCheck the answer:\n\n[b0, b1, r]\n\n[-9.331645569620253, 0.8327707454289733, 0.6970082792022007]\n\n\n\n\n\nInstead of all these calculations, we can also use linregress class from scipy.stats module:\n\nfrom scipy.stats import linregress\nlinregress(x,y)\n\nLinregressResult(slope=0.8327707454289732, intercept=-9.331645569620253, rvalue=0.6970082792022005, pvalue=2.9301969030656806e-07, stderr=0.13546119087983002, intercept_stderr=2.7036545086005135)\n\n\nYou may notice that it gives the same answers (up to a little calculation error), where slope stands for b1 (that is indeed the slope of \\(\\hat{y}=\\hat{\\beta}_0+\\hat{\\beta}_1 x\\)), intercept stands for b0, and rvalue stands for r. You may access these values as follows:\n\nlr = linregress(x,y)\n[lr.intercept, lr.slope, lr.rvalue]\n\n[-9.331645569620253, 0.8327707454289732, 0.6970082792022005]\n\n\nthat is pretty simular to [b0, b1, r] calculated before.\nWe are going now to draw now the graph of the regression line on the scatter plot. For this, we create an array of values on the horizontal axes by dividing the interval between min(x) and max(x) by e.g. \\(100\\) parts (for this we will use np.linespace function), and calculate the values of the linear regression line at these points:\n\nimport matplotlib.pyplot as plt\nxvalues = np.linspace(min(x), max(x), 100)\nyvalues = lr.intercept + lr.slope * xvalues\nplt.plot(xvalues, yvalues, color = 'r')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCombine the regression line with the scatter plot to get the following output:\n\n\nCode\nimport matplotlib.pyplot as plt\nplt.scatter(x, y)\nxvalues = np.linspace(min(x), max(x), 100)\nyvalues = lr.intercept + lr.slope * xvalues\nplt.plot(xvalues, yvalues, color = 'r')\nplt.xlabel('Length (in)')\nplt.ylabel('Weight (lb)')\nplt.show()\n\n\n\n\n\n\n\n\n\nAs we can see, the regression line reflects the trend between weights and lengths, however, the values are “jumping” around the line. We could see that the correlation coefficient (see lr.rvalue or r) is not large:\n\nlr.rvalue\n\n0.6970082792022005\n\n\ni.e. we see here a moderate correlation.\n\n\n\n\n\nDownload now file Experience-Salary.csv which contains data on how the salary depends on experience. Repeat the previous steps to show the scatter plot together with the regression line:\n\n\nCode\ndf = pd.read_csv(\"Experience-Salary.csv\")\nx = df['exp(in months)'].to_numpy()\ny = df['salary(in thousands)'].to_numpy()\nlr = linregress(x,y)\nimport matplotlib.pyplot as plt\nplt.scatter(x, y)\nxvalues = np.linspace(min(x), max(x), 100)\nyvalues = lr.intercept + lr.slope * xvalues\nplt.plot(xvalues, yvalues, color = 'r')\nplt.xlabel('Experience (months)')\nplt.ylabel('Salary (thousands)')\nplt.show()\n\n\n\n\n\n\n\n\n\nYou can see that here the regression line fits the data better. Indeed, in this case the correlation is higher:\n\nlr.rvalue #If you kept the notation lr for linregress object.\n\n0.8109692945840652",
    "crumbs": [
      "Labs - Solutions",
      "Lab 4 - Solutions"
    ]
  },
  {
    "objectID": "Lab4-solutions.html#section",
    "href": "Lab4-solutions.html#section",
    "title": "Lab 4 - Solutions",
    "section": "",
    "text": "Download file Birthweight.csv from Canvas (see Week 4 section in Modules) and upload it to Anaconda.com/app. Assign it to the dataframe named df. Show the first rows of df.\nHint: to import CSV file, use commands discussed in Lab 3. Don’t forget about pandas library.\nYou should get the following output\n\n\nCode\nimport pandas as pd\ndf = pd.read_csv(\"Birthweight.csv\")\ndf.head()\n\n\n\n\n\n\n\n\n\nid\nheadcircumference\nlength\nBirthweight\nGestation\nsmoker\nmotherage\nmnocig\nmheight\nmppwt\nfage\nfedyrs\nfnocig\nfheight\nlowbwt\nmage35\nLowBirthWeight\nQCL_1\n\n\n\n\n0\n1313\n12\n17\n5.8\n33\n0\n24\n0\n58\n99\n26\n16\n0\n66\n1\n0\nLow\n1\n\n\n1\n431\n12\n19\n4.2\n33\n1\n20\n7\n63\n109\n20\n10\n35\n71\n1\n0\nLow\n1\n\n\n2\n808\n13\n19\n6.4\n34\n0\n26\n0\n65\n140\n25\n12\n25\n69\n0\n0\nNormal\n2\n\n\n3\n300\n12\n18\n4.5\n35\n1\n41\n7\n65\n125\n37\n14\n25\n68\n1\n1\nLow\n1\n\n\n4\n516\n13\n18\n5.8\n35\n1\n20\n35\n67\n125\n23\n12\n50\n73\n1\n0\nLow\n2\n\n\n\n\n\n\n\n\nAs you can see, this dataframe contains the data about newborns and their parents. (Here values of headcircumference and length are in inches and Birthweight is in pounds.)\nWe will study dependence of newborn’s weights on their lengths.",
    "crumbs": [
      "Labs - Solutions",
      "Lab 4 - Solutions"
    ]
  },
  {
    "objectID": "Lab4-solutions.html#section-1",
    "href": "Lab4-solutions.html#section-1",
    "title": "Lab 4 - Solutions",
    "section": "",
    "text": "Plot a scatter plot making length data on the horizontal axes and Birthweight data on the vertical axes. Label axes appropriately, and show in labels the units (in and lb).\nHint: use commands discussed in Lab 3. Don’t forget about matplotlib.pyplot module. Note also that matplotlib allows to use Pandas series (e.g. dataframe columns), it’s not necessary to convert them into Numpy arrays using .to_numpy() command.\n\n\nCode\nimport matplotlib.pyplot as plt\nplt.scatter(df['length'], df['Birthweight'])\nplt.xlabel('Length (in)')\nplt.ylabel('Weight (lb)')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nAt first, we calculate the regression line manually, using the formulas above.",
    "crumbs": [
      "Labs - Solutions",
      "Lab 4 - Solutions"
    ]
  },
  {
    "objectID": "Lab4-solutions.html#section-2",
    "href": "Lab4-solutions.html#section-2",
    "title": "Lab 4 - Solutions",
    "section": "",
    "text": "Convert columns length and Birthweight to Numpy arrays x and y, respectively. Assign \\(\\bar{x}\\) and \\(\\bar{y}\\) to mx and my, respectively. Note that you may use either mean or np.mean functions.\n\n\nCode\nimport numpy as np\nx = df['length'].to_numpy()\ny = df['Birthweight'].to_numpy()\nmx = np.mean(x)\nmy = np.mean(y)\n\n\nCheck your answer:\n\n[mx, my]\n\n[19.928571428571427, 7.264285714285713]",
    "crumbs": [
      "Labs - Solutions",
      "Lab 4 - Solutions"
    ]
  },
  {
    "objectID": "Lab4-solutions.html#section-3",
    "href": "Lab4-solutions.html#section-3",
    "title": "Lab 4 - Solutions",
    "section": "",
    "text": "Assign values of \\(S_{xx}, S_{xy}, S_{yy}\\) to variables sxx, sxy, and syy, respectively (use the formulas at the beginning of this Lab). Note that you can use functions sum or np.sum, and remember about vector operations in Python.\n\n\nCode\nsxx = np.sum((x-mx)**2)\nsxy = np.sum((x-mx)*(y-my))\nsyy= np.sum((y-my)**2)\n\n\nCheck the answer:\n\n[sxx, sxy, syy]\n\n[50.785714285714285, 42.292857142857144, 72.49642857142857]",
    "crumbs": [
      "Labs - Solutions",
      "Lab 4 - Solutions"
    ]
  },
  {
    "objectID": "Lab4-solutions.html#section-4",
    "href": "Lab4-solutions.html#section-4",
    "title": "Lab 4 - Solutions",
    "section": "",
    "text": "Assign values of \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_0\\) to variables b1 and b0, respectively. Assign the Pearson regression coefficient to variable r. (To find the square root, you may use np.sqrt function.)\n\n\nCode\nb1 = sxy/sxx\nb0 = my - b1 * mx\nr = sxy / np.sqrt(sxx * syy)\n\n\nCheck the answer:\n\n[b0, b1, r]\n\n[-9.331645569620253, 0.8327707454289733, 0.6970082792022007]\n\n\n\n\n\nInstead of all these calculations, we can also use linregress class from scipy.stats module:\n\nfrom scipy.stats import linregress\nlinregress(x,y)\n\nLinregressResult(slope=0.8327707454289732, intercept=-9.331645569620253, rvalue=0.6970082792022005, pvalue=2.9301969030656806e-07, stderr=0.13546119087983002, intercept_stderr=2.7036545086005135)\n\n\nYou may notice that it gives the same answers (up to a little calculation error), where slope stands for b1 (that is indeed the slope of \\(\\hat{y}=\\hat{\\beta}_0+\\hat{\\beta}_1 x\\)), intercept stands for b0, and rvalue stands for r. You may access these values as follows:\n\nlr = linregress(x,y)\n[lr.intercept, lr.slope, lr.rvalue]\n\n[-9.331645569620253, 0.8327707454289732, 0.6970082792022005]\n\n\nthat is pretty simular to [b0, b1, r] calculated before.\nWe are going now to draw now the graph of the regression line on the scatter plot. For this, we create an array of values on the horizontal axes by dividing the interval between min(x) and max(x) by e.g. \\(100\\) parts (for this we will use np.linespace function), and calculate the values of the linear regression line at these points:\n\nimport matplotlib.pyplot as plt\nxvalues = np.linspace(min(x), max(x), 100)\nyvalues = lr.intercept + lr.slope * xvalues\nplt.plot(xvalues, yvalues, color = 'r')\nplt.show()",
    "crumbs": [
      "Labs - Solutions",
      "Lab 4 - Solutions"
    ]
  },
  {
    "objectID": "Lab4-solutions.html#section-5",
    "href": "Lab4-solutions.html#section-5",
    "title": "Lab 4 - Solutions",
    "section": "",
    "text": "Combine the regression line with the scatter plot to get the following output:\n\n\nCode\nimport matplotlib.pyplot as plt\nplt.scatter(x, y)\nxvalues = np.linspace(min(x), max(x), 100)\nyvalues = lr.intercept + lr.slope * xvalues\nplt.plot(xvalues, yvalues, color = 'r')\nplt.xlabel('Length (in)')\nplt.ylabel('Weight (lb)')\nplt.show()\n\n\n\n\n\n\n\n\n\nAs we can see, the regression line reflects the trend between weights and lengths, however, the values are “jumping” around the line. We could see that the correlation coefficient (see lr.rvalue or r) is not large:\n\nlr.rvalue\n\n0.6970082792022005\n\n\ni.e. we see here a moderate correlation.",
    "crumbs": [
      "Labs - Solutions",
      "Lab 4 - Solutions"
    ]
  },
  {
    "objectID": "Lab4-solutions.html#section-6",
    "href": "Lab4-solutions.html#section-6",
    "title": "Lab 4 - Solutions",
    "section": "",
    "text": "Download now file Experience-Salary.csv which contains data on how the salary depends on experience. Repeat the previous steps to show the scatter plot together with the regression line:\n\n\nCode\ndf = pd.read_csv(\"Experience-Salary.csv\")\nx = df['exp(in months)'].to_numpy()\ny = df['salary(in thousands)'].to_numpy()\nlr = linregress(x,y)\nimport matplotlib.pyplot as plt\nplt.scatter(x, y)\nxvalues = np.linspace(min(x), max(x), 100)\nyvalues = lr.intercept + lr.slope * xvalues\nplt.plot(xvalues, yvalues, color = 'r')\nplt.xlabel('Experience (months)')\nplt.ylabel('Salary (thousands)')\nplt.show()\n\n\n\n\n\n\n\n\n\nYou can see that here the regression line fits the data better. Indeed, in this case the correlation is higher:\n\nlr.rvalue #If you kept the notation lr for linregress object.\n\n0.8109692945840652",
    "crumbs": [
      "Labs - Solutions",
      "Lab 4 - Solutions"
    ]
  },
  {
    "objectID": "Lab4-solutions.html#section-7",
    "href": "Lab4-solutions.html#section-7",
    "title": "Lab 4 - Solutions",
    "section": "2.1 ",
    "text": "2.1 \n\nChange the code as explained and get the following summary.\n\n\nCode\nreg = smf.logit('admitted ~ gmat + gpa', data = df).fit()\nreg.summary()\n\n\nOptimization terminated successfully.\n         Current function value: 0.322141\n         Iterations 8\n\n\n\nLogit Regression Results\n\n\nDep. Variable:\nadmitted\nNo. Observations:\n29\n\n\nModel:\nLogit\nDf Residuals:\n26\n\n\nMethod:\nMLE\nDf Model:\n2\n\n\nDate:\nFri, 24 Oct 2025\nPseudo R-squ.:\n0.5348\n\n\nTime:\n13:35:51\nLog-Likelihood:\n-9.3421\n\n\nconverged:\nTrue\nLL-Null:\n-20.084\n\n\nCovariance Type:\nnonrobust\nLLR p-value:\n2.162e-05\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n-22.0171\n9.232\n-2.385\n0.017\n-40.111\n-3.923\n\n\ngmat\n0.0150\n0.014\n1.052\n0.293\n-0.013\n0.043\n\n\ngpa\n3.7604\n2.003\n1.877\n0.060\n-0.166\n7.686\n\n\n\n\n\n\nAgain, the coefficients are available using (in the previous notations) reg.params. As before, Intercept stands for \\(\\hat{\\beta}_0\\), gmat stands for \\(\\hat{\\beta}_1\\), and also gpa stands for \\(\\hat{\\beta}_2\\) in\n\\[\n\\mathrm{logit} (p) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2\n\\]\nand hence\n\\[\np = \\frac{e^{\\hat{\\beta}_0+\\hat{\\beta}_1 x_1+\\hat{\\beta}_2 x_2}}{1+e^{\\hat{\\beta}_0+\\hat{\\beta}_1 x_1+\\hat{\\beta}_2 x_2}}\n\\]\nWe can show how \\(p\\) separates the values (some lines of the code may be new for you - it’s just for your information, you are not required to learn them):\n\nx1 = df['gmat'].to_numpy()\nx2 = df['gpa'].to_numpy()\np = df['admitted'].to_numpy()\nfig = plt.figure()\nax = fig.add_subplot(1, 1, 1, projection='3d', computed_zorder=False)\nx1values = np.linspace(min(x1), max(x1), 100)\nx2values = np.linspace(min(x2), max(x2), 100)\n[X1, X2] = np.meshgrid(x1values, x2values)\nb0 = reg.params.iloc[0]\nb1 = reg.params.iloc[1]\nb2 = reg.params.iloc[2]\npvalues = np.exp(b0 + b1 * X1 + b2 * X2)/(1 + np.exp(b0 + b1 * X1 + b2 * X2))\nax.plot_surface(X1, X2, pvalues, color = 'r', alpha = 0.4)\nax.scatter(x1, x2, p)\nplt.show()\n\n\n\n\n\n\n\n\nAs you can see, the red graph (surface) of \\(p\\) separates values of \\(0\\) and \\(1\\). Again, we may try to predict the admission for the student with data stored in df_test. We assigned to gmat the corresponding mark, now we do the same for gpa and calculate p for these two values. As you can see, the result is much closer to \\(0\\), hence, we are more confident in our (correct) prediction that the student would not be admitted.\n\ngpa = df_test['gpa']\nnp.exp(b0 + b1 * gmat + b2 * gpa)/(1 + np.exp(b0 + b1 * gmat + b2 * gpa))\n\n0.07118806995711177\n\n\n\nLogistic regression for 3 independent variables\nNow, we consider the dependence of admitted on all three values: gmat, gpa, and work_experience. Surely, in this case, we will not be able to draw \\(p\\) (as it would be a 4-dimensional diagram), but we can calculate \\(\\hat{\\beta}_0,\\hat{\\beta}_1,\\hat{\\beta}_2,\\hat{\\beta}_3\\).",
    "crumbs": [
      "Labs - Solutions",
      "Lab 4 - Solutions"
    ]
  },
  {
    "objectID": "Lab4-solutions.html#section-8",
    "href": "Lab4-solutions.html#section-8",
    "title": "Lab 4 - Solutions",
    "section": "2.2 ",
    "text": "2.2 \n\nFind the coefficients of\n\\[\n\\mathrm{logit} (p) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2 + \\hat{\\beta}_3 x_3\n\\]\n\n\nCode\nreg = smf.logit('admitted ~ gmat + gpa + work_experience', data = df).fit()\n\n\nOptimization terminated successfully.\n         Current function value: 0.255306\n         Iterations 8\n\n\nCheck your answer:\n\nreg.params\n\nIntercept         -16.182243\ngmat                0.002624\ngpa                 3.258770\nwork_experience     0.994371\ndtype: float64",
    "crumbs": [
      "Labs - Solutions",
      "Lab 4 - Solutions"
    ]
  },
  {
    "objectID": "Lab4-solutions.html#section-9",
    "href": "Lab4-solutions.html#section-9",
    "title": "Lab 4 - Solutions",
    "section": "2.3 ",
    "text": "2.3 \n\nAssign to w_exp the work experience value for the student from df_test and calculate the function\n\\[\np = \\frac{e^{\\hat{\\beta}_0+\\hat{\\beta}_1 x_1+\\hat{\\beta}_2 x_2+\\hat{\\beta}_3 x_3}}{1+e^{\\hat{\\beta}_0+\\hat{\\beta}_1 x_1+\\hat{\\beta}_2 x_2+\\hat{\\beta}_3 x_3}}\n\\]\nfor that student. Check the answer:\n\n\nCode\nw_exp = df_test['work_experience']\nb0, b1, b2, b3 = reg.params #Note the trick\nnp.exp(b0 + b1 * gmat + b2 * gpa + b3 * w_exp)/(1 + np.exp(b0 + b1 * gmat + b2 * gpa + b3 * w_exp))\n\n\n0.3133331286316772\n\n\n\nAs you can see, the information about a relatively high work experience (5 years in this case), increased chances to be admitted, though the non-admission is still more likely.\nNote that the rest of information from summary actually explains the level of certainty we may have in the future prediction (we do not consider this now). Note also that, in practice, one predicts outcomes for a number of students (the dataframe df_test would contain a lot of rows), and the prediction is “good” if one predicted correctly for a big percentage of them.",
    "crumbs": [
      "Labs - Solutions",
      "Lab 4 - Solutions"
    ]
  },
  {
    "objectID": "Lab3-solutions.html",
    "href": "Lab3-solutions.html",
    "title": "Lab 3 - Solutions",
    "section": "",
    "text": "Module scipy.stats of the famous library scipy provides various tools to work with probability and statistics in Python. Today, we consider several discrete distributions; to work with them will use the same structure of commands. In particular, any distribution will have the following methods:\n\nrvs - to generate (several) values of a random variable \\(X\\)\npmf - to calculate probability mass function at any \\(k\\in\\mathbb{R}\\)\n\n\\[\np_X(k)=\\mathbb{P}(X=k)\n\\]\n\ncdf - to calculate the cumulative distribuion function\n\n\\[\nF_X(x)=\\sum_{k\\leq x} p_X(k)=\\mathbb{P}(X\\leq x)\n\\]\nrecall that\n\\[\n\\mathbb{P}(a&lt;X\\leq b) = F_X(b)-F_X(a)\n\\]\n\nmean - to calculate the mathematical expectation (mean) \\(\\mathbb{E}(X)\\)\nvar - to calculate the varaince \\(\\mathrm{Var}(X)\\)\nstd - to calculate the standard deviation \\(\\sigma(X)=\\sqrt{\\mathrm{Var}(X)}\\)",
    "crumbs": [
      "Labs - Solutions",
      "Lab 3 - Solutions"
    ]
  },
  {
    "objectID": "Lab3-solutions.html#section",
    "href": "Lab3-solutions.html#section",
    "title": "Lab 3 - Solutions",
    "section": "2.1 ",
    "text": "2.1 \n\nLet \\(X\\sim Bin(27, 0.45)\\).\n\nCalculate \\(\\mathbb{P}(X\\leq 20)\\).\n\nSolution. \\(\\mathbb{P}(X\\leq 20) = F_X(20)\\), hence, we calculate binom.cdf at k=20 with parameters n=27 and p=0.45.\n\n\n\nCode\n binom.cdf(20, 27, 0.45)\n\n\n0.9994575733214396\n\n\nCalculate \\(\\mathbb{P}(X &lt; 10)\\).\n\nSolution. Since \\(X\\) takes only integer values, \\(X&lt;10\\) means that \\(X\\leq 9\\). Hence,\n\\[\n\\mathbb{P}(X &lt; 10) = \\mathbb{P}(X \\leq 9) = F_X(9).\n\\]\n\n\n\nCode\nbinom.cdf(9, 27, 0.45)\n\n\n0.15256903022954094\n\n\nCalculate \\(\\mathbb{P}(7&lt;X\\leq 13)\\).\n\nSolution. We use the formula\n\\[\n\\mathbb{P}(7&lt;X\\leq 13) = F_X(13)-F_X(7).\n\\]\n\n\n\nCode\nbinom.cdf(13, 27, 0.45)-binom.cdf(7, 27, 0.45)\n\n\n0.666671672666459\n\n\nCalculate \\(\\mathbb{P}(7\\leq X\\leq 13)\\).\n\nSolution. To apply the same formula, we need to rewrite the inequality in the form \\(6&lt;X\\leq 13\\).\n\n\n\nCode\nbinom.cdf(13, 27, 0.45)-binom.cdf(6, 27, 0.45)\n\n\n0.6879613472859423",
    "crumbs": [
      "Labs - Solutions",
      "Lab 3 - Solutions"
    ]
  },
  {
    "objectID": "Lab3-solutions.html#section-1",
    "href": "Lab3-solutions.html#section-1",
    "title": "Lab 3 - Solutions",
    "section": "2.2 ",
    "text": "2.2 \n\nA company manufactures light bulbs, and \\(95\\)% of them are of good quality, while the rest are defective. If a customer buys \\(50\\) light bulbs, what is the probability that:\n\nExactly \\(5\\) of them are defective?\n\nSolution. Since we are looking for defective bulbs, we consider \\(p=0.05\\) that is the probabilioty that a bulb is defective (\\(5\\)% of all bulbs). Hence, we need to find \\(p_X(5)\\) for \\(X\\sim Bin(50,0.05)\\).\n\n\n\nCode\nbinom.pmf(5, 50, 0.05)\n\n\n0.06584063715436628\n\n\nAt most \\(5\\) of them are defective?\n\nSolution. We need to find (again, for \\(X\\sim Bin(50,0.05)\\))\n\\[\n\\mathbb{P}(X\\leq 5) = F_X(5).\n\\]\n\n\n\nCode\nbinom.cdf(5, 50, 0.05)\n\n\n0.9622238270102227\n\n\nAt least \\(5\\) of them are defective?\n\nSolution. We need to find\n\\[\n\\mathbb{P}(X\\geq 5) = 1- \\mathbb{P}(X &lt; 5)=1- \\mathbb{P}(X \\leq {\\color{red}4})=1-F_X(4).\n\\]\n\n\n\nCode\n1-binom.cdf(4, 50, 0.05)\n\n\n0.10361681014414348\n\n\nFind the expected value of the number of defective bulbs and the standard deviation of this quantity.\n\nSolution. Here we just use the corresponding methods to calculate mean and std.\n\n\n\nCode\n[binom.mean(50, 0.05), binom.std(50, 0.05)]\n\n\n[2.5, 1.541103500742244]",
    "crumbs": [
      "Labs - Solutions",
      "Lab 3 - Solutions"
    ]
  },
  {
    "objectID": "Lab3-solutions.html#section-2",
    "href": "Lab3-solutions.html#section-2",
    "title": "Lab 3 - Solutions",
    "section": "3.1 ",
    "text": "3.1 \n\nLet \\(X\\sim Geom(0.4)\\). Calculate \\(\\mathbb{P}(7\\leq X &lt;10)\\).\n\nSolution. Similarly to the previous examples, we rewrite\n\\[\n\\mathbb{P}(7\\leq X &lt;10)=\\mathbb{P}(6 &lt;X \\leq 9)=F_X(9)-F_X(6).\n\\]\n\n\n\nCode\ngeom.cdf(9, 0.4)-geom.cdf(6, 0.4)\n\n\n0.03657830400000006",
    "crumbs": [
      "Labs - Solutions",
      "Lab 3 - Solutions"
    ]
  },
  {
    "objectID": "Lab3-solutions.html#section-3",
    "href": "Lab3-solutions.html#section-3",
    "title": "Lab 3 - Solutions",
    "section": "3.2 ",
    "text": "3.2 \n\nA lazy student has to take a quiz, where each question may have as an answer an integer number from \\(1\\) to \\(100\\) (different questions may have equal answers). Instead of preparation, the student is going to guess the answers. The quiz contains \\(20\\) questions. As soon as the student gives a correct answer, the quiz stops, and it is considered as a passed one. What is the probability that the student will pass the quiz?\nHint: the quiz may stop after either of \\(1,2,3,\\ldots,20\\) questions.\n\nSolution. The students needs to guess a number from \\(100\\) possible numbers, so the proability to answer a question correctly is \\(\\frac1{100}=0.01\\). If \\(X\\sim Geom(0.01)\\), then \\(X\\) models the number of the question when the first correct answer is made, and the quiz stops. Hence, \\(X\\) may be either of \\(1,2,3,\\ldots,20\\), i.e. we need to find \\(\\mathbb{P}(X\\leq 20)\\).\n\n\n\nCode\ngeom.cdf(20, 0.01)\n\n\n0.18209306240276918",
    "crumbs": [
      "Labs - Solutions",
      "Lab 3 - Solutions"
    ]
  },
  {
    "objectID": "Lab3-solutions.html#section-4",
    "href": "Lab3-solutions.html#section-4",
    "title": "Lab 3 - Solutions",
    "section": "4.1 ",
    "text": "4.1 \n\nLet \\(X\\sim NB(5, 0.3)\\). Find \\(\\mathbb{P}(X&gt;10)\\).\n\nSolution. We have, by using the probability of the complement event,\n\\[\n\\mathbb{P}(X&gt;10)=1-\\mathbb{P}(X\\leq 10)=1-F_X(10).\n\\]\n\n\n\nCode\n1-nbinom.cdf(10, 5, 0.3)\n\n\n0.5154910592268431",
    "crumbs": [
      "Labs - Solutions",
      "Lab 3 - Solutions"
    ]
  },
  {
    "objectID": "Lab3-solutions.html#section-5",
    "href": "Lab3-solutions.html#section-5",
    "title": "Lab 3 - Solutions",
    "section": "4.2 ",
    "text": "4.2 \n\nA lazy student has to take a quiz, where each question may have as an answer an integer number from \\(1\\) to \\(100\\) (different questions may have equal answers). Instead of preparation, the student is going to guess the answers. The quiz contains \\(20\\) questions. The quiz stops as soon as the student answers correctly \\(3\\) questions. What is the probability to pass the test for this student?\nHint: think on how many wrong answers could be made by the student to still pass the test.\n\nSolution. Let \\(X\\sim NB(3, 0.01)\\) be the number of wrong answers made by the student before he made the \\(3\\)rd correct answer. Then the total number of answered questions will be \\(X+3\\). One needs to have \\(X+3\\leq20\\) (as there are only \\(20\\) questions), i.e. \\(X\\leq 17\\). Hence, we need to find\n\\[\n\\mathbb{P}(X\\leq 17)=F_X(17).\n\\]\n\n\n\nCode\nnbinom.cdf(17, 3, 0.01)\n\n\n0.0010035761681001162",
    "crumbs": [
      "Labs - Solutions",
      "Lab 3 - Solutions"
    ]
  },
  {
    "objectID": "Lab3-solutions.html#section-6",
    "href": "Lab3-solutions.html#section-6",
    "title": "Lab 3 - Solutions",
    "section": "5.1 ",
    "text": "5.1 \n\nLet \\(X\\sim Po(0.4)\\). Find\n\\[\n\\sum_{n=4}^\\infty \\mathbb{P}(X=n).\n\\]\n\nSolution. Note that\n\\[\n\\begin{aligned}\n\\sum_{n=4}^\\infty \\mathbb{P}(X=n)&=\\mathbb{P}(X\\geq 4) =1-\\mathbb{P}(X&lt; 4)\\\\&=1-\\mathbb{P}(X\\leq 3)=1-F_X(3).\n\\end{aligned}\n\\]\n\n\n\nCode\n1 - poisson.cdf(3, 0.4)\n\n\n0.0007762513762070711",
    "crumbs": [
      "Labs - Solutions",
      "Lab 3 - Solutions"
    ]
  },
  {
    "objectID": "Lab3-solutions.html#section-7",
    "href": "Lab3-solutions.html#section-7",
    "title": "Lab 3 - Solutions",
    "section": "5.2 ",
    "text": "5.2 \n\nIn an insurance company, customers’ claims are raised at an average rate of \\(5\\) claims per working day. Calculate the probability that\n\nExactly \\(30\\) claims will be raised in one working week (Monday – Friday).\n\nSolution. The average rate of \\(5\\) claims per working day means, on average, \\(5\\cdot 5=25\\) claims per \\(5\\) working days (that is the working week). Hence we consider now \\(X\\sim Po(25)\\) and we need to find \\(p_X(30)\\).\n\n\n\nCode\npoisson.pmf(30, 25)\n\n\n0.04541278513011904\n\n\nAt least \\(8\\) claims will be raised in the next \\(2\\) working days.\n\nSolution. The average rate per \\(2\\) working days is \\(2\\cdot 5=10\\), i.e. we deal now with \\(X\\sim Po(10)\\). We need to find\n\\[\n\\mathbb{P}(X\\geq8)=1-\\mathbb{P}(X&lt;8)=1-\\mathbb{P}(X\\leq7)=1-F_X(7).\n\\]\n\n\n\nCode\n1 - poisson.cdf(7, 10)\n\n\n0.7797793533983011",
    "crumbs": [
      "Labs - Solutions",
      "Lab 3 - Solutions"
    ]
  },
  {
    "objectID": "Lab3-solutions.html#section-8",
    "href": "Lab3-solutions.html#section-8",
    "title": "Lab 3 - Solutions",
    "section": "6.1 ",
    "text": "6.1 \n\nLet \\(X\\sim Po(3)\\) be a Poisson random variable with the parameter \\(\\lambda=3\\). Generate \\(100\\) random values of \\(X\\) fixing random_state = 111, and assign the resulting Numpy array to a variable f. Calculate the mean and the (population) variance of f. Check your answer. Don’t forget to load numpy first.\n\n\nCode\nimport numpy as np\nfrom scipy.stats import poisson\nf = poisson.rvs(3, size = 100, random_state = 111)\n[np.mean(f), np.var(f)]\n\n\n[2.9, 2.87]\n\n\nWe know that the theoretical mean (expected value) \\(\\mathbb{E}(X)\\) and variance \\(\\mathrm{Var}(X)\\) for a Poisson random variable are equal to \\(\\lambda\\):\n\\[\n\\mathbb{E}(X)=\\mathrm{Var}(X)=\\lambda.\n\\]\nClearly, \\(2.9\\neq 3\\neq 2.87\\). To make the statistics more “matching” the probability, we need to increase the size of the data: let’s generate \\(10^6\\) random variables (keeping random_state = 111). Check the answers.\n\n\nCode\nf = poisson.rvs(3, size = 10**6, random_state = 111)\n[np.mean(f), np.var(f)]\n\n\n[2.998107, 2.9899274165509997]",
    "crumbs": [
      "Labs - Solutions",
      "Lab 3 - Solutions"
    ]
  },
  {
    "objectID": "Lab2-solutions.html",
    "href": "Lab2-solutions.html",
    "title": "Lab 2 - Solutions",
    "section": "",
    "text": "A fair coin is tossed 25 times. Store at the variable a the probability to get 25 heads. A fair six-sided dice is throwing 10 times. Store at the variable b the probability to get 10 equal even scores. Find the ratio a/b.\n\nSolution. The probability of a head is \\(0.5\\), and all tossings are independend, hence, the probability to get \\(n\\) heads in a row is\n\\[\n0.5^n.\n\\]\nNext, \\(m\\) equal even scores in \\(m\\) throwings of the dice may be in \\(3\\) cases:\n\\[\n(\\underbrace{2,\\ldots,2}_{m\\ \\mathrm{times}}),\n(\\underbrace{4,\\ldots,4}_{m\\ \\mathrm{times}}),\n(\\underbrace{4,\\ldots,4}_{m\\ \\mathrm{times}})\n\\] These cases are mutually exclusive, hence the probability that either of them happens is the sum of probabilities of each event. Each probability is equal to\n\\[\n\\Bigl(\\frac16\\Bigr)^{m},\n\\] hence, for b, we need to multiply the latter number (for \\(m=10\\)) by \\(3\\).\n\n\n\nCode\n#\na = 0.5 ** 25\nb = 3 * (1/6) ** 10 # Read about the order of operations in Python\na/b\n\n\n0.6006774902343753\n\n\n\n\n\n\n\nAn experiment consists of selecting a token from a bag and spinning a coin. The bag contains \\(345\\) red tokens and \\(678\\) blue tokens. A token is selected at random from the bag, its colour is noted and then the token is returned to the bag.\nWhen a red token is selected, a biased coin with probability \\(\\dfrac45\\) of landing heads is tossed.\nWhen a blue token is selected, a biased coin with probability \\(\\dfrac25\\) of landing heads is spun.\nFind the probability c of obtaining tail. Round the answer to 3 decimal digits.\n\nSolution. Let \\(R\\) and \\(B\\) be the events of selecting a red (respectively, blue) token. Then\n\\[\n\\mathbb{P}(R) = \\frac{345}{345+678},\\qquad\n\\mathbb{P}(B) = 1-\\mathbb{P}(R)= \\frac{678}{345+678}.\n\\]\nLet \\(T\\) be the event of obtaining a tail. We will use the total probability law:\n\\[\n\\mathbb{P}(T) = \\mathbb{P}(T\\mid R)\\cdot \\mathbb{P}(R)+\n\\mathbb{P}(T \\mid B)\\cdot \\mathbb{P}(B).\n\\]\nWe know also that \\(\\mathbb{P}(H\\mid R)=\\frac45\\), hence, \\(\\mathbb{P}(T\\mid R)=1-\\frac45=\\frac15\\). Similarly, \\(\\mathbb{P}(T\\mid B)=1-\\frac25=\\frac35\\).\n\n\n\nCode\np_red =  345/(345+678)\np_blue = 1 - p_red\nc = p_red * (1 - 4/5) + p_blue * (1 - 2/5)\nround(c, 3)\n\n\n0.465\n\n\n\n\n\n\n\nIn the conditions of the previous task, if the tail was obtained, what is the probability \\(d\\) of having selected a red token? Round the answer to 3 decimal digits.\n\nSolution. By Bayes’ formula,\n\\[\n\\mathbb{P}(R | T) = \\frac{\\mathbb{P}(R) \\cdot \\mathbb{P}(T | R)}{\\mathbb{P}(T)}\n\\]\n\n\n\nCode\nd = p_red * (1-4/5) / c\nround(d, 3)\n\n\n0.145",
    "crumbs": [
      "Labs - Solutions",
      "Lab 2 - Solutions"
    ]
  },
  {
    "objectID": "Lab2-solutions.html#section",
    "href": "Lab2-solutions.html#section",
    "title": "Lab 2 - Solutions",
    "section": "",
    "text": "A fair coin is tossed 25 times. Store at the variable a the probability to get 25 heads. A fair six-sided dice is throwing 10 times. Store at the variable b the probability to get 10 equal even scores. Find the ratio a/b.\n\nSolution. The probability of a head is \\(0.5\\), and all tossings are independend, hence, the probability to get \\(n\\) heads in a row is\n\\[\n0.5^n.\n\\]\nNext, \\(m\\) equal even scores in \\(m\\) throwings of the dice may be in \\(3\\) cases:\n\\[\n(\\underbrace{2,\\ldots,2}_{m\\ \\mathrm{times}}),\n(\\underbrace{4,\\ldots,4}_{m\\ \\mathrm{times}}),\n(\\underbrace{4,\\ldots,4}_{m\\ \\mathrm{times}})\n\\] These cases are mutually exclusive, hence the probability that either of them happens is the sum of probabilities of each event. Each probability is equal to\n\\[\n\\Bigl(\\frac16\\Bigr)^{m},\n\\] hence, for b, we need to multiply the latter number (for \\(m=10\\)) by \\(3\\).\n\n\n\nCode\n#\na = 0.5 ** 25\nb = 3 * (1/6) ** 10 # Read about the order of operations in Python\na/b\n\n\n0.6006774902343753",
    "crumbs": [
      "Labs - Solutions",
      "Lab 2 - Solutions"
    ]
  },
  {
    "objectID": "Lab2-solutions.html#section-1",
    "href": "Lab2-solutions.html#section-1",
    "title": "Lab 2 - Solutions",
    "section": "",
    "text": "An experiment consists of selecting a token from a bag and spinning a coin. The bag contains \\(345\\) red tokens and \\(678\\) blue tokens. A token is selected at random from the bag, its colour is noted and then the token is returned to the bag.\nWhen a red token is selected, a biased coin with probability \\(\\dfrac45\\) of landing heads is tossed.\nWhen a blue token is selected, a biased coin with probability \\(\\dfrac25\\) of landing heads is spun.\nFind the probability c of obtaining tail. Round the answer to 3 decimal digits.\n\nSolution. Let \\(R\\) and \\(B\\) be the events of selecting a red (respectively, blue) token. Then\n\\[\n\\mathbb{P}(R) = \\frac{345}{345+678},\\qquad\n\\mathbb{P}(B) = 1-\\mathbb{P}(R)= \\frac{678}{345+678}.\n\\]\nLet \\(T\\) be the event of obtaining a tail. We will use the total probability law:\n\\[\n\\mathbb{P}(T) = \\mathbb{P}(T\\mid R)\\cdot \\mathbb{P}(R)+\n\\mathbb{P}(T \\mid B)\\cdot \\mathbb{P}(B).\n\\]\nWe know also that \\(\\mathbb{P}(H\\mid R)=\\frac45\\), hence, \\(\\mathbb{P}(T\\mid R)=1-\\frac45=\\frac15\\). Similarly, \\(\\mathbb{P}(T\\mid B)=1-\\frac25=\\frac35\\).\n\n\n\nCode\np_red =  345/(345+678)\np_blue = 1 - p_red\nc = p_red * (1 - 4/5) + p_blue * (1 - 2/5)\nround(c, 3)\n\n\n0.465",
    "crumbs": [
      "Labs - Solutions",
      "Lab 2 - Solutions"
    ]
  },
  {
    "objectID": "Lab2-solutions.html#section-2",
    "href": "Lab2-solutions.html#section-2",
    "title": "Lab 2 - Solutions",
    "section": "",
    "text": "In the conditions of the previous task, if the tail was obtained, what is the probability \\(d\\) of having selected a red token? Round the answer to 3 decimal digits.\n\nSolution. By Bayes’ formula,\n\\[\n\\mathbb{P}(R | T) = \\frac{\\mathbb{P}(R) \\cdot \\mathbb{P}(T | R)}{\\mathbb{P}(T)}\n\\]\n\n\n\nCode\nd = p_red * (1-4/5) / c\nround(d, 3)\n\n\n0.145",
    "crumbs": [
      "Labs - Solutions",
      "Lab 2 - Solutions"
    ]
  },
  {
    "objectID": "Lab2-solutions.html#section-3",
    "href": "Lab2-solutions.html#section-3",
    "title": "Lab 2 - Solutions",
    "section": "2.1 ",
    "text": "2.1 \n\nIn a school sport club, there are \\(20\\) children who like rugby more than football, and \\(15\\) chidlren who like football more than rugby. (Noone likes them equally.) The club wants to form a committee of \\(6\\) people: \\(3\\) football funs and \\(3\\) rugby funs. How many ways does there exist to form the committee? Store the answer in w.\n\nRemark. Note that since the choice of football funs does not depend on the choice of rugby funs, we need just to multiply the number of ways to choose fotball funs and the number of ways to choose rugby funs.\n\n\nSolution. Therefore,\n\\[\nw = \\binom{20}{3}\\cdot\\binom{15}{3}\n\\]\n\n\n\nCode\nw = comb(20,3, exact=True) * comb(15,3, exact=True)\nw\n\n\n518700",
    "crumbs": [
      "Labs - Solutions",
      "Lab 2 - Solutions"
    ]
  },
  {
    "objectID": "Lab2-solutions.html#section-4",
    "href": "Lab2-solutions.html#section-4",
    "title": "Lab 2 - Solutions",
    "section": "2.2 ",
    "text": "2.2 \n\nLet now the committee (of 6 children) is chosen without any restrictions on the preferences of its members. Find the number of ways to form the committee. Store the answer in r.\n\nSolution. We choose \\(6\\) children from \\(20+15=35\\), it can be done in \\(\\binom{35}{6}\\) ways.\n\n\n\nCode\n# We choose 6 children from 35\nr = comb(35,6, exact=True)\nr\n\n\n1623160",
    "crumbs": [
      "Labs - Solutions",
      "Lab 2 - Solutions"
    ]
  },
  {
    "objectID": "Lab2-solutions.html#section-5",
    "href": "Lab2-solutions.html#section-5",
    "title": "Lab 2 - Solutions",
    "section": "2.3 ",
    "text": "2.3 \n\nFinally, find the probability p that a randomly choosen committee consisting of 6 children has equal numbers of football and rugby funs. Round to \\(4\\) decimal digits.\n\n\nCode\np = round(w/r,4)\np\n\n\n0.3196",
    "crumbs": [
      "Labs - Solutions",
      "Lab 2 - Solutions"
    ]
  },
  {
    "objectID": "Lab2-solutions.html#section-6",
    "href": "Lab2-solutions.html#section-6",
    "title": "Lab 2 - Solutions",
    "section": "2.4 ",
    "text": "2.4 \n\nIf \\(10\\) balls are randomly drawn from a bag containing \\(17\\) blue and \\(19\\) yellow balls, what is the probability q that \\(4\\) of the balls are blue and the others are yellow? Round to \\(3\\) decimal digits.\nNote that despite the balls of the same colour are indistinguishable (in contrast to children of the same sport preference in the previous tasks), we may always think that the balls e.g. numbered, to apply the same arguments as before.\n\n\nCode\nq = comb(17, 4) * comb(19, 10-4) / comb(17+19, 10)\nq\n\n\n0.25404208941472567\n\n\n\n\n\n\n\n\n\nRemember\n\n\n\n\nSpecial cases:\n\n\\[\n\\binom{n}{0}=\\binom{0}{0}=1\n\\]\n\nSimple cases:\n\n\\[\n\\binom{n}{1} = n, \\qquad \\binom{n}{2}=\\frac{n(n-1)}{2}\n\\]\n\nOverall,\n\n\\[\n\\binom{n}{k} = \\frac{n\\cdot(n-1)\\cdot(n-2)\\cdot\\ldots\\cdot(n-k+1)}{1\\cdot2\\cdot3\\cdot\\ldots\\cdot k},\n\\]\nwhere numerator has \\(k\\) factors. E.g.\n\\[\n\\binom{37}{4}=\\frac{37\\cdot 36\\cdot 35\\cdot 34}{1\\cdot 2\\cdot 3\\cdot 4}.\n\\]",
    "crumbs": [
      "Labs - Solutions",
      "Lab 2 - Solutions"
    ]
  },
  {
    "objectID": "Lab2-solutions.html#section-7",
    "href": "Lab2-solutions.html#section-7",
    "title": "Lab 2 - Solutions",
    "section": "3.1 ",
    "text": "3.1 \n\nChoose a random element from the list coin. The output will be one of two letters h or t, e.g.:\n\n\nCode\nrandom.choice(coin)\n\n\n'h'\n\n\nNote that if you run the same code again and again you may get each time another output.\nCongratulations: you made your first probability model.",
    "crumbs": [
      "Labs - Solutions",
      "Lab 2 - Solutions"
    ]
  },
  {
    "objectID": "Lab2-solutions.html#section-8",
    "href": "Lab2-solutions.html#section-8",
    "title": "Lab 2 - Solutions",
    "section": "3.2 ",
    "text": "3.2 \n\nSuppose we tossed the coin n_trials = 10 times. Generate a list called series that would contain outputs of all these trials, e.g.\n\n\nCode\nn_trials = 10\nseries = [random.choice(coin) for _ in range(n_trials)]\nseries\n\n\n['h', 't', 'h', 't', 't', 't', 'h', 't', 't', 't']\n\n\n(your output will be probably different).\nAdvice: it may be more convenient to use list comprehension instead of loops, though it’s up to you. Recall that range(10) command generates 10 numbers from 0 to 9.\nCount the number of “heads” in series, using property list_name.count(element_name) to get the number of elements element_name in the list list_name.\n\n\nCode\nseries.count('h')\n\n\n3",
    "crumbs": [
      "Labs - Solutions",
      "Lab 2 - Solutions"
    ]
  },
  {
    "objectID": "Lab2-solutions.html#section-9",
    "href": "Lab2-solutions.html#section-9",
    "title": "Lab 2 - Solutions",
    "section": "3.3 ",
    "text": "3.3 \n\nLoad library pyplot of module matplotlib as follows:\n\nimport matplotlib.pyplot as plt\n\nChange now n_trials to 1000. We expect to see around 500 heads in a series of trials, though if you run your code, you will probably get another number. Define n_series = 1000 and repeat running the previous code n_series times, i.e. the output should be a list of n_series numbers. Store the output in a variable heads.\nAgain, you may find more useful to use list comprehension.\nCreate the scatter plot of numbers of heads in all series using the commands plt.scatter(x, heads) and plt.show(), where x would be all numbers from 0 to n_series.\nYour output should be similar to:\n\n\nCode\n#The first line of the code is to see the output in Jupyter notebook\n%matplotlib inline\nn_trials = 1000\nn_series = 1000\nheads = [[random.choice(coin) for _ in range(n_trials)].count('h') for _ in range(n_series)]\nx = range(n_series)\nplt.scatter(x, heads)\nplt.show()\n\n\n\n\n\n\n\n\n\nAs you can see all results indeed gather around 500. Note that you can find min(heads) and max(heads) to see how large is the spread of numbers, e.g. on the picture above it’s\n\n[min(heads),max(heads)]\n\n[451, 550]",
    "crumbs": [
      "Labs - Solutions",
      "Lab 2 - Solutions"
    ]
  },
  {
    "objectID": "Lab2-solutions.html#section-10",
    "href": "Lab2-solutions.html#section-10",
    "title": "Lab 2 - Solutions",
    "section": "4.1 ",
    "text": "4.1 \n\nConsider again that we toss the coin \\(1000\\) times and count the number of tails, but repeat this procedure \\(10^5\\) times. Calculate the average m of the obtained result.\n\n\nCode\nn = 1000\nb = np.random.randint(2, size=(10**5,n))\nb.sum(axis=1).mean()\n\n\n500.00994\n\n\nAs expected, the result is pretty close to \\(500\\) (your result may differ from this, of course).",
    "crumbs": [
      "Labs - Solutions",
      "Lab 2 - Solutions"
    ]
  },
  {
    "objectID": "Lab2-solutions.html#section-11",
    "href": "Lab2-solutions.html#section-11",
    "title": "Lab 2 - Solutions",
    "section": "4.2 ",
    "text": "4.2 \n\nWe can also model unfair coins. The command\n\nn=10\nnp.random.choice([0,1], size=(5,n), p=[0.6, 0.4])\n\narray([[1, 1, 0, 0, 0, 0, 0, 0, 1, 0],\n       [1, 1, 0, 0, 1, 1, 0, 0, 1, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 1, 0],\n       [1, 0, 1, 1, 0, 0, 0, 1, 0, 0],\n       [1, 0, 0, 0, 1, 1, 1, 0, 0, 1]])\n\n\ngenerates \\(5\\) series of \\(10\\) outputs each with the probability for head \\(0.6\\) and the probability for tail \\(0.4\\). Calculate again the average for \\(10^5\\) series of \\(1000\\) trials.\n\n\nCode\nn = 1000\nb = np.random.choice([0,1], size=(10**5,n), p=[0.6, 0.4])\nb.sum(axis=1).mean()\n\n\n399.90451\n\n\nThe result is pretty close to \\(400 = 0.4*1000\\). We discuss the theoretical justification for this on Week 3.",
    "crumbs": [
      "Labs - Solutions",
      "Lab 2 - Solutions"
    ]
  },
  {
    "objectID": "Lab1-solutions.html",
    "href": "Lab1-solutions.html",
    "title": "Lab 1 - Solutions",
    "section": "",
    "text": "As you know, statistics deals with data. There are several ways how to load data to Python.\nIn this lab we consider the first two of them.\nThere are several ways to calculate mean, mode, median, variance and other descriptive characteristics in Python. We will use numpy library for this, so run at the beginning of your Jupyter notebook the following command\nimport numpy as np",
    "crumbs": [
      "Labs - Solutions",
      "Lab 1 - Solutions"
    ]
  },
  {
    "objectID": "Lab1-solutions.html#section",
    "href": "Lab1-solutions.html#section",
    "title": "Lab 1 - Solutions",
    "section": "1.1 ",
    "text": "1.1 \n\nConvert the list a defined above to a Numpy array, reassign the result again to the variable a:\n\n\nCode\na = np.array(a)\na\n\n\narray([2, 1, 3, 4, 2, 6, 4, 0, 1, 1, 3, 3, 4, 1, 1, 5, 5, 2, 1, 3])\n\n\n\n\nMean\nTo calculate the mean of all values in the array \\(a=(a_1,\\ldots,a_{20})\\), i.e.\n\\[\n\\bar{a} = \\frac{a_1+\\ldots+a_{n}}{n},\n\\]\nfor \\(n=20\\), we run\n\nnp.mean(a)\n\n2.6\n\n\n(we could calculate this and further characteristics directly, e.g. for mean one could write sum(a)/len(a), however, the usage of special functions is more efficient in the case of large datasets).\n\n\nMedian\nSimilarly, to calculate the median of data, we can use np.median method.",
    "crumbs": [
      "Labs - Solutions",
      "Lab 1 - Solutions"
    ]
  },
  {
    "objectID": "Lab1-solutions.html#section-1",
    "href": "Lab1-solutions.html#section-1",
    "title": "Lab 1 - Solutions",
    "section": "1.2 ",
    "text": "1.2 \n\nFind the median of a. Check the answer.\n\n\nCode\nnp.median(a)\n\n\n2.5\n\n\n\n\nPopulation variance and standard deviation\nTo calculate the population variance of a, i.e. the quantity\n\\[\n\\sigma^2 = \\dfrac{(a_1-\\bar a)^2+\\ldots+(a_n-\\bar a)^2}{\\color{red}n},\n\\]\nwe use np.var method and to calculate the population standard deviation \\(\\sigma=\\sqrt{\\sigma^2}\\) we use np.std method. For example,\n\nnp.var(a)\n\n2.6399999999999997\n\n\nAgain, this is faster and more convenient than calculating them directly, e.g. instead of np.var(a) we could write\n\nsum((a - np.mean(a))**2)/len(a)\n\n2.64\n\n\n\n\n\n\n\n\nDo not miss this\n\n\n\nNote that we have used, in the last command, array (vector) operations: a - np.mean(a) means that we subtract the number np.mean(a) from each component of a, and the result is again an array of \\(a_1-\\bar a,\\ldots,a_n-\\bar{a}\\). Simmilarly, **2 means that we square each component of the array a - np.mean(a), and the result is again an array.",
    "crumbs": [
      "Labs - Solutions",
      "Lab 1 - Solutions"
    ]
  },
  {
    "objectID": "Lab1-solutions.html#section-2",
    "href": "Lab1-solutions.html#section-2",
    "title": "Lab 1 - Solutions",
    "section": "1.3 ",
    "text": "1.3 \n\nFind the standard deviation of a. Check the answer.\n\n\nCode\nnp.std(a)\n\n\n1.624807680927192\n\n\n\n\nSample variance and standard deviation\nTo get the sample variance\n\\[\ns^2 = \\dfrac{(a_1-\\bar a)^2+\\ldots+(a_n-\\bar a)^2}{\\color{red}n-1}.\n\\]\none could, of course, use that\n\\[\ns^2=\\frac{n}{n-1} \\sigma^2,\n\\]\n(where \\(n=20\\) for the given a), however, it is better to use a special key ddof = 1 inside np.var, where ddof stands for “delta degree of freedom”:\n\nnp.var(a, ddof = 1)\n\n2.7789473684210524\n\n\nWe can check that indeed e.g.\n\nnp.var(a, ddof = 1) == np.var(a)*20/19\n\nTrue",
    "crumbs": [
      "Labs - Solutions",
      "Lab 1 - Solutions"
    ]
  },
  {
    "objectID": "Lab1-solutions.html#section-3",
    "href": "Lab1-solutions.html#section-3",
    "title": "Lab 1 - Solutions",
    "section": "1.4 ",
    "text": "1.4 \n\nFind the sample standard deviation for a using an analogy with the previous commands. Check the answer.\n\n\nCode\nnp.std(a, ddof = 1)\n\n\n1.6670175069329813",
    "crumbs": [
      "Labs - Solutions",
      "Lab 1 - Solutions"
    ]
  },
  {
    "objectID": "Lab1-solutions.html#section-4",
    "href": "Lab1-solutions.html#section-4",
    "title": "Lab 1 - Solutions",
    "section": "2.1 ",
    "text": "2.1 \n\nFind the average weight of the members of the sports club. Check your answer.\n\n\nCode\nnp.mean(b)\n\n\n69.34",
    "crumbs": [
      "Labs - Solutions",
      "Lab 1 - Solutions"
    ]
  },
  {
    "objectID": "Lab1-solutions.html#section-5",
    "href": "Lab1-solutions.html#section-5",
    "title": "Lab 1 - Solutions",
    "section": "2.2 ",
    "text": "2.2 \n\nAssign to variable d the Numpy array of the heights of all members of the sports club. Find the median and the variance of the heights. Check your answers.\n\n\nCode\nd = df[\"Height\"].to_numpy()\n[np.median(d), np.var(d)]\n\n\n[170.0, 254.2239]",
    "crumbs": [
      "Labs - Solutions",
      "Lab 1 - Solutions"
    ]
  },
  {
    "objectID": "Lab1-solutions.html#section-6",
    "href": "Lab1-solutions.html#section-6",
    "title": "Lab 1 - Solutions",
    "section": "2.3 ",
    "text": "2.3 \n\nAssign to variable e the Numpy array of the body mass indexes (BMI) of the club members: if a member has weight \\(w\\) kg and height \\(h\\) cm, then its BMI is \\(10^4*\\dfrac{w}{h^2}\\). Remember that in Python all array operations are done component-wise. Find the mean and the standard deviation of e. Check your answer.\n\n\nCode\ne = 10**4 * b/(d*d)\n[np.mean(e), np.std(e)]\n\n\n[24.900949943455288, 6.54617400802904]",
    "crumbs": [
      "Labs - Solutions",
      "Lab 1 - Solutions"
    ]
  },
  {
    "objectID": "Lab1-solutions.html#section-7",
    "href": "Lab1-solutions.html#section-7",
    "title": "Lab 1 - Solutions",
    "section": "3.1 ",
    "text": "3.1 \n\nTake the first \\(100\\) members of the sports club from the sportsclub.csv discussed before, and draw the scatter plot of heights (vertical axis) over weights (horizontal axis). Use green colours for the markers. Label the axes appropriately.\n\n\nCode\nimport matplotlib.pyplot as plt\nweights = b[0:100]\nheights = d[0:100]\nplt.scatter(weights, heights, color = 'g')\nplt.xlabel('Weights')\nplt.ylabel('Heights')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nBox plot\nConsider now a box-and-whisker plot. The following code produce the blox plot for the weights of all members of the sports club:\n\nplt.boxplot(b)\nplt.show()\n\n\n\n\n\n\n\n\nBy default, the box plot is vertical. To make it horizontal, use the key vert = False in boxplot method.",
    "crumbs": [
      "Labs - Solutions",
      "Lab 1 - Solutions"
    ]
  },
  {
    "objectID": "Lab1-solutions.html#section-8",
    "href": "Lab1-solutions.html#section-8",
    "title": "Lab 1 - Solutions",
    "section": "3.2 ",
    "text": "3.2 \nPlot the horizontal box plot for the heights of all members of the sports club.\n\n\nCode\nplt.boxplot(d, vert = False)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nRemark. Recall that the box plot visualises, in particular, the rectangle bound by the lower and upper quartiles: \\(Q_1\\) and \\(Q_3\\), whereas the coloured line represents the median \\(Q_2\\). We know that \\(Q_2\\) is calculated differently if the sample has even or odd number of elements. By definition, recall, \\(Q_1\\) and \\(Q_3\\) are medians of the lower half and upper halves of the ordered sample. However, it’s a question of agreement whether to include the median \\(Q_2\\) to both halves or not, in the case when the number of elelements is odd. For example, let the data be \\[\n1,2,3,4,5,6,7.\n\\] The median is \\(4\\), i.e. \\(Q_2=4\\). Now, we may say that the lower half is \\(1,2,3\\), its median is \\(2\\), i.e. \\(Q_1=2\\), similarly then the upper half is \\(5,6,7\\), hence, \\(Q_3=6\\). Another approach is to say that the lower part is \\(1,2,3,4\\) (including \\(Q_2=4\\)), and it’s median is \\(\\frac{2+3}{2}=2.5\\), i.e. \\(Q_1=2.5\\); similarly then the upper half is \\(4,5,6,7\\), and hence, \\(Q_3=5.5\\). Python in Mathplotlib uses the second approach.\n\n\nHistogram\nLet’s create the histogram for the weights of all sports club members. Conside the histogram with \\(20\\) bins.\n\nimport matplotlib.pyplot as plt\nplt.hist(b, 20)\nplt.xlabel('Weights')\nplt.ylabel('Counts')\nplt.show()\n\n\n\n\n\n\n\n\nRecall that the change of the number of bins may drastically change the shapce of a histogram, e.g.\n\nimport matplotlib.pyplot as plt\nplt.hist(b, 50)                 # 50 bins\nplt.xlabel('Weights')\nplt.ylabel('Counts')\nplt.show()\n\n\n\n\n\n\n\n\nor\n\nplt.hist(b, 5)                 # 5 bins\nplt.xlabel('Weights')\nplt.ylabel('Counts')\nplt.show()",
    "crumbs": [
      "Labs - Solutions",
      "Lab 1 - Solutions"
    ]
  },
  {
    "objectID": "Lab1-solutions.html#section-9",
    "href": "Lab1-solutions.html#section-9",
    "title": "Lab 1 - Solutions",
    "section": "3.3 ",
    "text": "3.3 \n\nPlot the histogram of heights of the all members of the sports club. Make all the bins of the size \\(3\\) cm (perhaps, all but the last, the most right, one). To calculate the number of bins, use functions min() and max() and also round().\n\nSolution. The heights are stored in the array d. The range of values of heights is from min(d) to max(d) (i.e. from minimal height to maximal height); all heights are in centimeneters. To divide this range on parts of size \\(3\\) cm, we would need (max(d)-min(d))/3 parts. Since the latter number may be decimal, we round it with function round.\n\n\n\nCode\nn_bins = round((max(d) - min(d))/3)\nplt.hist(d, n_bins)  \nplt.xlabel('Heights')\nplt.ylabel('Counts')\nplt.show()",
    "crumbs": [
      "Labs - Solutions",
      "Lab 1 - Solutions"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction",
    "section": "",
    "text": "1 Sign up to Anaconda (do it once)\n\nGo to https://anaconda.com/app/\nSign up using your personal or University email address.\nAdvice: do not use your University password.\n\n\n\n2 Log in to Anaconda (do it for each lab)\n\nLogin to https://anaconda.com/app/\nChoose “Notebooks” in the Explore Anaconda section:\n\n\n\nOpen the file browser (from the left side menu bar)\n\n\n\nCreate a folder for all files related to this module:\n\n\nand name it, e.g., “MA-M27”.\n\nOpen the created folder.\nPress blue button with “+”\n\n\nand choose any of anaconda-notebooks:\n\n\nRename the created file\n\n\ne.g. to “Lab1.ipynb”, “Lab2.ipynb” etc.\n\n\n\n\n Back to top",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "Lab1.html",
    "href": "Lab1.html",
    "title": "Lab 1",
    "section": "",
    "text": "As you know, statistics deals with data. There are several ways how to load data to Python.\nIn this lab we consider the first two of them.\nThere are several ways to calculate mean, mode, median, variance and other descriptive characteristics in Python. We will use numpy library for this, so run at the beginning of your Jupyter notebook the following command\nimport numpy as np",
    "crumbs": [
      "Labs - Problems",
      "Lab 1 - Problems"
    ]
  },
  {
    "objectID": "Lab1.html#section",
    "href": "Lab1.html#section",
    "title": "Lab 1",
    "section": "1.1 ",
    "text": "1.1 \n\nConvert the list a defined above to a Numpy array, reassign the result again to the variable a to get the following output\n\na\n\narray([2, 1, 3, 4, 2, 6, 4, 0, 1, 1, 3, 3, 4, 1, 1, 5, 5, 2, 1, 3])\n\n\n\n\nMean\nTo calculate the mean of all values in the array \\(a=(a_1,\\ldots,a_{20})\\), i.e.\n\\[\n\\bar{a} = \\frac{a_1+\\ldots+a_{n}}{n},\n\\]\nfor \\(n=20\\), we run\n\nnp.mean(a)\n\n2.6\n\n\n(we could calculate this and further characteristics directly, e.g. for mean one could write sum(a)/len(a), however, the usage of special functions is more efficient in the case of large datasets).\n\n\nMedian\nSimilarly, to calculate the median of data, we can use np.median method.",
    "crumbs": [
      "Labs - Problems",
      "Lab 1 - Problems"
    ]
  },
  {
    "objectID": "Lab1.html#section-1",
    "href": "Lab1.html#section-1",
    "title": "Lab 1",
    "section": "1.2 ",
    "text": "1.2 \n\nFind the median of a. Check the answer.\n\n\n2.5\n\n\n\n\nPopulation variance and standard deviation\nTo calculate the population variance of a, i.e. the quantity\n\\[\n\\sigma^2 = \\dfrac{(a_1-\\bar a)^2+\\ldots+(a_n-\\bar a)^2}{\\color{red}n},\n\\]\nwe use np.var method and to calculate the population standard deviation \\(\\sigma=\\sqrt{\\sigma^2}\\) we use np.std method. For example,\n\nnp.var(a)\n\n2.6399999999999997\n\n\nAgain, this is faster and more convenient than calculating them directly, e.g. instead of np.var(a) we could write\n\nsum((a - np.mean(a))**2)/len(a)\n\n2.64\n\n\n\n\n\n\n\n\nDo not miss this\n\n\n\nNote that we have used, in the last command, array (vector) operations: a - np.mean(a) means that we subtract the number np.mean(a) from each component of a, and the result is again an array of \\(a_1-\\bar a,\\ldots,a_n-\\bar{a}\\).\nSimmilarly, **2 means that we square each component of the array a - np.mean(a), and the result is again an array.",
    "crumbs": [
      "Labs - Problems",
      "Lab 1 - Problems"
    ]
  },
  {
    "objectID": "Lab1.html#section-2",
    "href": "Lab1.html#section-2",
    "title": "Lab 1",
    "section": "1.3 ",
    "text": "1.3 \n\nFind the standard deviation of a. Check the answer.\n\n\n1.624807680927192\n\n\n\n\nSample variance and standard deviation\nTo get the sample variance\n\\[\ns^2 = \\dfrac{(a_1-\\bar a)^2+\\ldots+(a_n-\\bar a)^2}{\\color{red}n-1}.\n\\]\none could, of course, use that\n\\[\ns^2=\\frac{n}{n-1} \\sigma^2,\n\\]\n(where \\(n=20\\) for the given a), however, it is better to use a special key ddof = 1 inside np.var, where ddof stands for “delta degree of freedom”:\n\nnp.var(a, ddof = 1)\n\n2.7789473684210524\n\n\nWe can check that indeed e.g.\n\nnp.var(a, ddof = 1) == np.var(a)*20/19\n\nTrue",
    "crumbs": [
      "Labs - Problems",
      "Lab 1 - Problems"
    ]
  },
  {
    "objectID": "Lab1.html#section-3",
    "href": "Lab1.html#section-3",
    "title": "Lab 1",
    "section": "1.4 ",
    "text": "1.4 \n\nFind the sample standard deviation for a using an analogy with the previous commands. Check the answer.\n\n\n1.6670175069329813",
    "crumbs": [
      "Labs - Problems",
      "Lab 1 - Problems"
    ]
  },
  {
    "objectID": "Lab1.html#section-4",
    "href": "Lab1.html#section-4",
    "title": "Lab 1",
    "section": "2.1 ",
    "text": "2.1 \n\nFind the average weight of the members of the sports club. Check your answer.\n\n\n69.34",
    "crumbs": [
      "Labs - Problems",
      "Lab 1 - Problems"
    ]
  },
  {
    "objectID": "Lab1.html#section-5",
    "href": "Lab1.html#section-5",
    "title": "Lab 1",
    "section": "2.2 ",
    "text": "2.2 \n\nAssign to variable d the Numpy array of the heights of all members of the sports club. Find the median and the variance of the heights. Check your answers.\n\n\n[170.0, 254.2239]",
    "crumbs": [
      "Labs - Problems",
      "Lab 1 - Problems"
    ]
  },
  {
    "objectID": "Lab1.html#section-6",
    "href": "Lab1.html#section-6",
    "title": "Lab 1",
    "section": "2.3 ",
    "text": "2.3 \n\nAssign to variable e the Numpy array of the body mass indexes (BMI) of the club members: if a member has weight \\(w\\) kg and height \\(h\\) cm, then its BMI is \\(10^4*\\dfrac{w}{h^2}\\). Remember that in Python all array operations are done component-wise. Find the mean and the standard deviation of e. Check your answer.\n\n\n[24.900949943455288, 6.54617400802904]",
    "crumbs": [
      "Labs - Problems",
      "Lab 1 - Problems"
    ]
  },
  {
    "objectID": "Lab1.html#section-7",
    "href": "Lab1.html#section-7",
    "title": "Lab 1",
    "section": "3.1 ",
    "text": "3.1 \n\nTake the first \\(100\\) members of the sports club from the sportsclub.csv discussed before, and draw the scatter plot of heights (vertical axis) over weights (horizontal axis). Use green colours for the markers. Label the axes appropriately.\n\n\n\n\n\n\n\n\n\n\n\nBox plot\nConsider now a box-and-whisker plot. The following code produce the blox plot for the weights of all members of the sports club:\n\nplt.boxplot(b)\nplt.show()\n\n\n\n\n\n\n\n\nBy default, the box plot is vertical. To make it horizontal, use the key vert = False in boxplot method.",
    "crumbs": [
      "Labs - Problems",
      "Lab 1 - Problems"
    ]
  },
  {
    "objectID": "Lab1.html#section-8",
    "href": "Lab1.html#section-8",
    "title": "Lab 1",
    "section": "3.2 ",
    "text": "3.2 \n\nPlot the horizontal box plot for the heights of all members of the sports club.\n\n\n\n\n\n\n\n\n\n\n\nRemark. Recall that the box plot visualises, in particular, the rectangle bound by the lower and upper quartiles: \\(Q_1\\) and \\(Q_3\\), whereas the coloured line represents the median \\(Q_2\\). We know that \\(Q_2\\) is calculated differently if the sample has even or odd number of elements. By definition, recall, \\(Q_1\\) and \\(Q_3\\) are medians of the lower half and upper halves of the ordered sample. However, it’s a question of agreement whether to include the median \\(Q_2\\) to both halves or not, in the case when the number of elelements is odd. For example, let the data be \\[\n1,2,3,4,5,6,7.\n\\] The median is \\(4\\), i.e. \\(Q_2=4\\). Now, we may say that the lower half is \\(1,2,3\\), its median is \\(2\\), i.e. \\(Q_1=2\\), similarly then the upper half is \\(5,6,7\\), hence, \\(Q_3=6\\). Another approach is to say that the lower part is \\(1,2,3,4\\) (including \\(Q_2=4\\)), and it’s median is \\(\\frac{2+3}{2}=2.5\\), i.e. \\(Q_1=2.5\\); similarly then the upper half is \\(4,5,6,7\\), and hence, \\(Q_3=5.5\\). Python in Mathplotlib uses the second approach.\n\n\nHistogram\nLet’s create the histogram for the weights of all sports club members. Conside the histogram with \\(20\\) bins.\n\nimport matplotlib.pyplot as plt\nplt.hist(b, 20)\nplt.xlabel('Weights')\nplt.ylabel('Counts')\nplt.show()\n\n\n\n\n\n\n\n\nRecall that the change of the number of bins may drastically change the shapce of a histogram, e.g.\n\nimport matplotlib.pyplot as plt\nplt.hist(b, 50)                 # 50 bins\nplt.xlabel('Weights')\nplt.ylabel('Counts')\nplt.show()\n\n\n\n\n\n\n\n\nor\n\nplt.hist(b, 5)                 # 5 bins\nplt.xlabel('Weights')\nplt.ylabel('Counts')\nplt.show()",
    "crumbs": [
      "Labs - Problems",
      "Lab 1 - Problems"
    ]
  },
  {
    "objectID": "Lab1.html#section-9",
    "href": "Lab1.html#section-9",
    "title": "Lab 1",
    "section": "3.3 ",
    "text": "3.3 \n\nPlot the histogram of heights of the all members of the sports club. Make all the bins of the size \\(3\\) cm (perhaps, all but the last, the most right, one). To calculate the number of bins, use functions min() and max() and also round().",
    "crumbs": [
      "Labs - Problems",
      "Lab 1 - Problems"
    ]
  },
  {
    "objectID": "Lab2.html",
    "href": "Lab2.html",
    "title": "Lab 2",
    "section": "",
    "text": "A fair coin is tossed 25 times. Store at the variable a the probability to get 25 heads. A fair six-sided dice is throwing 10 times. Store at the variable b the probability to get 10 equal even scores. Find the ratio a/b.\n\na/b\n\n0.6006774902343753\n\n\n\n\n\n\n\nAn experiment consists of selecting a token from a bag and spinning a coin. The bag contains \\(345\\) red tokens and \\(678\\) blue tokens. A token is selected at random from the bag, its colour is noted and then the token is returned to the bag.\nWhen a red token is selected, a biased coin with probability \\(\\dfrac45\\) of landing heads is tossed.\nWhen a blue token is selected, a biased coin with probability \\(\\dfrac25\\) of landing heads is spun.\nFind the probability c of obtaining tail. Round the answer to 3 decimal digits.\n\nc\n\n0.465\n\n\n\n\n\n\n\nIn the conditions of the previous task, if the tail was obtained, what is the probability \\(d\\) of having selected a red token? Round the answer to 3 decimal digits.\n\nd\n\n0.145",
    "crumbs": [
      "Labs - Problems",
      "Lab 2 - Problems"
    ]
  },
  {
    "objectID": "Lab2.html#section",
    "href": "Lab2.html#section",
    "title": "Lab 2",
    "section": "",
    "text": "A fair coin is tossed 25 times. Store at the variable a the probability to get 25 heads. A fair six-sided dice is throwing 10 times. Store at the variable b the probability to get 10 equal even scores. Find the ratio a/b.\n\na/b\n\n0.6006774902343753",
    "crumbs": [
      "Labs - Problems",
      "Lab 2 - Problems"
    ]
  },
  {
    "objectID": "Lab2.html#section-1",
    "href": "Lab2.html#section-1",
    "title": "Lab 2",
    "section": "",
    "text": "An experiment consists of selecting a token from a bag and spinning a coin. The bag contains \\(345\\) red tokens and \\(678\\) blue tokens. A token is selected at random from the bag, its colour is noted and then the token is returned to the bag.\nWhen a red token is selected, a biased coin with probability \\(\\dfrac45\\) of landing heads is tossed.\nWhen a blue token is selected, a biased coin with probability \\(\\dfrac25\\) of landing heads is spun.\nFind the probability c of obtaining tail. Round the answer to 3 decimal digits.\n\nc\n\n0.465",
    "crumbs": [
      "Labs - Problems",
      "Lab 2 - Problems"
    ]
  },
  {
    "objectID": "Lab2.html#section-2",
    "href": "Lab2.html#section-2",
    "title": "Lab 2",
    "section": "",
    "text": "In the conditions of the previous task, if the tail was obtained, what is the probability \\(d\\) of having selected a red token? Round the answer to 3 decimal digits.\n\nd\n\n0.145",
    "crumbs": [
      "Labs - Problems",
      "Lab 2 - Problems"
    ]
  },
  {
    "objectID": "Lab2.html#section-3",
    "href": "Lab2.html#section-3",
    "title": "Lab 2",
    "section": "2.1 ",
    "text": "2.1 \n\nIn a school sport club, there are \\(20\\) children who like rugby more than football, and \\(15\\) chidlren who like football more than rugby. (Noone likes them equally.) The club wants to form a committee of \\(6\\) people: \\(3\\) football funs and \\(3\\) rugby funs. How many ways does there exist to form the committee? Store the answer in w.\n\nRemark. Note that since the choice of football funs does not depend on the choice of rugby funs, we need just to multiply the number of ways to choose fotball funs and the number of ways to choose rugby funs.\n\n\nw\n\n518700",
    "crumbs": [
      "Labs - Problems",
      "Lab 2 - Problems"
    ]
  },
  {
    "objectID": "Lab2.html#section-4",
    "href": "Lab2.html#section-4",
    "title": "Lab 2",
    "section": "2.2 ",
    "text": "2.2 \n\nLet now the committee (of 6 children) is chosen without any restrictions on the preferences of its members. Find the number of ways to form the committee. Store the answer in r.\n\nr\n\n1623160",
    "crumbs": [
      "Labs - Problems",
      "Lab 2 - Problems"
    ]
  },
  {
    "objectID": "Lab2.html#section-5",
    "href": "Lab2.html#section-5",
    "title": "Lab 2",
    "section": "2.3 ",
    "text": "2.3 \n\nFinally, find the probability p that a randomly choosen committee consisting of 6 children has equal numbers of football and rugby funs. Round to \\(4\\) decimal digits.\n\np\n\n0.3196",
    "crumbs": [
      "Labs - Problems",
      "Lab 2 - Problems"
    ]
  },
  {
    "objectID": "Lab2.html#section-6",
    "href": "Lab2.html#section-6",
    "title": "Lab 2",
    "section": "2.4 ",
    "text": "2.4 \n\nIf \\(10\\) balls are randomly drawn from a bag containing \\(17\\) blue and \\(19\\) yellow balls, what is the probability q that \\(4\\) of the balls are blue and the others are yellow? Round to \\(3\\) decimal digits.\nNote that despite the balls of the same colour are indistinguishable (in contrast to children of the same sport preference in the previous tasks), we may always think that the balls e.g. numbered, to apply the same arguments as before.\n\nq\n\n0.25404208941472567\n\n\n\n\n\n\n\n\n\nRemember\n\n\n\n\nSpecial cases: \\[\n\\binom{n}{0}=\\binom{0}{0}=1\n\\]\nSimple cases: \\[\n\\binom{n}{1} = n, \\qquad \\binom{n}{2}=\\frac{n(n-1)}{2}\n\\]\nOverall, \\[\n\\binom{n}{k} = \\frac{n\\cdot(n-1)\\cdot(n-2)\\cdot\\ldots\\cdot(n-k+1)}{1\\cdot2\\cdot3\\cdot\\ldots\\cdot k},\n\\] where numerator has \\(k\\) factors. E.g. \\[\n\\binom{37}{4}=\\frac{37\\cdot 36\\cdot 35\\cdot 34}{1\\cdot 2\\cdot 3\\cdot 4}.\n\\]",
    "crumbs": [
      "Labs - Problems",
      "Lab 2 - Problems"
    ]
  },
  {
    "objectID": "Lab2.html#section-7",
    "href": "Lab2.html#section-7",
    "title": "Lab 2",
    "section": "3.1 ",
    "text": "3.1 \n\nChoose a random element from the list coin. The output will be one of two letters h or t, e.g.:\n\n\n't'\n\n\nNote that if you run the same code again and again you may get each time another output.\nCongratulations: you made your first probability model.",
    "crumbs": [
      "Labs - Problems",
      "Lab 2 - Problems"
    ]
  },
  {
    "objectID": "Lab2.html#section-8",
    "href": "Lab2.html#section-8",
    "title": "Lab 2",
    "section": "3.2 ",
    "text": "3.2 \n\nSuppose we tossed the coin n_trials = 10 times. Generate a list called series that would contain outputs of all these trials, e.g.\n\n\n['t', 't', 'h', 'h', 't', 't', 't', 'h', 't', 't']\n\n\n(your output will be probably different).\nAdvice: it may be more convenient to use list comprehension instead of loops, though it’s up to you. Recall that range(10) command generates 10 numbers from 0 to 9.\nCount the number of “heads” in series, using property list_name.count(element_name) to get the number of elements element_name in the list list_name.\n\n\n3",
    "crumbs": [
      "Labs - Problems",
      "Lab 2 - Problems"
    ]
  },
  {
    "objectID": "Lab2.html#section-9",
    "href": "Lab2.html#section-9",
    "title": "Lab 2",
    "section": "3.3 ",
    "text": "3.3 \n\nLoad library pyplot of module matplotlib as follows:\n\nimport matplotlib.pyplot as plt\n\nChange now n_trials to 1000. We expect to see around 500 heads in a series of trials, though if you run your code, you will probably get another number. Define n_series = 1000 and repeat running the previous code n_series times, i.e. the output should be a list of n_series numbers. Store the output in a variable heads.\nAgain, you may find more useful to use list comprehension.\nCreate the scatter plot of numbers of heads in all series using the commands plt.scatter(x, heads) and plt.show(), where x would be all numbers from 0 to n_series.\nYour output should be similar to:\n\n\n\n\n\n\n\n\n\nAs you can see all results indeed gather around 500. Note that you can find min(heads) and max(heads) to see how large is the spread of numbers, e.g. on the picture above it’s\n\n[min(heads),max(heads)]\n\n[442, 548]",
    "crumbs": [
      "Labs - Problems",
      "Lab 2 - Problems"
    ]
  },
  {
    "objectID": "Lab2.html#section-10",
    "href": "Lab2.html#section-10",
    "title": "Lab 2",
    "section": "4.1 ",
    "text": "4.1 \n\nConsider again that we toss the coin \\(1000\\) times and count the number of tails, but repeat this procedure \\(10^5\\) times. Calculate the average m of the obtained result.\n\n\n500.01605\n\n\nAs expected, the result is pretty close to \\(500\\) (your result may differ from this, of course).",
    "crumbs": [
      "Labs - Problems",
      "Lab 2 - Problems"
    ]
  },
  {
    "objectID": "Lab2.html#section-11",
    "href": "Lab2.html#section-11",
    "title": "Lab 2",
    "section": "4.2 ",
    "text": "4.2 \n\nWe can also model unfair coins. The command\n\nn=10\nnp.random.choice([0,1], size=(5,n), p=[0.6, 0.4])\n\narray([[0, 1, 0, 1, 1, 0, 0, 1, 1, 0],\n       [0, 1, 0, 1, 0, 0, 1, 1, 0, 0],\n       [1, 1, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 1, 1, 1, 1, 1, 1],\n       [0, 0, 1, 0, 0, 0, 1, 0, 0, 1]])\n\n\ngenerates \\(5\\) series of \\(10\\) outputs each with the probability for head \\(0.6\\) and the probability for tail \\(0.4\\). Calculate again the average for \\(10^5\\) series of \\(1000\\) trials.\n\n\n400.05303\n\n\nThe result is pretty close to \\(400 = 0.4*1000\\). We discuss the theoretical justification for this on Week 3.",
    "crumbs": [
      "Labs - Problems",
      "Lab 2 - Problems"
    ]
  },
  {
    "objectID": "Lab3.html",
    "href": "Lab3.html",
    "title": "Lab 3",
    "section": "",
    "text": "Module scipy.stats of the famous library scipy provides various tools to work with probability and statistics in Python. Today, we consider several discrete distributions; to work with them will use the same structure of commands. In particular, any distribution will have the following methods:\n\nrvs - to generate (several) values of a random variable \\(X\\) (see below in this Lab)\npmf - to calculate probability mass function at any \\(k\\in\\mathbb{R}\\) \\[\np_X(k)=\\mathbb{P}(X=k)\n\\]\ncdf - to calculate the cumulative distribuion function \\[\nF_X(x)=\\sum_{k\\leq x} p_X(k)=\\mathbb{P}(X\\leq x)\n\\] recall that \\[\n\\mathbb{P}(a&lt;X\\leq b) = F_X(b)-F_X(a)\n\\]\nmean - to calculate the mathematical expectation (mean) \\(\\mathbb{E}(X)\\)\nvar - to calculate the varaince \\(\\mathrm{Var}(X)\\)\nstd - to calculate the standard deviation \\(\\sigma(X)=\\sqrt{\\mathrm{Var}(X)}\\)",
    "crumbs": [
      "Labs - Problems",
      "Lab 3 - Problems"
    ]
  },
  {
    "objectID": "Lab3.html#section",
    "href": "Lab3.html#section",
    "title": "Lab 3",
    "section": "2.1 ",
    "text": "2.1 \n\nLet \\(X\\sim Bin(27, 0.45)\\).\n\nCalculate \\(\\mathbb{P}(X\\leq 20)\\). Check the answer:\n\n\n\n0.9994575733214396\n\n\n\nCalculate \\(\\mathbb{P}(X &lt; 10)\\). Check the answer:\n\n\n\n0.15256903022954094\n\n\n\nCalculate \\(\\mathbb{P}(7&lt;X\\leq 13)\\). Check the answer:\n\n\n\n0.666671672666459\n\n\n\nCalculate \\(\\mathbb{P}(7\\leq X\\leq 13)\\). Check the answer:\n\n\n\n0.6879613472859423",
    "crumbs": [
      "Labs - Problems",
      "Lab 3 - Problems"
    ]
  },
  {
    "objectID": "Lab3.html#section-1",
    "href": "Lab3.html#section-1",
    "title": "Lab 3",
    "section": "2.2 ",
    "text": "2.2 \n\nA company manufactures light bulbs, and \\(95\\)% of them are of good quality, while the rest are defective. If a customer buys \\(50\\) light bulbs, what is the probability that:\n\nExactly \\(5\\) of them are defective? Check the answer:\n\n\n\n0.06584063715436628\n\n\n\nAt most \\(5\\) of them are defective? Check the answer:\n\n\n\n0.9622238270102227\n\n\n\nAt least \\(5\\) of them are defective? Check the answer:\n\n\n\n0.10361681014414348\n\n\n\nFind the expected value of the number of defective bulbs and the standard deviation of this quantity. Check the answer:\n\n\n\n[2.5, 1.541103500742244]",
    "crumbs": [
      "Labs - Problems",
      "Lab 3 - Problems"
    ]
  },
  {
    "objectID": "Lab3.html#section-2",
    "href": "Lab3.html#section-2",
    "title": "Lab 3",
    "section": "3.1 ",
    "text": "3.1 \n\nLet \\(X\\sim Geom(0.4)\\). Calculate \\(\\mathbb{P}(7\\leq X &lt;10)\\). Check the answer.\n\n\n0.03657830400000006",
    "crumbs": [
      "Labs - Problems",
      "Lab 3 - Problems"
    ]
  },
  {
    "objectID": "Lab3.html#section-3",
    "href": "Lab3.html#section-3",
    "title": "Lab 3",
    "section": "3.2 ",
    "text": "3.2 \n\nA lazy student has to take a quiz, where each question may have as an answer an integer number from \\(1\\) to \\(100\\) (different questions may have equal answers). Instead of preparation, the student is going to guess the answers. The quiz contains \\(20\\) questions. As soon as the student gives a correct answer, the quiz stops, and it is considered as a passed one. What is the probability that the student will pass the quiz?\nHint: the quiz may stop after either of \\(1,2,3,\\ldots,20\\) questions.\nCheck the answer:\n\n\n0.18209306240276918",
    "crumbs": [
      "Labs - Problems",
      "Lab 3 - Problems"
    ]
  },
  {
    "objectID": "Lab3.html#section-4",
    "href": "Lab3.html#section-4",
    "title": "Lab 3",
    "section": "4.1 ",
    "text": "4.1 \n\nLet \\(X\\sim NB(5, 0.3)\\). Find \\(\\mathbb{P}(X&gt;10)\\). Check the answer.\n\n\n0.5154910592268431",
    "crumbs": [
      "Labs - Problems",
      "Lab 3 - Problems"
    ]
  },
  {
    "objectID": "Lab3.html#section-5",
    "href": "Lab3.html#section-5",
    "title": "Lab 3",
    "section": "4.2 ",
    "text": "4.2 \n\nA lazy student has to take a quiz, where each question may have as an answer an integer number from \\(1\\) to \\(100\\) (different questions may have equal answers). Instead of preparation, the student is going to guess the answers. The quiz contains \\(20\\) questions. The quiz stops as soon as the student answers correctly \\(3\\) questions. What is the probability to pass the test for this student?\nHint: think on how many wrong answers could be made by the student to still pass the test.\nCheck the answer.\n\n\n0.0010035761681001162",
    "crumbs": [
      "Labs - Problems",
      "Lab 3 - Problems"
    ]
  },
  {
    "objectID": "Lab3.html#section-7",
    "href": "Lab3.html#section-7",
    "title": "Lab 3",
    "section": "5.1 ",
    "text": "5.1 \n\nIn an insurance company, customers’ claims are raised at an average rate of \\(5\\) claims per working day. Calculate the probability that\n\nExactly \\(30\\) claims will be raised in one working week (Monday – Friday). Check the answer.\n\n\n\n0.04541278513011904\n\n\n\nAt least \\(8\\) claims will be raised in the next \\(2\\) working days. Check the answer.\n\n\n\n0.7797793533983011",
    "crumbs": [
      "Labs - Problems",
      "Lab 3 - Problems"
    ]
  },
  {
    "objectID": "Lab3.html#section-8",
    "href": "Lab3.html#section-8",
    "title": "Lab 3",
    "section": "6.1 ",
    "text": "6.1 \n\nLet \\(X\\sim Po(3)\\) be a Poisson random variable with the parameter \\(\\lambda=3\\). Generate \\(100\\) random values of \\(X\\) fixing random_state = 111, and assign the resulting Numpy array to a variable f. Calculate the mean and the (population) variance of f. Check your answer. Don’t forget to load numpy first.\n\n\n[2.9, 2.87]\n\n\nWe know that the theoretical mean (expected value) \\(\\mathbb{E}(X)\\) and variance \\(\\mathrm{Var}(X)\\) for a Poisson random variable are equal to \\(\\lambda\\):\n\\[\n\\mathbb{E}(X)=\\mathrm{Var}(X)=\\lambda.\n\\]\nClearly, \\(2.9\\neq 3\\neq 2.87\\). To make the statistics more “matching” the probability, we need to increase the size of the data: let’s generate \\(10^6\\) random variables (keeping random_state = 111). Check the answers.\n\n\n[2.998107, 2.9899274165509997]",
    "crumbs": [
      "Labs - Problems",
      "Lab 3 - Problems"
    ]
  },
  {
    "objectID": "Lab4.html",
    "href": "Lab4.html",
    "title": "Lab 4",
    "section": "",
    "text": "Recall that the linear regression provides the “best line” that reflects the relation between two sets of data. Namely, let \\(X=(x_1,\\ldots,x_n)\\) and \\(Y=(y_1,\\ldots,y_n)\\) be vectors (arrays) of data. We define\n\\[\n\\begin{aligned}\n\\bar{x}& = \\frac1n \\sum_{i=1}^n x_i,\\\\\nS_{xx} &= \\sum_{i=1}^n(x_i-\\bar{x})^2=\\sum_{i=1}^nx_i^2-n\\bar{x}^2,\\\\\nS_{xy}&=\\sum_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y})=\\sum_{i=1}^nx_iy_i-n\\bar{x}\\bar{y},\\\\\nS_{yy}&= \\sum_{i=1}^n(y_i-\\bar{y})^2=\\sum_{i=1}^ny_i^2-n\\bar{y}^2.\n\\end{aligned}\n\\]\nThen the best fit line is\n\\[\n\\hat{y}=\\hat{\\beta}_0+\\hat{\\beta}_1 x,\n\\]\nwhere\n\\[\n\\begin{aligned}\n\\hat{\\beta}_1 &= \\dfrac{S_{xy}}{S_{xx}},\\\\\n\\hat{\\beta}_0 &= \\bar{y}-\\hat{\\beta}_1\\bar{x}.\n\\end{aligned}\n\\]\nThe strength of a linear relationship between the variables can be measured by the Pearson correlation coefficient (or just the correlation coefficient) which is given by\n\\[\nr=\\dfrac{S_{xy}}{\\sqrt{S_{xx}S_{yy}}}.\n\\]\nWe know that \\(-1\\leq r\\leq 1\\) and we say that\n\\[\n\\begin{aligned}\n|r|&gt;0.7 & \\quad \\text{means strong correlation}\\\\\n0.7\\geq |r|&gt;0.4& \\quad \\text{means moderate correlation}\\\\\n|r|\\leq 0.4 & \\quad \\text{means weak correlation}\n\\end{aligned}\n\\]\n\n\n\nDownload file Birthweight.csv and upload it to Anaconda.com/app. Assign it to the dataframe named df. Show the first rows of df.\nHint: to import CSV file, use commands discussed in Lab 1. Don’t forget about pandas library.\nYou should get the following output\n\n\n\n\n\n\n\n\n\nid\nheadcircumference\nlength\nBirthweight\nGestation\nsmoker\nmotherage\nmnocig\nmheight\nmppwt\nfage\nfedyrs\nfnocig\nfheight\nlowbwt\nmage35\nLowBirthWeight\nQCL_1\n\n\n\n\n0\n1313\n12\n17\n5.8\n33\n0\n24\n0\n58\n99\n26\n16\n0\n66\n1\n0\nLow\n1\n\n\n1\n431\n12\n19\n4.2\n33\n1\n20\n7\n63\n109\n20\n10\n35\n71\n1\n0\nLow\n1\n\n\n2\n808\n13\n19\n6.4\n34\n0\n26\n0\n65\n140\n25\n12\n25\n69\n0\n0\nNormal\n2\n\n\n3\n300\n12\n18\n4.5\n35\n1\n41\n7\n65\n125\n37\n14\n25\n68\n1\n1\nLow\n1\n\n\n4\n516\n13\n18\n5.8\n35\n1\n20\n35\n67\n125\n23\n12\n50\n73\n1\n0\nLow\n2\n\n\n\n\n\n\n\n\nAs you can see, this dataframe contains the data about newborns and their parents. (Here values of headcircumference and length are in inches and Birthweight is in pounds.)\nWe will study dependence of newborn’s weights on their lengths.\n\n\n\n\nPlot a scatter plot making length data on the horizontal axes and Birthweight data on the vertical axes. Label axes appropriately, and show in labels the units (in and lb).\nHint: use commands discussed in Lab 3. Don’t forget about matplotlib.pyplot module. Note also that matplotlib allows to use Pandas series (e.g. dataframe columns), it’s not necessary to convert them into Numpy arrays using .to_numpy() command.\n\n\n\n\n\n\n\n\n\n\n\n\nAt first, we calculate the regression line manually, using the formulas above.\n\n\n\n\n\nConvert columns length and Birthweight to Numpy arrays x and y, respectively. Assign \\(\\bar{x}\\) and \\(\\bar{y}\\) to mx and my, respectively. Note that you may use either mean or np.mean functions.\nCheck your answer:\n\n[mx, my]\n\n[19.928571428571427, 7.264285714285713]\n\n\n\n\n\n\n\nAssign values of \\(S_{xx}, S_{xy}, S_{yy}\\) to variables sxx, sxy, and syy, respectively (use the formulas at the beginning of this Lab). Note that you can use functions sum or np.sum, and remember about vector operations in Python.\nCheck the answer:\n\n[sxx, sxy, syy]\n\n[50.785714285714285, 42.292857142857144, 72.49642857142857]\n\n\n\n\n\n\n\nAssign values of \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_0\\) to variables b1 and b0, respectively. Assign the Pearson regression coefficient to variable r. (To find the square root, you may use np.sqrt function.)\nCheck the answer:\n\n[b0, b1, r]\n\n[-9.331645569620253, 0.8327707454289733, 0.6970082792022007]\n\n\n\n\n\nInstead of all these calculations, we can also use linregress class from scipy.stats module:\n\nfrom scipy.stats import linregress\nlinregress(x,y)\n\nLinregressResult(slope=0.8327707454289732, intercept=-9.331645569620253, rvalue=0.6970082792022005, pvalue=2.9301969030656806e-07, stderr=0.13546119087983002, intercept_stderr=2.7036545086005135)\n\n\nYou may notice that it gives the same answers (up to a little calculation error), where slope stands for b1 (that is indeed the slope of \\(\\hat{y}=\\hat{\\beta}_0+\\hat{\\beta}_1 x\\)), intercept stands for b0, and rvalue stands for r. You may access these values as follows:\n\nlr = linregress(x,y)\n[lr.intercept, lr.slope, lr.rvalue]\n\n[-9.331645569620253, 0.8327707454289732, 0.6970082792022005]\n\n\nthat is pretty simular to [b0, b1, r] calculated before.\nWe are going now to draw now the graph of the regression line on the scatter plot. For this, we create an array of values on the horizontal axes by dividing the interval between min(x) and max(x) by e.g. \\(100\\) parts (for this we will use np.linespace function), and calculate the values of the linear regression line at these points:\n\nimport matplotlib.pyplot as plt\nxvalues = np.linspace(min(x), max(x), 100)\nyvalues = lr.intercept + lr.slope * xvalues\nplt.plot(xvalues, yvalues, color = 'r')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCombine the regression line with the scatter plot to get the following output:\n\n\n\n\n\n\n\n\n\nAs we can see, the regression line reflects the trend between weights and lengths, however, the values are “jumping” around the line. We could see that the correlation coefficient (see lr.rvalue or r) is not large:\n\nlr.rvalue\n\n0.6970082792022005\n\n\ni.e. we see here a moderate correlation.\n\n\n\n\n\nDownload now file Experience-Salary.csv which contains data on how the salary depends on experience. Repeat the previous steps to show the scatter plot together with the regression line:\n\n\n\n\n\n\n\n\n\nYou can see that here the regression line fits the data better. Indeed, in this case the correlation is higher:\n\nlr.rvalue #If you kept the notation lr for linregress object.\n\n0.8109692945840652",
    "crumbs": [
      "Labs - Problems",
      "Lab 4 - Problems"
    ]
  },
  {
    "objectID": "Lab4.html#section",
    "href": "Lab4.html#section",
    "title": "Lab 4",
    "section": "",
    "text": "Download file Birthweight.csv and upload it to Anaconda.com/app. Assign it to the dataframe named df. Show the first rows of df.\nHint: to import CSV file, use commands discussed in Lab 1. Don’t forget about pandas library.\nYou should get the following output\n\n\n\n\n\n\n\n\n\nid\nheadcircumference\nlength\nBirthweight\nGestation\nsmoker\nmotherage\nmnocig\nmheight\nmppwt\nfage\nfedyrs\nfnocig\nfheight\nlowbwt\nmage35\nLowBirthWeight\nQCL_1\n\n\n\n\n0\n1313\n12\n17\n5.8\n33\n0\n24\n0\n58\n99\n26\n16\n0\n66\n1\n0\nLow\n1\n\n\n1\n431\n12\n19\n4.2\n33\n1\n20\n7\n63\n109\n20\n10\n35\n71\n1\n0\nLow\n1\n\n\n2\n808\n13\n19\n6.4\n34\n0\n26\n0\n65\n140\n25\n12\n25\n69\n0\n0\nNormal\n2\n\n\n3\n300\n12\n18\n4.5\n35\n1\n41\n7\n65\n125\n37\n14\n25\n68\n1\n1\nLow\n1\n\n\n4\n516\n13\n18\n5.8\n35\n1\n20\n35\n67\n125\n23\n12\n50\n73\n1\n0\nLow\n2\n\n\n\n\n\n\n\n\nAs you can see, this dataframe contains the data about newborns and their parents. (Here values of headcircumference and length are in inches and Birthweight is in pounds.)\nWe will study dependence of newborn’s weights on their lengths.",
    "crumbs": [
      "Labs - Problems",
      "Lab 4 - Problems"
    ]
  },
  {
    "objectID": "Lab4.html#section-1",
    "href": "Lab4.html#section-1",
    "title": "Lab 4",
    "section": "",
    "text": "Plot a scatter plot making length data on the horizontal axes and Birthweight data on the vertical axes. Label axes appropriately, and show in labels the units (in and lb).\nHint: use commands discussed in Lab 3. Don’t forget about matplotlib.pyplot module. Note also that matplotlib allows to use Pandas series (e.g. dataframe columns), it’s not necessary to convert them into Numpy arrays using .to_numpy() command.\n\n\n\n\n\n\n\n\n\n\n\n\nAt first, we calculate the regression line manually, using the formulas above.",
    "crumbs": [
      "Labs - Problems",
      "Lab 4 - Problems"
    ]
  },
  {
    "objectID": "Lab4.html#section-2",
    "href": "Lab4.html#section-2",
    "title": "Lab 4",
    "section": "",
    "text": "Convert columns length and Birthweight to Numpy arrays x and y, respectively. Assign \\(\\bar{x}\\) and \\(\\bar{y}\\) to mx and my, respectively. Note that you may use either mean or np.mean functions.\nCheck your answer:\n\n[mx, my]\n\n[19.928571428571427, 7.264285714285713]",
    "crumbs": [
      "Labs - Problems",
      "Lab 4 - Problems"
    ]
  },
  {
    "objectID": "Lab4.html#section-3",
    "href": "Lab4.html#section-3",
    "title": "Lab 4",
    "section": "",
    "text": "Assign values of \\(S_{xx}, S_{xy}, S_{yy}\\) to variables sxx, sxy, and syy, respectively (use the formulas at the beginning of this Lab). Note that you can use functions sum or np.sum, and remember about vector operations in Python.\nCheck the answer:\n\n[sxx, sxy, syy]\n\n[50.785714285714285, 42.292857142857144, 72.49642857142857]",
    "crumbs": [
      "Labs - Problems",
      "Lab 4 - Problems"
    ]
  },
  {
    "objectID": "Lab4.html#section-4",
    "href": "Lab4.html#section-4",
    "title": "Lab 4",
    "section": "",
    "text": "Assign values of \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_0\\) to variables b1 and b0, respectively. Assign the Pearson regression coefficient to variable r. (To find the square root, you may use np.sqrt function.)\nCheck the answer:\n\n[b0, b1, r]\n\n[-9.331645569620253, 0.8327707454289733, 0.6970082792022007]\n\n\n\n\n\nInstead of all these calculations, we can also use linregress class from scipy.stats module:\n\nfrom scipy.stats import linregress\nlinregress(x,y)\n\nLinregressResult(slope=0.8327707454289732, intercept=-9.331645569620253, rvalue=0.6970082792022005, pvalue=2.9301969030656806e-07, stderr=0.13546119087983002, intercept_stderr=2.7036545086005135)\n\n\nYou may notice that it gives the same answers (up to a little calculation error), where slope stands for b1 (that is indeed the slope of \\(\\hat{y}=\\hat{\\beta}_0+\\hat{\\beta}_1 x\\)), intercept stands for b0, and rvalue stands for r. You may access these values as follows:\n\nlr = linregress(x,y)\n[lr.intercept, lr.slope, lr.rvalue]\n\n[-9.331645569620253, 0.8327707454289732, 0.6970082792022005]\n\n\nthat is pretty simular to [b0, b1, r] calculated before.\nWe are going now to draw now the graph of the regression line on the scatter plot. For this, we create an array of values on the horizontal axes by dividing the interval between min(x) and max(x) by e.g. \\(100\\) parts (for this we will use np.linespace function), and calculate the values of the linear regression line at these points:\n\nimport matplotlib.pyplot as plt\nxvalues = np.linspace(min(x), max(x), 100)\nyvalues = lr.intercept + lr.slope * xvalues\nplt.plot(xvalues, yvalues, color = 'r')\nplt.show()",
    "crumbs": [
      "Labs - Problems",
      "Lab 4 - Problems"
    ]
  },
  {
    "objectID": "Lab4.html#section-5",
    "href": "Lab4.html#section-5",
    "title": "Lab 4",
    "section": "",
    "text": "Combine the regression line with the scatter plot to get the following output:\n\n\n\n\n\n\n\n\n\nAs we can see, the regression line reflects the trend between weights and lengths, however, the values are “jumping” around the line. We could see that the correlation coefficient (see lr.rvalue or r) is not large:\n\nlr.rvalue\n\n0.6970082792022005\n\n\ni.e. we see here a moderate correlation.",
    "crumbs": [
      "Labs - Problems",
      "Lab 4 - Problems"
    ]
  },
  {
    "objectID": "Lab4.html#section-6",
    "href": "Lab4.html#section-6",
    "title": "Lab 4",
    "section": "",
    "text": "Download now file Experience-Salary.csv which contains data on how the salary depends on experience. Repeat the previous steps to show the scatter plot together with the regression line:\n\n\n\n\n\n\n\n\n\nYou can see that here the regression line fits the data better. Indeed, in this case the correlation is higher:\n\nlr.rvalue #If you kept the notation lr for linregress object.\n\n0.8109692945840652",
    "crumbs": [
      "Labs - Problems",
      "Lab 4 - Problems"
    ]
  },
  {
    "objectID": "Lab4.html#section-7",
    "href": "Lab4.html#section-7",
    "title": "Lab 4",
    "section": "2.1 ",
    "text": "2.1 \n\nChange the code as explained and get the following summary.\n\n\nOptimization terminated successfully.\n         Current function value: 0.322141\n         Iterations 8\n\n\n\nLogit Regression Results\n\n\nDep. Variable:\nadmitted\nNo. Observations:\n29\n\n\nModel:\nLogit\nDf Residuals:\n26\n\n\nMethod:\nMLE\nDf Model:\n2\n\n\nDate:\nWed, 22 Oct 2025\nPseudo R-squ.:\n0.5348\n\n\nTime:\n14:04:23\nLog-Likelihood:\n-9.3421\n\n\nconverged:\nTrue\nLL-Null:\n-20.084\n\n\nCovariance Type:\nnonrobust\nLLR p-value:\n2.162e-05\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n-22.0171\n9.232\n-2.385\n0.017\n-40.111\n-3.923\n\n\ngmat\n0.0150\n0.014\n1.052\n0.293\n-0.013\n0.043\n\n\ngpa\n3.7604\n2.003\n1.877\n0.060\n-0.166\n7.686\n\n\n\n\n\n\nAgain, the coefficients are available using (in the previous notations) reg.params. As before, Intercept stands for \\(\\hat{\\beta}_0\\), gmat stands for \\(\\hat{\\beta}_1\\), and also gpa stands for \\(\\hat{\\beta}_2\\) in\n\\[\n\\mathrm{logit} (p) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2\n\\]\nand hence\n\\[\np = \\frac{e^{\\hat{\\beta}_0+\\hat{\\beta}_1 x_1+\\hat{\\beta}_2 x_2}}{1+e^{\\hat{\\beta}_0+\\hat{\\beta}_1 x_1+\\hat{\\beta}_2 x_2}}\n\\]\nWe can show how \\(p\\) separates the values (some lines of the code may be new for you - it’s just for your information, you are not required to learn them):\n\nx1 = df['gmat'].to_numpy()\nx2 = df['gpa'].to_numpy()\np = df['admitted'].to_numpy()\nfig = plt.figure()\nax = fig.add_subplot(1, 1, 1, projection='3d', computed_zorder=False)\nx1values = np.linspace(min(x1), max(x1), 100)\nx2values = np.linspace(min(x2), max(x2), 100)\n[X1, X2] = np.meshgrid(x1values, x2values)\nb0 = reg.params.iloc[0]\nb1 = reg.params.iloc[1]\nb2 = reg.params.iloc[2]\npvalues = np.exp(b0 + b1 * X1 + b2 * X2)/(1 + np.exp(b0 + b1 * X1 + b2 * X2))\nax.plot_surface(X1, X2, pvalues, color = 'r', alpha = 0.4)\nax.scatter(x1, x2, p)\nplt.show()\n\n\n\n\n\n\n\n\nAs you can see, the red graph (surface) of \\(p\\) separates values of \\(0\\) and \\(1\\). Again, we may try to predict the admission for the student with data stored in df_test. We assigned to gmat the corresponding mark, now we do the same for gpa and calculate p for these two values. As you can see, the result is much closer to \\(0\\), hence, we are more confident in our (correct) prediction that the student would not be admitted.\n\ngpa = df_test['gpa']\nnp.exp(b0 + b1 * gmat + b2 * gpa)/(1 + np.exp(b0 + b1 * gmat + b2 * gpa))\n\n0.07118806995711177\n\n\n\nLogistic regression for 3 independent variables\nNow, we consider the dependence of admitted on all three values: gmat, gpa, and work_experience. Surely, in this case, we will not be able to draw \\(p\\) (as it would be a 4-dimensional diagram), but we can calculate \\(\\hat{\\beta}_0,\\hat{\\beta}_1,\\hat{\\beta}_2,\\hat{\\beta}_3\\).",
    "crumbs": [
      "Labs - Problems",
      "Lab 4 - Problems"
    ]
  },
  {
    "objectID": "Lab4.html#section-8",
    "href": "Lab4.html#section-8",
    "title": "Lab 4",
    "section": "2.2 ",
    "text": "2.2 \n\nFind the coefficients of\n\\[\n\\mathrm{logit} (p) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2 + \\hat{\\beta}_3 x_3\n\\]\n\n\nOptimization terminated successfully.\n         Current function value: 0.255306\n         Iterations 8\n\n\nCheck your answer:\n\nreg.params\n\nIntercept         -16.182243\ngmat                0.002624\ngpa                 3.258770\nwork_experience     0.994371\ndtype: float64",
    "crumbs": [
      "Labs - Problems",
      "Lab 4 - Problems"
    ]
  },
  {
    "objectID": "Lab4.html#section-9",
    "href": "Lab4.html#section-9",
    "title": "Lab 4",
    "section": "2.3 ",
    "text": "2.3 \n\nAssign to w_exp the work experience value for the student from df_test and calculate the function\n\\[\np = \\frac{e^{\\hat{\\beta}_0+\\hat{\\beta}_1 x_1+\\hat{\\beta}_2 x_2+\\hat{\\beta}_3 x_3}}{1+e^{\\hat{\\beta}_0+\\hat{\\beta}_1 x_1+\\hat{\\beta}_2 x_2+\\hat{\\beta}_3 x_3}}\n\\]\nfor that student. Check the answer:\n\n\n0.3133331286316772\n\n\n\nAs you can see, the information about a relatively high work experience (5 years in this case), increased chances to be admitted, though the non-admission is still more likely.\nNote that the rest of information from summary actually explains the level of certainty we may have in the future prediction (we do not consider this now). Note also that, in practice, one predicts outcomes for a number of students (the dataframe df_test would contain a lot of rows), and the prediction is “good” if one predicted correctly for a big percentage of them.",
    "crumbs": [
      "Labs - Problems",
      "Lab 4 - Problems"
    ]
  },
  {
    "objectID": "Lab5.html",
    "href": "Lab5.html",
    "title": "Lab 5",
    "section": "",
    "text": "Continuous probability distributions in Python can be treated similarly to the discrete distributions. For example, for the uniform distribution we start with\n\nfrom scipy.stats import uniform\n\nWe know that a uniform random variables \\(X\\) depends on two parameters \\(a\\) and \\(b\\), namely, \\(X\\sim U(a,b)\\) is distributed uniformly on a segment \\([a,b]\\). The length of this segment is \\(b-a\\). In Python, the uniform distribution also depends on two parameters: loc = a and scale = b - a. The default values (which are used if these parameters are omitted) are loc = 0 and scale = 1 that corresponds to \\(U(0,1)\\).\n\n\nThe PDF \\(f_X(x)\\) is then 1/scale on \\([a,b]\\) and \\(0\\) otherwise. For example, for \\(X\\sim(1,6)\\), we can calculate\n\nuniform.pdf(3, loc = 1, scale = 5)\n\n0.2\n\n\nthat is \\(\\frac15\\), whereas,\n\nuniform.pdf(7, loc = 1, scale = 5)\n\n0.0\n\n\nas \\(7\\notin[1,6]\\).\nWe can also plot the graph of the PDF for \\(X\\sim U(1,6)\\), e.g. we plot it for \\(x\\in[-2,9]\\):\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nx = np.linspace(-2, 9, 1000)\nplt.plot(x, uniform.pdf(x, loc = 1, scale = 5))\nplt.show()\n\n\n\n\n\n\n\n\nNot that the jumps at \\(x=1\\) and \\(x=6\\) are shown by vertical segments.\n\n\n\nSimilarly, CDF \\(F_X(x)\\) of \\(X\\sim U(a,b)\\) can be calculated by using the command uniform.cdf(x, loc = a, scale = b - a).\n\n\n\n\nPlot the graph of \\(F_X(x)\\) for \\(X\\sim U(1,6)\\) on the interval \\([-2,9]\\). Check the output:\n\n\n\n\n\n\n\n\n\n\n\n\n\\(n\\) values of the random variable \\(X\\sim U(a,b)\\) can be generated using the command uniform.rvs(size = n, loc = a, scale = b - a). As we discussed on Lab 3, if we want to fix the output, we use the key random_state, for example, the following output\n\nuniform.rvs(size = 3, loc = 1, scale = 5, random_state = 1)\n\narray([3.08511002, 4.60162247, 1.00057187])\n\n\nwill be the same each time you run the code, whereas if you omit random_state = 1 the result will be different every time you run the code (try!)\n\n\n\n\n\nAssign to variable x the array of \\(10^6\\) random values uniformly distributed on \\([0,1]\\), fix the random state equal to 123. Calculate the mean of \\(x\\). Check the output.\n\n\n0.49993343872814583\n\n\nAs you can see the output is pretty close to \\(0.5 =  \\frac{0+1}{2}=\\mathbb{E}(X)\\).",
    "crumbs": [
      "Labs - Problems",
      "Lab 5 - Problems"
    ]
  },
  {
    "objectID": "Lab5.html#section",
    "href": "Lab5.html#section",
    "title": "Lab 5",
    "section": "",
    "text": "Plot the graph of \\(F_X(x)\\) for \\(X\\sim U(1,6)\\) on the interval \\([-2,9]\\). Check the output:\n\n\n\n\n\n\n\n\n\n\n\n\n\\(n\\) values of the random variable \\(X\\sim U(a,b)\\) can be generated using the command uniform.rvs(size = n, loc = a, scale = b - a). As we discussed on Lab 3, if we want to fix the output, we use the key random_state, for example, the following output\n\nuniform.rvs(size = 3, loc = 1, scale = 5, random_state = 1)\n\narray([3.08511002, 4.60162247, 1.00057187])\n\n\nwill be the same each time you run the code, whereas if you omit random_state = 1 the result will be different every time you run the code (try!)",
    "crumbs": [
      "Labs - Problems",
      "Lab 5 - Problems"
    ]
  },
  {
    "objectID": "Lab5.html#section-1",
    "href": "Lab5.html#section-1",
    "title": "Lab 5",
    "section": "",
    "text": "Assign to variable x the array of \\(10^6\\) random values uniformly distributed on \\([0,1]\\), fix the random state equal to 123. Calculate the mean of \\(x\\). Check the output.\n\n\n0.49993343872814583\n\n\nAs you can see the output is pretty close to \\(0.5 =  \\frac{0+1}{2}=\\mathbb{E}(X)\\).",
    "crumbs": [
      "Labs - Problems",
      "Lab 5 - Problems"
    ]
  },
  {
    "objectID": "Lab5.html#section-2",
    "href": "Lab5.html#section-2",
    "title": "Lab 5",
    "section": "2.1 ",
    "text": "2.1 \n\nGenerate \\(10^6\\) values of the random variable \\(X\\sim \\mathrm{Exp}(0.2)\\), using the random_state key equal to 12. Calculate the variance of the generated values, using the formula \\[\n\\mathrm{Var}(X) = \\mathbb{E}(X^2)- \\bigl(\\mathbb{E}(X)\\bigr)^2\n\\] Check the answer.\n\n\n25.04882818201701\n\n\nYou can also use np.var command. Check the output in this case:\n\n\n25.048828182017044\n\n\nThe results are almost identical (there is always some numerical error). Moreover, the result is close to the theoretical value \\(\\frac1{0.2^2}=25\\).\n\n\nPercentiles\nAnother important function available for all random variables in scipy.stats module is ppf, which provides percentiles. By the definition, for a random variable \\(X\\) and for any \\(q\\in[0,1]\\), the \\(q\\)-percentile of \\(X\\) is the number \\(a\\) such that \\[\nF_X(a) = \\mathbb{P}(X\\leq a) = q.\n\\] In other words, the percentile is the inverse function to CDF.\nFor example, for \\(X\\sim \\mathrm{Exp}(0.2)\\),\n\na = expon.ppf(0.3, scale = 1/0.2)\na\n\n1.7833747196936622\n\n\nis \\(0.3\\)-percentile of \\(X\\), and then we can see that\n\nexpon.cdf(a, scale = 1/0.2)\n\n0.30000000000000004\n\n\nis effectively the initial \\(0.3\\).",
    "crumbs": [
      "Labs - Problems",
      "Lab 5 - Problems"
    ]
  },
  {
    "objectID": "Lab5.html#section-3",
    "href": "Lab5.html#section-3",
    "title": "Lab 5",
    "section": "2.2 ",
    "text": "2.2 \n\nLet \\(X\\sim \\mathrm{Exp}(0.7)\\). Find \\(b\\) such that \\[\n\\mathbb{P}(1 \\leq X \\leq b) = 0.4.\n\\] Hint: use first the formula \\[\n\\mathbb{P}(a \\leq X \\leq b) = F_X(b) - F_X(a).\n\\] Check the answer:\n\n\n3.3390409771454475",
    "crumbs": [
      "Labs - Problems",
      "Lab 5 - Problems"
    ]
  },
  {
    "objectID": "Lab5.html#section-4",
    "href": "Lab5.html#section-4",
    "title": "Lab 5",
    "section": "3.1 ",
    "text": "3.1 \n\nPlot the graph of CDF for \\(X\\sim \\mathcal{N}(2,3^2)\\) (also on the interval \\(x\\in(-10,14)\\)). Use green colour and label the axis with \\(x\\) and \\(y=F(x)\\).",
    "crumbs": [
      "Labs - Problems",
      "Lab 5 - Problems"
    ]
  },
  {
    "objectID": "Lab5.html#section-5",
    "href": "Lab5.html#section-5",
    "title": "Lab 5",
    "section": "3.2 ",
    "text": "3.2 \n\nCalculate the probability that a randomly selected individual has a height between \\(160\\) cm and \\(170\\) cm, given that the population mean height is \\(165.5\\) cm and the standard deviation is \\(10.2\\) cm, and that the heights follow the normal distribution.\nSub-task 1: calculate the answer using functions for general normal random variable \\(X\\sim \\mathcal{N}(\\mu,\\sigma^2)\\). Check the answer:\n\n\n0.37558835807069463\n\n\nSub-task 2: recalculate the answer using functions for standard normal random variable \\(Z\\sim \\mathcal{N}(0,1)\\). Check the answer:\n\n\n0.37558835807069463",
    "crumbs": [
      "Labs - Problems",
      "Lab 5 - Problems"
    ]
  },
  {
    "objectID": "Lab5.html#section-6",
    "href": "Lab5.html#section-6",
    "title": "Lab 5",
    "section": "3.3 ",
    "text": "3.3 \n\nLet \\(X\\sim\\mathcal{N}(12,5^2)\\). Find \\(c\\) such that \\[\n\\mathbb{P}(c \\leq X \\leq 15) = 0.5.\n\\] Check the answer.\n\n\n8.235364852055193",
    "crumbs": [
      "Labs - Problems",
      "Lab 5 - Problems"
    ]
  },
  {
    "objectID": "Lab5.html#section-7",
    "href": "Lab5.html#section-7",
    "title": "Lab 5",
    "section": "3.4 ",
    "text": "3.4 \n\nGenerate \\(10^6\\) values of standard normal random variable. Check their standard deviation (use np.std function) and ensure that the result is close to \\(1\\).\n\n\n0.999753865697528\n\n\n(you may get a different answer, as we didn’t fix random_state here).",
    "crumbs": [
      "Labs - Problems",
      "Lab 5 - Problems"
    ]
  },
  {
    "objectID": "Lab6.html",
    "href": "Lab6.html",
    "title": "Lab 6",
    "section": "",
    "text": "Consider an example similar to the one discussed on a lecture: let \\(X:\\Omega\\to\\{0,1\\}\\) and \\(Y:\\Omega\\to\\{0, 1, 2, 3\\}\\) be two discrete random random variables whose joint probability mass function\n\\[\np_{X,Y}(x_i,y_j)=\\mathbb{P}(X=x_i, Y=y_j), \\qquad x_i\\in \\{0,1\\}, y_j\\in \\{0.1,2,3\\}\n\\]\nis given through the following table:\n\n\n\n\\(X\\) \\(\\backslash\\) \\(Y\\)\n\\(0\\)\n\\(1\\)\n\\(2\\)\n\\(3\\)\n\n\n\n\n\\(0\\)\n\\(0.1\\)\n\\(0.15\\)\n\\(0.05\\)\n\\(0.25\\)\n\n\n\\(1\\)\n\\(0.15\\)\n\\(0.05\\)\n\\(0.15\\)\n\\(0.1\\)\n\n\n\n(so that here, e.g. \\(p_{X,Y}(0,0)=0.1\\), \\(p_{X,Y}(0,3)=0.25\\) and so on).\nThe values of \\(p_{X,Y}\\) naturally form a matrix, of size \\(2\\times 4\\). To work with this matrix in Python, we will use two-dimensional Numpy-arrays:\n\nimport numpy as np\npxy = np.array([[0.1, 0.15, 0.05, 0.25],\n                [0.15, 0.05, 0.15, 0.1]])\npxy\n\narray([[0.1 , 0.15, 0.05, 0.25],\n       [0.15, 0.05, 0.15, 0.1 ]])\n\n\nNote that indents inside np.array are NOT important, you may write e.g. everything in one line.\nRecall that marginal distributions (i.e. distributions of \\(X\\) and of \\(Y\\) separately) can be obtained from this table by summing over rows (for \\(X\\)) or columns (for \\(Y\\)):\n\\[\n\\begin{aligned}\np_X(x_i)&=\\mathbb{P}(X=x_i) = \\sum_{j}p_{X,Y}(x_i,y_j),\\\\\np_Y(y_j)&=\\mathbb{P}(Y=y_j) = \\sum_{i}p_{X,Y}(x_i,y_j).\n\\end{aligned}\n\\]\nIn Numpy, we can sum-up along any dimension, called axis. The first dimension (rows) corresponds to axis = 0, the second dimension (columns) corresponds toaxis = 1. However, to get sums of elements in each row of array pxy we can write np.sum(pxy, axis = 1) or pxy.sum(axis = 1):\n\npx = np.sum(pxy, axis = 1)\npx\n\narray([0.55, 0.45])\n\n\ni.e. \\(\\mathbb{P}(X=0)=0.55\\) (the sum of the first row) and \\(\\mathbb{P}(X=1)=0.45\\) (the sum of the second row).\nThe reason of axis = 1 here is that these sums naturally form a column:\n\n\n\n\\(X\\) \\(\\backslash\\) \\(Y\\)\n\\(0\\)\n\\(1\\)\n\\(2\\)\n\\(3\\)\nsum of row\n\n\n\n\n\\(0\\)\n\\(0.1\\)\n\\(0.15\\)\n\\(0.05\\)\n\\(0.25\\)\n\\(0.55\\)\n\n\n\\(1\\)\n\\(0.15\\)\n\\(0.05\\)\n\\(0.15\\)\n\\(0.1\\)\n\\(0.45\\)\n\n\n\nSimilarly, to get the sums of all columns we write np.sum(pxy, axis = 0) or pxy.sum(axis = 0) as the results form a row:\n\npy = pxy.sum(axis = 0)\npy\n\narray([0.25, 0.2 , 0.2 , 0.35])\n\n\ni.e. \\(\\mathbb{P}(Y=0)=0.25\\) (the sum of the first column) and so on:\n\n\n\n\\(X\\) \\(\\backslash\\) \\(Y\\)\n\\(0\\)\n\\(1\\)\n\\(2\\)\n\\(3\\)\n\n\n\n\n\\(0\\)\n\\(0.1\\)\n\\(0.15\\)\n\\(0.05\\)\n\\(0.25\\)\n\n\n\\(1\\)\n\\(0.15\\)\n\\(0.05\\)\n\\(0.15\\)\n\\(0.1\\)\n\n\nsum of column\n\\(0.25\\)\n\\(0.2\\)\n\\(0.2\\)\n\\(0.35\\)\n\n\n\nAnother way of thinking is that axis = 0 tells “to get rid of” rows, so only columns would remain (that’s why py has 4 components, the number of columns in the original pxy), whereas axis = 1 tells to get rid of columns so only 2 rows remain in px.\nNote that, as it should be, the sum of all probabilities for \\(X\\) is \\(1\\), and the same sum for \\(Y\\) is \\(1\\). Again, we can use either np.sum(px) or px.sum() command:\n\n[np.sum(px), py.sum()]\n\n[1.0, 1.0]\n\n\nThis corresponds to\n\\[\n\\sum_{i}\\sum_{j}p_{X,Y}(x_i,y_j)=1.\n\\]\nWe will also need the array of values of \\(X\\) and of \\(Y\\). In this case, they can be defined manually:\n\nx = np.array([0,1])\nx\n\narray([0, 1])\n\n\nor using e.g. arange function:\n\ny = np.arange(4)\ny\n\narray([0, 1, 2, 3])\n\n\nWe can e.g. calculate\n\\[\n\\mathbb{E}(X) = x_1\\cdot \\mathbb{P}(X=x_1)+\\ldots + x_n\\cdot \\mathbb{P}(X=x_n)\n\\]\nWe discussed how to do this using basic entry-wise product of arrays in Numpy, however, it can be also done faster, using the dot-product of the arrays x and px (you will study the dot-product or inner-product of vectors in MA-M26). Namely, for vectors \\(a=(a_1,\\ldots,a_n)\\) and \\(b=(b_1,\\ldots,b_n)\\),\n\\[\na \\cdot b = a_1b_1+\\ldots+b_n b_n.\n\\] In Numpy, we can write just a.dot(b), or b.dot(a) (the results will be the same).\n\n\n\nCalculate \\(\\mathbb{E}(X)\\) and \\(\\mathbb{E}(Y)\\) and assign the results to mx and my, respectively.\nCheck the answer:\n\n[mx, my]\n\n[0.44999999999999996, 1.65]\n\n\n(Surely, \\(\\mathbb{E}(X)=0\\cdot 0.55+1\\cdot 0.45=0.45\\), so here it’s just a rounding error.)\n\nRecall that\n\\[\n\\mathbb{E}(XY) = \\sum_{i}\\sum_{j} x_i\\cdot  y_j \\cdot p_{X,Y}(x_i,y_j)=\\sum_{i}x_i\\cdot \\sum_{j}  y_j \\cdot p_{X,Y}(x_i,y_j).\n\\]\nFor each \\(x_i\\), the second sum \\(\\sum\\limits_{j}  y_j\\cdot p_{X,Y}(x_i,y_j)\\) is nothing but the \\(i\\)-th component of the vector \\(Qy\\), where \\(Q\\) is the matrix whose entries are \\(p_{X,Y}(x_i,y_j)\\):\n\\[\nQ = \\begin{pmatrix}\np_{X,Y}(x_1,y_1) & \\ldots & p_{X,Y}(x_1,y_n)\\\\\n\\vdots & \\vdots &\\vdots\\\\\np_{X,Y}(x_m,y_1) & \\ldots & p_{X,Y}(x_m,y_n)\n\\end{pmatrix}\n\\]\n(in our example, \\(m=2\\), \\(x_1=0\\), \\(x_2=1\\), \\(n=4\\), \\(y_1=0\\), \\(y_2=1\\), \\(y_3=2\\), \\(y_4=3\\)).\nand for \\(y=(y_1,\\ldots, y_n)\\), we indeed have that \\(Qy = ((Qy)_1, \\ldots, (Qy)_m)\\) and\n\\[\n(Qy)_i = \\sum_{j}  y_j \\cdot p_{X,Y}(x_i,y_j).\n\\]\nThe Numpy command is the same Q.dot(y) (in our case Q was previously denoted pxy), though here the order is important (we multiply a matrix by a vector).\n\nQy = pxy.dot(y)\nQy\n\narray([1.  , 0.65])\n\n\nand now\n\\[\n\\mathbb{E}(XY) = \\sum_{i} x_i \\cdot (Qy)_i = x\\cdot Qy,\n\\]\ni.e.\n\nmxy = x.dot(Qy)\nmxy\n\n0.65\n\n\nSurely, it was possible to do not introduce the intermediate Qy and claculate all in one command:\n\nx.dot(pxy.dot(y))\n\n0.65\n\n\n\n\n\n\nFind \\(\\mathrm{cov}(X,Y)=\\mathbb{E}(XY) - \\mathbb{E}(X)\\mathbb{E}(Y)\\) and assign it to covxy.\nCheck the answer:\n\ncovxy\n\n-0.09249999999999992\n\n\n\n\n\n\n\nFind \\(\\sigma(X)\\) and \\(\\sigma(Y)\\) using the formulas\n\\[\n\\sigma^2(X)=\\mathrm{Var}(X)= \\mathbb{E}(X^2)- (\\mathbb{E}(X))^2\n\\]\nand assign them to sx,sy, respectively. Recall that the square root can be obtained by using np.sqrt function.\nCheck the answer:\n\n[sx, sy]\n\n[0.49749371855331, 1.1947803145348523]\n\n\n\n\n\n\n\nFind \\(\\mathrm{corr}(X,Y)=\\frac{\\mathrm{cov}(X,Y)}{\\sigma(X)\\sigma(Y)}\\) and assign it to corrxy.\nCheck the answer:\n\ncorrxy\n\n-0.15562023709382963\n\n\nTherefore, \\(X\\) and \\(Y\\) are weakly negatively correlated.\n\n\n\n\n\n\n\n\nDownload file jointdistr.csv, and upload it to anaconda.com/app. Import Pandas library and load the data from this file to Pandas data frame df.\nCheck the result:\n\ndf.head()\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n\n\n0\n0.003\n0.005\n0.001\n0.002\n0.004\n0.001\n0.005\n0.004\n0.004\n0.004\n0.003\n0.006\n0.001\n0.004\n0.006\n0.001\n0.006\n0.003\n0.005\n\n\n1\n0.001\n0.006\n0.002\n0.006\n0.001\n0.006\n0.006\n0.002\n0.002\n0.002\n0.003\n0.003\n0.006\n0.006\n0.001\n0.006\n0.002\n0.001\n0.001\n\n\n2\n0.001\n0.001\n0.006\n0.004\n0.004\n0.001\n0.004\n0.006\n0.005\n0.006\n0.006\n0.004\n0.001\n0.004\n0.003\n0.006\n0.001\n0.001\n0.005\n\n\n3\n0.005\n0.005\n0.002\n0.004\n0.006\n0.006\n0.006\n0.004\n0.001\n0.005\n0.006\n0.006\n0.001\n0.006\n0.003\n0.006\n0.006\n0.001\n0.001\n\n\n4\n0.001\n0.006\n0.006\n0.002\n0.001\n0.001\n0.004\n0.001\n0.002\n0.002\n0.001\n0.001\n0.002\n0.005\n0.005\n0.001\n0.003\n0.006\n0.001\n\n\n\n\n\n\n\n\nWe can also inspect the size of the data frame:\n\ndf.shape\n\n(15, 19)\n\n\nIt is possible to work with the data frame directly, but for simplicity, we convert its content into Numpy array:\n\npxy = df.to_numpy()\n\nLook at the output of pxy (it’s quite long).\n\n\n\n\nLet \\(X\\in\\{0,\\ldots,14\\}\\) and \\(Y\\in\\{0,\\ldots,18\\}\\) be discrete random variables with the joint distribution given by the table above. Find \\(\\mathrm{corr}(X,Y)\\).\nCheck the answer:\n\ncorrxy\n\n-0.014393645358846184",
    "crumbs": [
      "Labs - Problems",
      "Lab 6 - Problems"
    ]
  },
  {
    "objectID": "Lab6.html#section",
    "href": "Lab6.html#section",
    "title": "Lab 6",
    "section": "",
    "text": "Calculate \\(\\mathbb{E}(X)\\) and \\(\\mathbb{E}(Y)\\) and assign the results to mx and my, respectively.\nCheck the answer:\n\n[mx, my]\n\n[0.44999999999999996, 1.65]\n\n\n(Surely, \\(\\mathbb{E}(X)=0\\cdot 0.55+1\\cdot 0.45=0.45\\), so here it’s just a rounding error.)\n\nRecall that\n\\[\n\\mathbb{E}(XY) = \\sum_{i}\\sum_{j} x_i\\cdot  y_j \\cdot p_{X,Y}(x_i,y_j)=\\sum_{i}x_i\\cdot \\sum_{j}  y_j \\cdot p_{X,Y}(x_i,y_j).\n\\]\nFor each \\(x_i\\), the second sum \\(\\sum\\limits_{j}  y_j\\cdot p_{X,Y}(x_i,y_j)\\) is nothing but the \\(i\\)-th component of the vector \\(Qy\\), where \\(Q\\) is the matrix whose entries are \\(p_{X,Y}(x_i,y_j)\\):\n\\[\nQ = \\begin{pmatrix}\np_{X,Y}(x_1,y_1) & \\ldots & p_{X,Y}(x_1,y_n)\\\\\n\\vdots & \\vdots &\\vdots\\\\\np_{X,Y}(x_m,y_1) & \\ldots & p_{X,Y}(x_m,y_n)\n\\end{pmatrix}\n\\]\n(in our example, \\(m=2\\), \\(x_1=0\\), \\(x_2=1\\), \\(n=4\\), \\(y_1=0\\), \\(y_2=1\\), \\(y_3=2\\), \\(y_4=3\\)).\nand for \\(y=(y_1,\\ldots, y_n)\\), we indeed have that \\(Qy = ((Qy)_1, \\ldots, (Qy)_m)\\) and\n\\[\n(Qy)_i = \\sum_{j}  y_j \\cdot p_{X,Y}(x_i,y_j).\n\\]\nThe Numpy command is the same Q.dot(y) (in our case Q was previously denoted pxy), though here the order is important (we multiply a matrix by a vector).\n\nQy = pxy.dot(y)\nQy\n\narray([1.  , 0.65])\n\n\nand now\n\\[\n\\mathbb{E}(XY) = \\sum_{i} x_i \\cdot (Qy)_i = x\\cdot Qy,\n\\]\ni.e.\n\nmxy = x.dot(Qy)\nmxy\n\n0.65\n\n\nSurely, it was possible to do not introduce the intermediate Qy and claculate all in one command:\n\nx.dot(pxy.dot(y))\n\n0.65",
    "crumbs": [
      "Labs - Problems",
      "Lab 6 - Problems"
    ]
  },
  {
    "objectID": "Lab6.html#section-1",
    "href": "Lab6.html#section-1",
    "title": "Lab 6",
    "section": "",
    "text": "Find \\(\\mathrm{cov}(X,Y)=\\mathbb{E}(XY) - \\mathbb{E}(X)\\mathbb{E}(Y)\\) and assign it to covxy.\nCheck the answer:\n\ncovxy\n\n-0.09249999999999992",
    "crumbs": [
      "Labs - Problems",
      "Lab 6 - Problems"
    ]
  },
  {
    "objectID": "Lab6.html#section-2",
    "href": "Lab6.html#section-2",
    "title": "Lab 6",
    "section": "",
    "text": "Find \\(\\sigma(X)\\) and \\(\\sigma(Y)\\) using the formulas\n\\[\n\\sigma^2(X)=\\mathrm{Var}(X)= \\mathbb{E}(X^2)- (\\mathbb{E}(X))^2\n\\]\nand assign them to sx,sy, respectively. Recall that the square root can be obtained by using np.sqrt function.\nCheck the answer:\n\n[sx, sy]\n\n[0.49749371855331, 1.1947803145348523]",
    "crumbs": [
      "Labs - Problems",
      "Lab 6 - Problems"
    ]
  },
  {
    "objectID": "Lab6.html#section-3",
    "href": "Lab6.html#section-3",
    "title": "Lab 6",
    "section": "",
    "text": "Find \\(\\mathrm{corr}(X,Y)=\\frac{\\mathrm{cov}(X,Y)}{\\sigma(X)\\sigma(Y)}\\) and assign it to corrxy.\nCheck the answer:\n\ncorrxy\n\n-0.15562023709382963\n\n\nTherefore, \\(X\\) and \\(Y\\) are weakly negatively correlated.",
    "crumbs": [
      "Labs - Problems",
      "Lab 6 - Problems"
    ]
  },
  {
    "objectID": "Lab6.html#section-4",
    "href": "Lab6.html#section-4",
    "title": "Lab 6",
    "section": "",
    "text": "Download file jointdistr.csv, and upload it to anaconda.com/app. Import Pandas library and load the data from this file to Pandas data frame df.\nCheck the result:\n\ndf.head()\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n\n\n0\n0.003\n0.005\n0.001\n0.002\n0.004\n0.001\n0.005\n0.004\n0.004\n0.004\n0.003\n0.006\n0.001\n0.004\n0.006\n0.001\n0.006\n0.003\n0.005\n\n\n1\n0.001\n0.006\n0.002\n0.006\n0.001\n0.006\n0.006\n0.002\n0.002\n0.002\n0.003\n0.003\n0.006\n0.006\n0.001\n0.006\n0.002\n0.001\n0.001\n\n\n2\n0.001\n0.001\n0.006\n0.004\n0.004\n0.001\n0.004\n0.006\n0.005\n0.006\n0.006\n0.004\n0.001\n0.004\n0.003\n0.006\n0.001\n0.001\n0.005\n\n\n3\n0.005\n0.005\n0.002\n0.004\n0.006\n0.006\n0.006\n0.004\n0.001\n0.005\n0.006\n0.006\n0.001\n0.006\n0.003\n0.006\n0.006\n0.001\n0.001\n\n\n4\n0.001\n0.006\n0.006\n0.002\n0.001\n0.001\n0.004\n0.001\n0.002\n0.002\n0.001\n0.001\n0.002\n0.005\n0.005\n0.001\n0.003\n0.006\n0.001\n\n\n\n\n\n\n\n\nWe can also inspect the size of the data frame:\n\ndf.shape\n\n(15, 19)\n\n\nIt is possible to work with the data frame directly, but for simplicity, we convert its content into Numpy array:\n\npxy = df.to_numpy()\n\nLook at the output of pxy (it’s quite long).",
    "crumbs": [
      "Labs - Problems",
      "Lab 6 - Problems"
    ]
  },
  {
    "objectID": "Lab6.html#section-5",
    "href": "Lab6.html#section-5",
    "title": "Lab 6",
    "section": "",
    "text": "Let \\(X\\in\\{0,\\ldots,14\\}\\) and \\(Y\\in\\{0,\\ldots,18\\}\\) be discrete random variables with the joint distribution given by the table above. Find \\(\\mathrm{corr}(X,Y)\\).\nCheck the answer:\n\ncorrxy\n\n-0.014393645358846184",
    "crumbs": [
      "Labs - Problems",
      "Lab 6 - Problems"
    ]
  },
  {
    "objectID": "Lab6.html#section-6",
    "href": "Lab6.html#section-6",
    "title": "Lab 6",
    "section": "2.1 ",
    "text": "2.1 \n\nRepeat the previous calculations fr \\(n=50\\), use the same random_state = 12. Assign \\(\\overline{X}_n\\) to xbar.\nCheck the answer:\n\nxbar\n\n3.06\n\n\nThat is much closer to \\(\\mathbb{E}(X)=3\\).\n\nSurely, if you change random_state, you will get another sample and another value of \\(\\overline{X}_n\\), that may be more or less closer to \\(\\mathbb{E}(X)\\). Let now generate many such samples (of the same size \\(n=50\\)).\nWe will use a loop for this. The following code generates \\(K=50\\) samples of length \\(n=50\\) of values of \\(X\\sim Bin(10,0.3)\\) and store the sample means in the list Xbar (we use the loop index as the random state, it’s not compulsory, of course):\n\nn = 50\nK = 50\nXbar = []\nfor i in range(K):\n        x = binom.rvs(10, 0.3, size = n, random_state = i)\n        Xbar.append(x.mean())\n\nLook at the output of Xbar.\nNote that more “Pythonic”-way would be to use the list comprehension:\n\nXbar = [np.mean(binom.rvs(10, 0.3, size = n, random_state = i)) for i in range(K)]\n\nWe will work with a Numpy array, not just a list:\n\nXbar = np.array(Xbar)\n\nNow, suppose we want to calculate \\(\\mathbb{P}(\\overline{X}_{n=50}&lt;2.9)\\). We may look at the proportion of the entries of Xbar which are less than 2.9: namely, Xbar&lt;2.9 is the list of True and False, where True = 1 and False = 0. Hence, the sum of this list gives the number of True among all \\(K\\) trials. Dividing by \\(K\\) we get the desired proportion:\n\np = sum(Xbar &lt; 2.9)/K\np\n\n0.28\n\n\nNext, we know that \\(\\mu=\\mathbb{E}(X)=3\\) and \\(\\sigma=\\sigma(X)=\\sqrt{10\\cdot0.3\\cdot(1-0.3)}=1.449\\), or:\n\nbinom.std(10, 0.3)\n\n1.4491376746189437\n\n\nThen\n\\[\n\\begin{aligned}\n\\mathbb{P}(\\overline{X}_{n=50}&lt;2.9)&=\n\\mathbb{P}\\biggl(\\frac{\\overline{X}_{n}-\\mu}{\\frac{\\sigma}{\\sqrt{n}}}&lt;\\frac{2.9-\\mu}{\\frac{\\sigma}{\\sqrt{n}}}\\biggr)\\\\\n&=\\mathbb{P}(Z_n &lt;\\underbrace{\\frac{2.9-3}{\\frac{1.449}{\\sqrt{50}}}}_{\\color{red}=z})\n\\end{aligned}\n\\]\n\nz = (2.9 - binom.mean(10, 0.3))/(binom.std(10, 0.3)/np.sqrt(n))\nz\n\n-0.48795003647426705\n\n\nBy CLT, since \\(z&lt;0\\),\n\\[\n\\mathbb{P}(\\overline{X}_{n=50}&lt;2.9)\\approx \\Phi(z)=\\Phi(-0.488).\n\\]\nWe have\n\nfrom scipy.stats import norm\nnorm.cdf(z)\n\n0.31279261576216244\n\n\nand we see that the value is quite close to the previously found p. Recall that p was the frequency of the event \\(\\overline{X}_{n=50}&lt;2.9\\) in \\(K=50\\) trials, rather than real probability.",
    "crumbs": [
      "Labs - Problems",
      "Lab 6 - Problems"
    ]
  },
  {
    "objectID": "Lab6.html#section-7",
    "href": "Lab6.html#section-7",
    "title": "Lab 6",
    "section": "2.2 ",
    "text": "2.2 \n\nRepeat the previous considerations find p for \\(K=1000\\).\nCheck the answer:\n\np\n\n0.306\n\n\nThe answer now is closer to the desired \\(\\Phi(z)\\).\n\n\nIf you have time\nWe can also build the histogram for e.g. \\(Z_n=\\frac{\\overline{X}_n-\\mu}{\\frac{\\sigma}{\\sqrt{n}}}\\) and compare it visually with the PDF of the standard normal distribution:\n\nimport matplotlib.pyplot as plt\nZ = (Xbar - binom.mean(10, 0.3))/(binom.std(10, 0.3)/np.sqrt(n))\nplt.hist(Z, bins = 10, density = True) # To have density values, i.e. the frequences\nx = np.linspace(min(Z), max(Z), 100)\nplt.plot(x, norm.pdf(x), color = 'r')\nplt.show()",
    "crumbs": [
      "Labs - Problems",
      "Lab 6 - Problems"
    ]
  },
  {
    "objectID": "Lab6.html#section-8",
    "href": "Lab6.html#section-8",
    "title": "Lab 6",
    "section": "2.3 ",
    "text": "2.3 \n\nGenerate \\(K=10000\\) samples of \\(n=50\\) exponential random variables with the parameter \\(\\lambda=0.1\\). Use random_state equal to loop index as before. Calculate \\(p\\approx\\mathbb{P}(\\overline{X}_{50}&lt;9.5)\\) from the frequency of the corresponding events. Calculate the approximate value \\(q=\\Phi(z)\\) of \\(\\mathbb{P}(\\overline{X}_{50}&lt;10)\\) using the central limit theorem. (Don’t forget that expon from scipy.stats has the parameter scale inverse to \\(\\lambda\\).)\nCheck the difference:\n\nnp.abs(p-q)\n\n0.01326319508411844",
    "crumbs": [
      "Labs - Problems",
      "Lab 6 - Problems"
    ]
  },
  {
    "objectID": "Lab7.html",
    "href": "Lab7.html",
    "title": "Lab 7",
    "section": "",
    "text": "As you know from the Lecture Notes, we use \\(z\\)-tests if we either know that the data is distributed normally or if the sample is large enough so we may assume that its averaged value is distributed normally according to the central limit theorem. Note that, to use \\(z\\)-tests, we have to know the variance of the distribution.\nRecall the main quantities we may need:\n\n\nE.g. in the case of one sample, it’s\n\\[\nz=\\frac{\\bar{x}-\\mu}{\\dfrac{\\sigma}{\\sqrt{n}}}\n\\tag{1}\\]\nThis works e.g. for the estimation of \\(\\mu\\) of \\(n\\) observations from \\(\\mathcal{N}(\\mu, \\sigma^2)\\) (when \\(\\sigma^2\\) is known); and this also works for large samples.\n\n\n\nInformally speaking, \\(p\\)-value describes the area of the “tail” of distribution bounded by the test statistic. On the diagram below, the tails are shaded in pink. Recall that \\(\\varphi(z)=f_Z(z)\\) is the PDF of the standard normal distribution, where \\(Z\\sim \\mathcal{N}(0,1)\\). The pink areas are then related to the CDF \\(\\Phi_Z\\) of \\(Z\\sim \\mathcal{N}(0,1)\\), namely:\n\n\n\n\n\n\n\n\n\n\n\n\nIf the test statistic \\(z&lt;0\\), then the area of the tail left to the \\(z\\) is \\(\\Phi(z)=F_Z(z)\\).\nIf the test statistic \\(z&gt;0\\), then the area of the tail right to the \\(z\\) is \\(1-\\Phi(z)=1-F_Z(z)\\).\nIf we consider the two-tailed case (e.g. the alternative hypothesis \\(H_1\\) is \\(\\mu\\neq\\mu_0\\)), then \\(p\\)-value is the sum of the tail pink areas (i.e. \\(p\\) is twice bigger than each of them).\nIf we consider the one-tailed case (e.g. \\(H_1\\) is \\(\\mu&lt;\\mu_0\\) or \\(H_1\\) is \\(\\mu&gt;\\mu_0\\)), the \\(p\\)-value is either of the (equal) tail pink areas.\n\n\n\n\n\nWe reject the null-hypothesis \\(H_0\\) if \\(p\\leq \\alpha\\).\nWe do not reject the null-hypothesis \\(H_0\\) if \\(p&gt;\\alpha\\).\n\n\n\n\n\\(z\\)-scores provide another approach to test the hypothesis (i.e. practically we calculate either \\(p\\)-value or \\(z\\)-score).\nIn simple words, \\(z_\\mathrm{score}\\) is the number such that its \\(p\\)-value would be \\(\\alpha\\). Therefore, to find \\(z_\\mathrm{score}\\), we would use the inverse function to \\(\\Phi\\), i.e. the percentile (see Lab 5). We will choose \\(z_\\mathrm{score}\\) so that its sign is the same as it is for the test statistic.\n\n\n\n\n\n\n\n\n\n\n\n\nWe reject \\(H_0\\), if the test statistic \\(z\\) is inside the critical region constructed by \\(z_\\mathrm{score}\\) (the pink area), i.e. if \\(z&gt;z_\\mathrm{score}&gt;0\\) or if \\(z&lt;z_\\mathrm{score}&lt;0\\).\nWe do note reject \\(H_0\\) otherwise.\n\n\n\n\n\nConsider an example from the Lecture notes.\nA machine produces items having a nominal mass of \\(1\\) kg. The mass of a randomly selected item \\(x\\) follows the distribution \\(X\\sim \\mathcal{N}(\\mu, 0.02^2)\\). If \\(\\mu\\neq 1\\) then the machinery should be corrected. The mean mass of a randomly sample of \\(25\\) items was found to be \\(0.989\\) kg. Test the null hypothesis that \\(H_0:\\mu=1\\) at the \\(1\\%\\) significance level.\nNote that the alternative hypothesis here is \\(\\mu\\neq1\\), i.e. we deal with the two-tailed test.\nWe have\n\nmu = 1\nsigma = 0.02\nn = 25\nxbar = 0.989\nalpha = 0.01\n\nand we can calculate the test statistic. Recall that, for the square root function we need either use an additional module, e.g. import numpy as np and then np.sqrt(n) (this is reasonable in the case we use numpy also for some other purposes) or we just use that \\(\\sqrt{n}=n^{\\frac12}\\), namely, we write by Equation 1,\n\nz = (xbar - mu) / (sigma / (n ** 0.5))\nz\n\n-2.750000000000002\n\n\nSince the test statistic is negative, to calculate the \\(p\\)-value we consider the \\(\\Phi(z)\\), and since this is the two-tailed case, we have that \\(p=2\\Phi(z)\\). Namely, we calculate\n\nfrom scipy.stats import norm\np = 2 * norm.cdf(z)\np\n\n0.0059595264701090694\n\n\nSince \\(p &lt; \\alpha = 0.01\\), i.e. the significance of the \\(p\\)-value is below the significance level, we reject \\(H_0\\).\nAlternatively, we can use the \\(z\\)-score approach (that is exactly what you use doing calculations by hand and using the statistical tables), we need to use norm.ppf function (discussed on Lab 5); since this is the two-tailed case, we calculate it at \\(\\frac{\\alpha}2\\), that is then the area of the left pink tail on Figure 2, as we need to choose the negative \\(z_\\mathrm{score}\\):\n\nzscore = norm.ppf(alpha/2)\nzscore\n\n-2.575829303548901\n\n\nSince \\(z&lt;z_\\mathrm{score}\\), i.e. the test statistic is inside the critical (left tail, left pink) region, we reject \\(H_0\\).\n\n\n\nThe number of strokes a golfer takes to complete a round of golf has mean \\(84.1\\) and standard deviation \\(2.6\\). After a month of holidays without playing golf her mean is now \\(85.1\\) in \\(36\\) subsequent rounds. At the \\(5\\%\\) significance level test the null hypothesis that her standard of play is unaltered against the alternative hypothesis that it became worse, i.e.\n\n\\(H_0: \\mu=84.1\\)\n\\(H_1: \\mu&gt;84.1\\) (one-tailed)\n\n\n\n\n\nCalculate the test statistic and assign it to z.\nCheck the answer:\n\nz\n\n2.3076923076923075\n\n\n\n\n\n\n\nCalculate the \\(p\\)-value, taking into account that \\(z&gt;0\\) and we test the one-tailed case.\nCheck the answer:\n\np\n\n0.01050812811375934\n\n\nSince \\(p&lt;0.05=\\alpha\\), we reject \\(H_0\\).\n\n\n\n\n\nAlternatively, calculate the \\(z\\)-score correspondent to the considered significance level (and assign it to zscore). Note that we are going to compare the \\(z\\)-score with the test statistic, hence, we will be looking for the positive \\(z\\)-score (as the test statistic is positive).\nCheck the answer:\n\nzscore\n\n1.6448536269514722\n\n\n\nSince \\(0&lt;z_\\mathrm{score}&lt;z\\), we have that the test statistic is inside the critical (tail) region, and we will reject \\(H_0\\).",
    "crumbs": [
      "Labs - Problems",
      "Lab 7 - Problems"
    ]
  },
  {
    "objectID": "Lab7.html#section",
    "href": "Lab7.html#section",
    "title": "Lab 7",
    "section": "",
    "text": "Calculate the test statistic and assign it to z.\nCheck the answer:\n\nz\n\n2.3076923076923075",
    "crumbs": [
      "Labs - Problems",
      "Lab 7 - Problems"
    ]
  },
  {
    "objectID": "Lab7.html#section-1",
    "href": "Lab7.html#section-1",
    "title": "Lab 7",
    "section": "",
    "text": "Calculate the \\(p\\)-value, taking into account that \\(z&gt;0\\) and we test the one-tailed case.\nCheck the answer:\n\np\n\n0.01050812811375934\n\n\nSince \\(p&lt;0.05=\\alpha\\), we reject \\(H_0\\).",
    "crumbs": [
      "Labs - Problems",
      "Lab 7 - Problems"
    ]
  },
  {
    "objectID": "Lab7.html#section-2",
    "href": "Lab7.html#section-2",
    "title": "Lab 7",
    "section": "",
    "text": "Alternatively, calculate the \\(z\\)-score correspondent to the considered significance level (and assign it to zscore). Note that we are going to compare the \\(z\\)-score with the test statistic, hence, we will be looking for the positive \\(z\\)-score (as the test statistic is positive).\nCheck the answer:\n\nzscore\n\n1.6448536269514722\n\n\n\nSince \\(0&lt;z_\\mathrm{score}&lt;z\\), we have that the test statistic is inside the critical (tail) region, and we will reject \\(H_0\\).",
    "crumbs": [
      "Labs - Problems",
      "Lab 7 - Problems"
    ]
  },
  {
    "objectID": "Lab7.html#section-3",
    "href": "Lab7.html#section-3",
    "title": "Lab 7",
    "section": "2.1 ",
    "text": "2.1 \n\nDownload file Heights.csv, upload it to Anaconda cloud, and load its content to Pandas data frame df_heights (see Lab 1 if you don’t remember how to do this).\nCheck the first rows to see the structure:\n\ndf_heights.head()\n\n\n\n\n\n\n\n\nHEIGHTS\n\n\n\n\n0\n6.50\n\n\n1\n6.25\n\n\n2\n6.33\n\n\n3\n6.50\n\n\n4\n6.42\n\n\n\n\n\n\n\n\nWe assume that it is known that this sample of heights has normal distribution.",
    "crumbs": [
      "Labs - Problems",
      "Lab 7 - Problems"
    ]
  },
  {
    "objectID": "Lab7.html#section-4",
    "href": "Lab7.html#section-4",
    "title": "Lab 7",
    "section": "2.2 ",
    "text": "2.2 \n\nAssign the values of the only column of this data frame to Numpy array x. Assign to n the size of the sample, i.e. the length of x using function len.\nCheck the answer:\n\nn\n\n65\n\n\n\nRecall that, to calculate the sample standard deviation \\(s\\) (that has \\(n-1\\) in the denominator), we need to use the key ddof = 1, e.g.\n\nimport numpy as np\na = np.array([1,2,3])\nnp.std(a, ddof = 1)\n\n1.0\n\n\nNote that we can also write\n\na.std(ddof = 1)\n\n1.0\n\n\ni.e. instead of applying function np.std to a we can use the property (method) std of the Numpy-array a. The same trick works for other statistical characteristics, e.g. a.mean() and np.mean(a) give the same.",
    "crumbs": [
      "Labs - Problems",
      "Lab 7 - Problems"
    ]
  },
  {
    "objectID": "Lab7.html#section-5",
    "href": "Lab7.html#section-5",
    "title": "Lab 7",
    "section": "2.3 ",
    "text": "2.3 \n\nAssign to xbar the mean of x. Assign to s the sample standard deviation of x.\nCheck the answers:\n\n[xbar, s]\n\n[6.467692323784616, 0.3302970630589082]",
    "crumbs": [
      "Labs - Problems",
      "Lab 7 - Problems"
    ]
  },
  {
    "objectID": "Lab7.html#section-6",
    "href": "Lab7.html#section-6",
    "title": "Lab 7",
    "section": "2.4 ",
    "text": "2.4 \n\nWe are going to use a one-sample \\(t\\)-test to determine whether the heights in the data frame has a mean of \\(\\mu = 6.5\\). Assign to tstat the corresponding test statistic, using Equation 2.\nCheck the answer\n\ntstat\n\n-0.7886016620455049\n\n\nHere \\(H_0: \\mu=6.5\\) and \\(H_1: \\mu\\neq 6.5\\), i.e. we deal with the two-tailed test.\nWe can calculate \\(p\\)-value, recall that for the one-sample test we should take \\(t\\)-distribution with \\(n-1\\) degrees of freedom. Since the test statistic is negative, we have\n\np = 2 * t.cdf(tstat, df = n-1)\np\n\n0.433256350170654\n\n\nTherefore, if we consider any significance level below this \\(p\\): e.g. \\(5\\%=0.05\\), then we do not have any evidence to reject \\(H_0\\).\n\nAlternatively, we calculate the \\(t\\)-score corresponding to the chosen significance level, e.g. \\(\\alpha = 0.05\\). Since this is two-tailed test, we calculate the percentile at \\(\\frac{\\alpha}{2}\\):\n\ntscore = t.ppf(0.05/2, df = n-1)\ntscore\n\n-1.997729654317693\n\n\nSince (the negative) \\(t\\)-score is smaller than (the negative) \\(t\\)-statistic, the latter lies outside of the critical interval, and again, we can’t reject \\(H_0\\).\nApparently, scipy.stats module proposes a faster way to calculate both the test statistic and the \\(p\\)-value for the \\(1\\)-sample \\(t\\)-test:\n\nfrom scipy.stats import ttest_1samp\nmu = 6.5 # If you haven't assign it before\nttest_1samp(x, mu)\n\nTtestResult(statistic=-0.7886016620455049, pvalue=0.433256350170654, df=64)\n\n\nSurely the answers are the same as we found previously. They are also accessible directly:\n\nresults = ttest_1samp(x, mu)\nresults.pvalue\n\n0.433256350170654\n\n\n(and similarly for results.statistic).",
    "crumbs": [
      "Labs - Problems",
      "Lab 7 - Problems"
    ]
  },
  {
    "objectID": "Lab7.html#section-7",
    "href": "Lab7.html#section-7",
    "title": "Lab 7",
    "section": "2.5 ",
    "text": "2.5 \n\nDownload the file HealthData.csv, upload it to Anaconda.com/app, and consider the column DENSITY. Use a one-sample \\(t\\)-test to determine whether the density variable in the data set Health Data has a mean of \\(1.051\\) using the \\(5\\)% significance level. Assign the corresponding \\(p\\)-value to p (you may use either of the considered approaches).\nCheck the answer:\n\np\n\n0.00017145125375254356\n\n\nAs you can see \\(p\\)-value is less than the significance level: surely, you check this in Python :-)\n\np &lt; 0.05\n\nTrue\n\n\nHence, we reject the null-hypothesis that \\(\\mu=1.051\\).\n\n\nTwo-sample test\nWe consider now examples of two-sample tests.\nFirst, we will use a paired \\(t\\)-test to determine whether there is any difference in the two processes (Process A and Process B) of preserving meat joints. This data can be found in the file MeatJoints.csv, and we will test at the 5% significance level whether the means of the two processes are equal.\nSo, we have \\(H_0: \\mu_A=\\mu_B\\) and \\(H_1: \\mu_A\\neq \\mu_B\\).",
    "crumbs": [
      "Labs - Problems",
      "Lab 7 - Problems"
    ]
  },
  {
    "objectID": "Lab7.html#section-8",
    "href": "Lab7.html#section-8",
    "title": "Lab 7",
    "section": "2.6 ",
    "text": "2.6 \n\nLoad the the data to Anaconda.com/app, and assign to \\(a\\) the column Process A and to \\(b\\) the column Process B.\nThen use the following command to apply two-sample test\n\nfrom scipy.stats import ttest_rel\nresults = ttest_rel(a,b)\nresults\n\nTtestResult(statistic=-2.29517764444372, pvalue=0.047371692861499864, df=9)\n\n\nSince\n\nresults.pvalue &lt; 0.05\n\nTrue\n\n\nwe reject \\(H_0\\).",
    "crumbs": [
      "Labs - Problems",
      "Lab 7 - Problems"
    ]
  },
  {
    "objectID": "Lab7.html#section-9",
    "href": "Lab7.html#section-9",
    "title": "Lab 7",
    "section": "3.1 ",
    "text": "3.1 \n\nLoad the file to Anaconda.com/app and assign to f the heights of football players and to b the heights of basketball players.\nThen perform the following test:\n\nfrom scipy.stats import ttest_ind\nresults = ttest_ind(f, b, nan_policy='omit')\nresults\n\nTtestResult(statistic=-3.684107948156318, pvalue=0.00040777606606915155, df=83.0)\n\n\nNote the key nan_policy='omit' as b and f have different sizes here.\nSince\n\nresults.pvalue &lt; 0.05\n\nTrue\n\n\nwe reject the null hypothesis that the mean heights of basketball and football players are equal.",
    "crumbs": [
      "Labs - Problems",
      "Lab 7 - Problems"
    ]
  },
  {
    "objectID": "Lab8.html",
    "href": "Lab8.html",
    "title": "Lab 8",
    "section": "",
    "text": "Let \\(X\\) be a random variable whose distribution depends on a parameter \\(\\theta\\in\\mathbb{R}\\), or on a group of parameters, \\(\\theta=(\\theta_1,\\ldots,\\theta_k)\\). Let \\(x_1,\\ldots,x_n\\) be a sample of values of the random variable \\(X\\). The likelihood function \\(\\mathcal{L}(\\theta\\mid x_1,\\ldots,x_n)\\) is defined as follows:\n\nfor a discrete random variable \\(X\\) with the probability mass function \\(p_X(x)=\\mathbb{P}(X=x)\\),\n\n\\[\n\\mathcal{L}(\\theta\\mid x_1,\\ldots,x_n) = p_X(x_1)\\cdot\\ldots\\cdot p_X(x_n);\n\\]\n\nfor a continuous random variable \\(X\\) with the probability density function \\(f_X(x)\\),\n\n\\[\n\\mathcal{L}(\\theta\\mid x_1,\\ldots,x_n) = f_X(x_1)\\cdot\\ldots\\cdot f_X(x_n).\n\\]\nThe log-likelihood function is the (natural) logarithm of the likelihood:\n\\[\n\\begin{aligned}\nL(\\theta\\mid x_1,\\ldots,x_n): &= \\ln \\mathcal{L}(\\theta\\mid x_1,\\ldots,x_n)\\\\\n&= \\ln p_X(x_1)+\\ldots + \\ln p_X(x_n)\n\\end{aligned}\n\\]\nfor the discrete case; and the same formula with \\(p_X\\) replaced by \\(f_X\\) holds for the continuous case.\n\n\n\nSuppose we are given a sample\n\nimport numpy as np\ndata = np.array([8,  4,  7, 11,  9,  7,  5,  9,  8,  7])\n\nAnd suppose that we are told that this is a sample generated by a binomial random variable \\(X\\sim Bin(20,p)\\), i.e \\(X:\\Omega\\to\\{0,1,\\ldots,20\\}\\) with\n\\[\n\\mathbb{P}(X=k)= \\binom{20}{k} p^k (1-p)^{n-k}.\n\\]\nhowever, the probability \\(p\\) of a “success” is unknown (i.e. here \\(\\theta=p\\)). One needs to find the value of \\(p\\) which would make the probability to observe data the highest possible.\nThe following code would do the needed:\n\nfrom scipy.stats import binom, fit\nbounds = [(20,20), (0, 1)]\nfit(binom, data, bounds)\n\n  params: FitParams(n=20.0, p=0.37500002089341816, loc=0.0)\n success: True\n message: 'Optimization terminated successfully.'\n\n\nLet’s discuss this code from the end. The function fit has three arguments:\n\nbinom is the class of the distributions (from scipy.stats) in which we are looking for a best fit;\ndata is the given sample;\nbounds sets bounds for the parameters of the considered distribution: in this case, binom has two parameters: \\(X\\sim Bin(n,p)\\), i.e. there are \\(n\\) and \\(p\\). \\(n\\) is given to be \\(20\\) (we know the number of attempts), whereas \\(p\\in[0,1]\\) is the unknown probability. Note that bounds is a Python list, whose entries are tuples. Tuples are very similar to lists, the main difference is that the tuple can’t be changed after it is defined (whereas a list may be e.g. extended or some its elements may be changed or removed). Each tuple sets the lower and the upper bounds for the corresponding parameter. So, for \\(n\\) we request \\(20\\leq n\\leq 20\\) (that just means \\(n=20\\), as needed), and for \\(p\\) we have \\(0\\leq p\\leq 1\\).\n\nThe output tells us that the best possible \\(n=20.0\\) (that is not surprising as we requested this) and the best possible \\(p\\approx 0.375\\) (we ignore loc for now). The values are accessible:\n\nres = fit(binom, data, bounds)\nres.params\n\nFitParams(n=20.0, p=0.3749999983579152, loc=0.0)\n\n\nso that there are two parameters, and the maximum likelihood estimator is the second of them:\n\nres.params[1]\n\n0.3749999983579152\n\n\n(and res.params[0] would return 20.0).\n\nRemark. You may notice that the values of p appeared to be slightly different. It’s because the internal algorithm of how fit maximises the likelihood function is the so-called stochastic algorithm, it has a random output (though pretty close to the “real” point of maximum).\n\nIn the first task, we will test the work of fit function for the case when we know the answer. Recall that, to generate e.g. \\(5\\) values (outputs) of a Bernoulli random variable \\(X\\in\\{0,1\\}\\) with \\(\\mathbb{P}(X=1)=p\\) for some \\(p\\), e.g. for \\(p=0.4\\), we need to use the code\n\nfrom scipy.stats import bernoulli\np = 0.4\nbernoulli.rvs(p, size = 5, random_state = 10)\n\narray([1, 0, 1, 1, 0], dtype=int64)\n\n\nHere, recall, random_state can be any integer number, but if you use 10 your output will be exactly like above.\n\nRemark. Note also that you could write here more “full” code\n\nbernoulli.rvs(p = p, size = 5, random_state = 10)\n\narray([1, 0, 1, 1, 0], dtype=int64)\n\n\nDon’t be confused with p = p: the first p here is the name of an argument of function bernoulli.rvs (indeed, just such a short name), where the second p in p = p is the name of the variable we defined previously. Python allows such clashes (as it is clear what is what). Moreover, this is a kind of standard coding practice.\n\n\n\n\n\nGenerate \\(n = 100\\) values of a Bernoulli random variable \\(X\\) with \\(\\mathbb{P}(X=1)=0.3\\), using random_state = 100, and assign the resulting Numpy-array to the variable data_ber.\nCheck the output by calculating its sum:\n\ndata_ber.sum()\n\n25\n\n\nStress that the calculated sum is the number of \\(1\\)-s in the generated data.\n\n\n\n\n\nFind the maximum likelihood estimate for \\(p=\\mathbb{P}(X=1)\\), using fit function for the data data_ber (stress that bernoulli has only one parameter, denoted below by p, hence, bounds should contain only one tuple for the range of \\(p\\in[0,1]\\)). Assign the result to res_ber.\nCheck the output:\n\n\nFitParams(p=0.24999999423017177, loc=0.0)\n\n\nAs we can see, the answer is pretty close to the real maximum likelihood estimate (known from lectures), which \\(\\frac{k}{n}\\), where \\(k\\) is the number of \\(1\\)-s and \\(n\\) is the total number of trials: the absolution value of the difference:\n\nnp.abs(data_ber.sum()/n - res_ber.params[0])\n\n5.76982822630967e-09\n\n\nOn the other hand, the maximum likelihood estimator does not recover the real probability p = 0.3 with which the data data_ber was generated. This because the sample size is relatively small.\n\n\n\n\n\nRepeat the previous steps but generate now a sample of the size \\(n = 100000\\). Assign the output of fit function to res_big_ber.\nCheck the found value:\n\nres_big_ber.params[0]\n\n0.2998799962872814\n\n\nso the result is much closer to \\(0.3\\).\n\n\n\nLet’s now return back to the dataset data. Suppose now that we know only that it follows the binomial distribution: \\(X\\sim Bin(n,p)\\), where \\(n\\) is not know exactly, but we expect that \\(1\\leq n\\leq 25\\). We can then modify the bounds for \\(n\\):\n\nbounds = [(1, 25), (0, 1)]\nres = fit(binom, data, bounds)\nres.params\n\nFitParams(n=14.0, p=0.5357142714632469, loc=0.0)\n\n\nThus, the highest probability to see the sample data, for the given restriction on \\(n\\), would be if \\(X\\sim Bin(14, \\approx 0.5357)\\).\nMoreover, the output of fit function (which we denoted res) can be plotted:\n\nimport matplotlib.pyplot as plt \nres.plot()\nplt.show()\n\n\n\n\n\n\n\n\n\nRemark. Similarly, to the tasks above, the data here was initially generated by very different parameters: \\(X\\sim Bin(18, 0.435)\\). However, the fitting for so small sample can’t reconstruct this. The sample is random, and it appeared that among all \\(Bin(n,p)\\) the distribution \\(X\\sim Bin(14, \\approx 0.5357)\\) is the closest (the most typical) for such sample.\n\n\n\n\n\n\nGenerate \\(n=10\\) random values of the normal distribution of \\(X\\sim \\mathcal{N}(10, 1.5^2)\\) (see Lab 5 (solutions): Task 3.4 for rvs function and e.g. Task 3.3 for using loc argument for the mean and scale for the standard deviation; don’t forget to import norm). Use random_state = 123. Assign the output to data_norm.\nCheck yourself by calculating mean and standard deviation of the output:\n\n[data_norm.mean(), data_norm.std()]\n\n[9.595725834510507, 1.8544572022485344]\n\n\nAs you can see, the sample is not large enough to “catch” the original mean, though, it’s relatively close.\n\n\n\n\n\nFit the data to a normal distribution: find the normal distribution \\(\\mathcal{N}(\\mu,\\sigma^2)\\) with \\(\\mu\\in [7,12]\\) and \\(\\sigma\\in[1,2]\\) which is most likely to be the distribution for the sample data_norm. Assign the result to res_norm.\nCheck the output:\n\nres_norm.params\n\nFitParams(loc=9.595723817982465, scale=1.8544586662018996)\n\n\nAs you can see, the fitting reflects the characteristics of the sample, not the initial distribution (as the sample was not large enough).\n\n\nRemark. Some of distributions in scipy.stats have own fit methods which do not require using bounds. For example, one can write\n\nnorm.fit(data_norm)\n\n(9.595725834510507, 1.8544572022485344)\n\n\nIn other words, Python looked here among all \\(\\mathcal{N}(\\mu,\\sigma^2)\\), without any restrictions on \\(\\mu\\) and \\(\\sigma\\).",
    "crumbs": [
      "Labs - Problems",
      "Lab 8 - Problems"
    ]
  },
  {
    "objectID": "Lab8.html#section",
    "href": "Lab8.html#section",
    "title": "Lab 8",
    "section": "",
    "text": "Generate \\(n = 100\\) values of a Bernoulli random variable \\(X\\) with \\(\\mathbb{P}(X=1)=0.3\\), using random_state = 100, and assign the resulting Numpy-array to the variable data_ber.\nCheck the output by calculating its sum:\n\ndata_ber.sum()\n\n25\n\n\nStress that the calculated sum is the number of \\(1\\)-s in the generated data.",
    "crumbs": [
      "Labs - Problems",
      "Lab 8 - Problems"
    ]
  },
  {
    "objectID": "Lab8.html#section-1",
    "href": "Lab8.html#section-1",
    "title": "Lab 8",
    "section": "",
    "text": "Find the maximum likelihood estimate for \\(p=\\mathbb{P}(X=1)\\), using fit function for the data data_ber (stress that bernoulli has only one parameter, denoted below by p, hence, bounds should contain only one tuple for the range of \\(p\\in[0,1]\\)). Assign the result to res_ber.\nCheck the output:\n\n\nFitParams(p=0.24999999423017177, loc=0.0)\n\n\nAs we can see, the answer is pretty close to the real maximum likelihood estimate (known from lectures), which \\(\\frac{k}{n}\\), where \\(k\\) is the number of \\(1\\)-s and \\(n\\) is the total number of trials: the absolution value of the difference:\n\nnp.abs(data_ber.sum()/n - res_ber.params[0])\n\n5.76982822630967e-09\n\n\nOn the other hand, the maximum likelihood estimator does not recover the real probability p = 0.3 with which the data data_ber was generated. This because the sample size is relatively small.",
    "crumbs": [
      "Labs - Problems",
      "Lab 8 - Problems"
    ]
  },
  {
    "objectID": "Lab8.html#section-2",
    "href": "Lab8.html#section-2",
    "title": "Lab 8",
    "section": "",
    "text": "Repeat the previous steps but generate now a sample of the size \\(n = 100000\\). Assign the output of fit function to res_big_ber.\nCheck the found value:\n\nres_big_ber.params[0]\n\n0.2998799962872814\n\n\nso the result is much closer to \\(0.3\\).\n\n\n\nLet’s now return back to the dataset data. Suppose now that we know only that it follows the binomial distribution: \\(X\\sim Bin(n,p)\\), where \\(n\\) is not know exactly, but we expect that \\(1\\leq n\\leq 25\\). We can then modify the bounds for \\(n\\):\n\nbounds = [(1, 25), (0, 1)]\nres = fit(binom, data, bounds)\nres.params\n\nFitParams(n=14.0, p=0.5357142714632469, loc=0.0)\n\n\nThus, the highest probability to see the sample data, for the given restriction on \\(n\\), would be if \\(X\\sim Bin(14, \\approx 0.5357)\\).\nMoreover, the output of fit function (which we denoted res) can be plotted:\n\nimport matplotlib.pyplot as plt \nres.plot()\nplt.show()\n\n\n\n\n\n\n\n\n\nRemark. Similarly, to the tasks above, the data here was initially generated by very different parameters: \\(X\\sim Bin(18, 0.435)\\). However, the fitting for so small sample can’t reconstruct this. The sample is random, and it appeared that among all \\(Bin(n,p)\\) the distribution \\(X\\sim Bin(14, \\approx 0.5357)\\) is the closest (the most typical) for such sample.",
    "crumbs": [
      "Labs - Problems",
      "Lab 8 - Problems"
    ]
  },
  {
    "objectID": "Lab8.html#section-3",
    "href": "Lab8.html#section-3",
    "title": "Lab 8",
    "section": "",
    "text": "Generate \\(n=10\\) random values of the normal distribution of \\(X\\sim \\mathcal{N}(10, 1.5^2)\\) (see Lab 5 (solutions): Task 3.4 for rvs function and e.g. Task 3.3 for using loc argument for the mean and scale for the standard deviation; don’t forget to import norm). Use random_state = 123. Assign the output to data_norm.\nCheck yourself by calculating mean and standard deviation of the output:\n\n[data_norm.mean(), data_norm.std()]\n\n[9.595725834510507, 1.8544572022485344]\n\n\nAs you can see, the sample is not large enough to “catch” the original mean, though, it’s relatively close.",
    "crumbs": [
      "Labs - Problems",
      "Lab 8 - Problems"
    ]
  },
  {
    "objectID": "Lab8.html#section-4",
    "href": "Lab8.html#section-4",
    "title": "Lab 8",
    "section": "",
    "text": "Fit the data to a normal distribution: find the normal distribution \\(\\mathcal{N}(\\mu,\\sigma^2)\\) with \\(\\mu\\in [7,12]\\) and \\(\\sigma\\in[1,2]\\) which is most likely to be the distribution for the sample data_norm. Assign the result to res_norm.\nCheck the output:\n\nres_norm.params\n\nFitParams(loc=9.595723817982465, scale=1.8544586662018996)\n\n\nAs you can see, the fitting reflects the characteristics of the sample, not the initial distribution (as the sample was not large enough).\n\n\nRemark. Some of distributions in scipy.stats have own fit methods which do not require using bounds. For example, one can write\n\nnorm.fit(data_norm)\n\n(9.595725834510507, 1.8544572022485344)\n\n\nIn other words, Python looked here among all \\(\\mathcal{N}(\\mu,\\sigma^2)\\), without any restrictions on \\(\\mu\\) and \\(\\sigma\\).",
    "crumbs": [
      "Labs - Problems",
      "Lab 8 - Problems"
    ]
  },
  {
    "objectID": "Lab8.html#section-5",
    "href": "Lab8.html#section-5",
    "title": "Lab 8",
    "section": "2.1 ",
    "text": "2.1 \n\nAssign to logprobabilities the array of natural logarithms of all entries in probabilities. Assign to loglikelihood the sum of all these logarithms.\nCheck the output:\n\nloglikelihood\n\n-21.08353673407401\n\n\n\n\nRemark. Note that the distributions in scipy.stats contains functions logpmf and logpdf for calculation logarithms of PMF (for discrete random variables) and PDF (for continuous random variables), respectively. We could write also:\n\nlogprobabilities = binom.logpmf(data, n = n, p = p)\nloglikelihood = logprobabilities.sum()\nloglikelihood\n\n-21.08353673407403\n\n\n\nNow, we are going to calculate the log-likelihood for a range of \\(p\\in[0,1]\\). We, hence, define a function which combines all previous steps. Let’s call e.g. loglbinom, it will have only one argument: the value of \\(p\\); we keep the value of \\(n\\) and data fixed. We can do this in one line\n\ndef loglbinom(p):\n    return binom.logpmf(data, n = n, p = p).sum()\n\nIt can be also written “longer”:\n\ndef loglbinom1(p):\n    probabilities = binom.pmf(data, n = n, p = p)\n    logprobabilities = np.log(probabilities)\n    loglikelihood = logprobabilities.sum()\n    return loglikelihood\n\nLet’s check that it would be the same as in the previous computations for \\(p=0.4\\):\n\n[loglbinom(0.4), loglbinom1(0.4)]\n\n[-21.08353673407403, -21.08353673407401]\n\n\nso both of course coincides with the previously found loglikelihood.\nWe are going to apply loglbinom to an array fo values of p. For this, we create its vectorised version:\n\nvloglbinom = np.vectorize(loglbinom)",
    "crumbs": [
      "Labs - Problems",
      "Lab 8 - Problems"
    ]
  },
  {
    "objectID": "Lab8.html#section-6",
    "href": "Lab8.html#section-6",
    "title": "Lab 8",
    "section": "2.2 ",
    "text": "2.2 \n\nDefine array x of 1000 points from \\([0,1]\\), using np.linspace function (see e.g. Lab 4 or Lab 5). Apply the vectorised function vloglbinom to x (it keeps n=20 and data fixed), and assign the result to y. Plot the graph of y against x.\n\n\n\n\n\n\n\n\n\n\nAnd finally, one can find now the point of maximum of the log-likelihood: namely, np.argmax(y) returns the index in array y of the maximal element:\n\nind_max = np.argmax(y)\n\ntherefore, the value y[ind_max] is the maximal value of the log-likelihood function. However, we are interested in the argument, that is the corresponding x[ind_max]:\n\nx[ind_max]\n\n0.37537537537537535\n\n\nthat is pretty close to the initially found value. (Surely, if we divide \\([0,1]\\) on a larger number of pieces, the prediction will be better.)\nThe following graph has the vertical line at the found value of x[ind_max], as you can see, it comes (pretty close) to the maximum of the log-likelihood function.",
    "crumbs": [
      "Labs - Problems",
      "Lab 8 - Problems"
    ]
  },
  {
    "objectID": "Lab8.html#section-7",
    "href": "Lab8.html#section-7",
    "title": "Lab 8",
    "section": "3.1 ",
    "text": "3.1 \n\nGenerate \\(m=1000\\) samples, each sample of size \\(5\\), of Bernoulli random variable with \\(p=\\mathbb{P}(X=1)=0.3\\). For each sample calculate the maximum likelihood estimate \\(\\hat{p}\\) of \\(p\\). Find the average of the obtained \\(MLE\\).\n\n\n0.29140000731451227\n\n\nSince the samples are random, your answer will be different. Notice that it’s quite close to \\(p=0.3\\).",
    "crumbs": [
      "Labs - Problems",
      "Lab 8 - Problems"
    ]
  }
]